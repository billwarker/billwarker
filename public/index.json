[{"content":"Notes Main Steps:\n Frame the problem Get data EDA (exploratory data analysis) Prepare the data for ML Model Selection Tune the model Present solution Launch, monitor, iterate  Look at the big picture  predict the median housing price for a district in CA what\u0026rsquo;s the business objective (not building a model for fun) ask: what is the current, non-ML solution? why can\u0026rsquo;t we use that start thinking about/designing the system  Pipelines\n sequence of data processing components typically runs asynchronously: When you execute something synchronously, you wait for it to finish before moving on to another task. When you execute something asynchronously, you can move on to another task before it finishes. components are self contained, downstream components can keep working for a while by just using the last output from the broken component (async)  Types of Regression\n multiple regression: multiple features to make a prediction univariate regression: only trying to predict a single value  Selecting a performance measure\n Root Mean Square Error (RMSE):  $$ RMSE(X,h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^{2}} $$\n gives higher weight to larger errors lowercase italic font for scalars, lowercase bold for vectors, uppercase bold for matrices if there are many outliers, then Mean Absolute Error (MAE) might be a better cost function:  $$ MAE(X,h) = \\frac{1}{m} \\sum_{i=1}^{m} \\lvert h(x^{(i)}) - y^{(i)}\\rvert$$\n different ways to measure difference between vectors RMSE is euclidian distance, $l_2$ norm computing sum of absolutes is $l_1$ norm, measures the distance between two points if you can only travel along orthogonal (perpendicular) lines $l_0$ just gives the number of non-zero elements in vector higher the norm index, the more it focuses on large values and neglects smaller ones. this is why RMSE is more sensitive to outliers than MAE  Verify Assumptions\n list and verify assumptions e.g. what does the output exactly need to be, what do we think is true about the problem/solution we\u0026rsquo;ve proposed  Get the Data  usually data is in DBs or spread across many files, so common first step is jumping through the hoops of access, getting used to schemas, legal precautions, etc. best to automate process of fetching data, future proof against changes  import os import tarfile from six.moves import urllib  DOWNLOAD_ROOT = \u0026quot;https://raw.githubusercontent.com/ageron/handson-ml/master/\u0026quot; HOUSING_PATH = os.path.join(\u0026quot;datasets\u0026quot;, \u0026quot;housing\u0026quot;) HOUSING_URL = DOWNLOAD_ROOT + \u0026quot;datasets/housing/housing.tgz\u0026quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.makedirs(housing_path) tgz_path = os.path.join(housing_path, \u0026quot;housing.tgz\u0026quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close()  fetch_housing_data()  import pandas as pd  def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(HOUSING_PATH, \u0026quot;housing.csv\u0026quot;) return pd.read_csv(csv_path)  housing = load_housing_data() housing.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY     housing.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total_rooms 20640 non-null float64 total_bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB  housing[\u0026quot;ocean_proximity\u0026quot;].value_counts()  \u0026lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64  housing.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value     count 20640.000000 20640.000000 20640.000000 20640.000000 20433.000000 20640.000000 20640.000000 20640.000000 20640.000000   mean -119.569704 35.631861 28.639486 2635.763081 537.870553 1425.476744 499.539680 3.870671 206855.816909   std 2.003532 2.135952 12.585558 2181.615252 421.385070 1132.462122 382.329753 1.899822 115395.615874   min -124.350000 32.540000 1.000000 2.000000 1.000000 3.000000 1.000000 0.499900 14999.000000   25% -121.800000 33.930000 18.000000 1447.750000 296.000000 787.000000 280.000000 2.563400 119600.000000   50% -118.490000 34.260000 29.000000 2127.000000 435.000000 1166.000000 409.000000 3.534800 179700.000000   75% -118.010000 37.710000 37.000000 3148.000000 647.000000 1725.000000 605.000000 4.743250 264725.000000   max -114.310000 41.950000 52.000000 39320.000000 6445.000000 35682.000000 6082.000000 15.000100 500001.000000      percentiles: what percentage of the data falls beneath this point. i.e. if 50th percentile is 100 for an attribute that means half of all the samples have a value less than 100 for that attribute if mean varies a lot from median that speaks to the presence of outliers pulling it up/down  %matplotlib inline import matplotlib.pyplot as plt  housing.hist(bins=50, figsize=(20,15)) plt.show()   Median income doesn\u0026rsquo;t seem to be expressed as USD Median house value and age seem to be capped features have different scales (needs feature scaling)  Create a test set\n before anything else, set aside test set this will avoid data snooping bias; i.e. fitting the model to better generalize on the test set  import numpy as np  def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices]  train_set, test_set = split_train_test(housing, 0.2)  len(train_set)  16512  len(test_set)  4128   this function will regenerate different sets on each run you can seed the random number generator, but this breaks if you add new data to the dataset (regenerates new train/test) eventually data you\u0026rsquo;ve trained on before will make it into the test set on multiple reruns with this pipeline can use each instance\u0026rsquo;s indentifier (assuming unique and immutable) to decide whether or not it should go in the test set code below computes a hash of each instance\u0026rsquo;s indentifier and puts that instance in the test set if hash is lower or equal to 20% of the maximum hash value. ensures consistency across multiple runs, and the test set will always contain 20% of the new data, but never any instance that was previously in the training set  from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) \u0026amp; 0xffffffff \u0026lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set]  housing_with_id = housing.reset_index() train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026quot;index\u0026quot;)  train_set   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   3 3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY   6 6 -122.25 37.84 52.0 2535.0 489.0 1094.0 514.0 3.6591 299200.0 NEAR BAY   ... ... ... ... ... ... ... ... ... ... ... ...   20635 20635 -121.09 39.48 25.0 1665.0 374.0 845.0 330.0 1.5603 78100.0 INLAND   20636 20636 -121.21 39.49 18.0 697.0 150.0 356.0 114.0 2.5568 77100.0 INLAND   20637 20637 -121.22 39.43 17.0 2254.0 485.0 1007.0 433.0 1.7000 92300.0 INLAND   20638 20638 -121.32 39.43 18.0 1860.0 409.0 741.0 349.0 1.8672 84700.0 INLAND   20639 20639 -121.24 39.37 16.0 2785.0 616.0 1387.0 530.0 2.3886 89400.0 INLAND    16512 rows Ã— 11 columns\n   note: this approach looks fugazi, indices aren\u0026rsquo;t shuffled\n  the problem still exists but would need a better implementation than what\u0026rsquo;s here\n  if you use row index as a unique identifier, you must make sure that new data always gets appended to the end of the dataset and a row is never deleted\n  instead, you can try to engineer a unique ID for each row by combining some of the (ideally most stable/constant) features\n  e.g. a district\u0026rsquo;s latitude/longitude is guaranteed to be stable for a few million years lol:\n  housing_with_id['id'] = housing[\u0026quot;longitude\u0026quot;] * 1000 + housing[\u0026quot;latitude\u0026quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026quot;id\u0026quot;)  len(housing_with_id)  20640  len(housing_with_id[\u0026quot;id\u0026quot;].unique()) # also fugazi  12590   the approach above in the book obviously doesn\u0026rsquo;t work haha  from sklearn.model_selection import train_test_split  train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)  train_set   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     14196 -117.03 32.71 33.0 3126.0 627.0 2300.0 623.0 3.2596 103000.0 NEAR OCEAN   8267 -118.16 33.77 49.0 3382.0 787.0 1314.0 756.0 3.8125 382100.0 NEAR OCEAN   17445 -120.48 34.66 4.0 1897.0 331.0 915.0 336.0 4.1563 172600.0 NEAR OCEAN   14265 -117.11 32.69 36.0 1421.0 367.0 1418.0 355.0 1.9425 93400.0 NEAR OCEAN   2271 -119.80 36.78 43.0 2382.0 431.0 874.0 380.0 3.5542 96500.0 INLAND   ... ... ... ... ... ... ... ... ... ... ...   11284 -117.96 33.78 35.0 1330.0 201.0 658.0 217.0 6.3700 229200.0 \u0026lt;1H OCEAN   11964 -117.43 34.02 33.0 3084.0 570.0 1753.0 449.0 3.0500 97800.0 INLAND   5390 -118.38 34.03 36.0 2101.0 569.0 1756.0 527.0 2.9344 222100.0 \u0026lt;1H OCEAN   860 -121.96 37.58 15.0 3575.0 597.0 1777.0 559.0 5.7192 283500.0 \u0026lt;1H OCEAN   15795 -122.42 37.77 52.0 4226.0 1315.0 2619.0 1242.0 2.5755 325000.0 NEAR BAY    16512 rows Ã— 10 columns\n  the above are random sampling methods which work well enough on a large dataset if the dataset is small then you should do stratified sampling (i.e. take steps to ensure that the sample is representative of the whole pop.) divide the data into different groups (strata) and randomly sample from those in a way which is representative e.g. stratify by median income:  # use pd.cut to bin the median income into categories housing['income_cat'] = pd.cut(housing['median_income'], bins=[0., 1.5, 3., 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])  housing['income_cat'].hist()  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc82c0cf90\u0026gt;  from sklearn.model_selection import StratifiedShuffleSplit  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing['income_cat']): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index]  # checking to see if it worked as expected strat_test_set[\u0026quot;income_cat\u0026quot;].value_counts()/len(strat_test_set)  3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64   the stratified sampling matches the proportions seen in the histogram; the test set in this instance is representative of the income_cat distribution found in the whole dataset now remove income_cat attribute to restore data to original form  for set_ in (strat_train_set, strat_test_set): set_.drop(\u0026quot;income_cat\u0026quot;, axis=1, inplace=True)  Discover and Visualize the Data to Gain Insights (EDA)  only explore the training set, pust the test set aside if training set is large, you might want to just explore a sample to make manipulations faster create a copy of the training set so you can play around without harming it  Visualizing Geographical Data:\nhousing = strat_train_set.copy()  housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc822c8990\u0026gt;   Can visualize density in the geographical viz with lower alpha  housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=housing['population']/100, label='population', figsize=(10,7), c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True) plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7fcc80e608d0\u0026gt;   prices are higher along the coast (duh), with some hotspots in the bay area and around LA could use a clustering algo to find many clusters, and then create features that measure proximity to the cluster centers Ocean proxmity is useful feature  Look for correlations:\n standard correlation coefficient only measures linear relationships, misses any other kinds  corr_matrix = housing.corr()  corr_matrix['median_house_value'].sort_values(ascending=False)  median_house_value 1.000000 median_income 0.687160 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population -0.026920 longitude -0.047432 latitude -0.142724 Name: median_house_value, dtype: float64  from pandas.plotting import scatter_matrix  attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age'] scatter_matrix(housing[attributes], figsize=(12,8)) plt.show()   diagonal lines are histograms of attributes  # focus on median income housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc83a74110\u0026gt;   clear correlation can also see the cap at $500k quite clearly other less visible lines at \\$350K, $450K; might want to remove from data to stop model from picking up on quirks  Experimenting with Attribute Combinations\n EDA shows some interesting correlations so far some attributes are tail heavy, so could transform to normal distribution by taking their log can take combinations of features to get relationships; some attributes make less sense when considered independently e.g. compute avg. rooms per household as rooms/households  housing['rooms_per_household'] = housing['total_rooms']/housing['households'] housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms'] housing['population_per_household'] = housing['population']/housing['households']  # check corr matrix again corr_matrix = housing.corr() corr_matrix['median_house_value'].sort_values(ascending=False)  median_house_value 1.000000 median_income 0.687160 rooms_per_household 0.146285 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population_per_household -0.021985 population -0.026920 longitude -0.047432 latitude -0.142724 bedrooms_per_room -0.259984 Name: median_house_value, dtype: float64   bedrooms per room is much more correlated (negatively) to median house value than total rooms/bedrooms houses with a lower bedroom/room ratio seem to be more expensive EDA doesn\u0026rsquo;t need to be completely thorough, point is to get insights that will get you a reasonably decent prototype iterative process  Prepare the Data for Machine Learning Algorithms write functions to prepare data because:\n can reproduce on fresh data build a library of transformation functions can use in live system easier experimentation  housing = strat_train_set.drop(\u0026quot;median_house_value\u0026quot;, axis=1) housing_labels = strat_train_set[\u0026quot;median_house_value\u0026quot;].copy()  Handling Missing Values\n 3 options: get rid of of corresponding samples get rid of the entire attribute impute values to (mean, median, etc)  from sklearn.impute import SimpleImputer  imputer = SimpleImputer(strategy=\u0026quot;median\u0026quot;)  # create copy of numerical features housing_num = housing.drop(\u0026quot;ocean_proximity\u0026quot;, axis=1)  imputer.fit(housing_num) imputer.statistics_  array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])  X = imputer.transform(housing_num)  housing_tr = pd.DataFrame(X, columns=housing_num.columns)  Handling Text and Categorical Attributes\n encode to numerical values  housing_cat = housing[[\u0026quot;ocean_proximity\u0026quot;]]  from sklearn.preprocessing import OrdinalEncoder  ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)  housing_cat_encoded[:5]  array([[0.], [0.], [4.], [1.], [0.]])  ordinal_encoder.categories_  [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]   ordinal encoding places ranking on the attributes, so some algos might interpret nearby values as similar (4\u0026amp;3 are more similar than 1\u0026amp;7, although these are all just arbitrary indices) for nominal categories its better to one-hot encode them; create a binary attribute per category also known as creating dummy variables  from sklearn.preprocessing import OneHotEncoder  cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat)  housing_cat_1hot  \u0026lt;16512x5 sparse matrix of type '\u0026lt;class 'numpy.float64'\u0026gt;' with 16512 stored elements in Compressed Sparse Row format\u0026gt;   sparse matrices don\u0026rsquo;t use memory to store zero elements  cat_encoder.categories_  [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]   if a categorical attribute has a large number of categories, one hot encoding will create a large number of input features and may hurt performance. in this case embeddings are useful (denser representations)  Custom Transformers\n write to align with Scikit-Learn so you can use pipelines add hyperparameters to gate any data preparation steps you aren\u0026rsquo;t sure about  from sklearn.base import BaseEstimator, TransformerMixin  rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6  class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, population_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household]  attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs = attr_adder.transform(housing.values)  np.c_ stacks arrays on their last axis (turns column vectors into a matrix)\nnp.c_[np.array([1,2,3]), np.array([4,5,6])]  array([[1, 4], [2, 5], [3, 6]])  Feature Scaling:\n  ML algos don\u0026rsquo;t perform well when features have different scales\n  Two ways to feature scale:\n  min-max scaling: attributes are shifted and rescaled so they range from 0-1. $$x_{scaled} = \\frac{x_n - x_{min}}{x_{max} - x_{min}}$$\n  standardization: subtract the mean and divide by standard deviation\n  standardization is less affected by outliers in the data $$x_{scaled} = \\frac{x_n - {\\bar{x}}}{\\sigma}$$\n  fit all transformers to the training set, then use them on the test\n  Transformation Pipelines\nfrom sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler  num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy=\u0026quot;median\u0026quot;)), ('attribs_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()) ])  housing_num_tr = num_pipeline.fit_transform(housing_num)   calls fit_transform() on all of the transformers sequentially  from sklearn.compose import ColumnTransformer  num_attribs = list(housing_num) cat_attribs = [\u0026quot;ocean_proximity\u0026quot;] full_pipeline = ColumnTransformer([ (\u0026quot;num\u0026quot;, num_pipeline, num_attribs), (\u0026quot;cat\u0026quot;, OneHotEncoder(), cat_attribs), ])  housing_prepared = full_pipeline.fit_transform(housing)  Select and Train a Model Training and Evaluating on the Training Set\n preprocessing steps make training models simple  from sklearn.linear_model import LinearRegression  lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels)  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)  some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(f\u0026quot;Predictions: {lin_reg.predict(some_data_prepared)}\u0026quot;) print(f\u0026quot;Labels: {list(some_labels)}\u0026quot;)  Predictions: [211944.80589799 321295.84907457 210851.33029021 62359.51850965 194954.19182968] Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]  from sklearn.metrics import mean_squared_error  housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) print(lin_rmse)  68898.54780411992   error of \\$68K here isn\u0026rsquo;t very satisfying (housing prices range from \\$120K to \\$265K) if a model is underfitting the data it means that the features don\u0026rsquo;t provide enough information to make good decisions, or model is too weak  from sklearn.tree import DecisionTreeRegressor  tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels)  DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')  housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse  0.0  Better Evaluation with Cross Validation\n K-fold Cross Validation splits the training set into K folds, training the model K times by testing on one of the folds and composing the training set on the remaining K-1 Sklearns CV feature expects a utility function (greater is better) as opposed to a cost function (lower is better), so the scoring function is actually opposite of MSE, which is why the following code uses -scores in the square root  from sklearn.model_selection import cross_val_score  scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) tree_rmse_scores = np.sqrt(-scores)  def display_scores(scores): print(\u0026quot;Scores:\u0026quot;, scores) print(\u0026quot;Mean:\u0026quot;, scores.mean()) print(\u0026quot;Standard Deviation\u0026quot;, scores.std())  display_scores(tree_rmse_scores)  Scores: [67971.85204287 67101.92229431 69341.35567834 66956.22248918 69739.80377843 72468.59865874 66736.84144169 67241.24548885 72220.02744352 70187.38922156] Mean: 68996.52585375118 Standard Deviation 2037.77518366374   CV not only allows you to get an averaged estimate of model\u0026rsquo;s performance, but allows you to get an idea of how precise this estimate is through the standard deviation  lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores)  Scores: [67500.31361237 68404.48325957 68239.95757613 74813.56736728 68419.88576794 71632.92651865 65216.31837467 68702.06708289 71793.11060978 68131.30099374] Mean: 69285.3931163006 Standard Deviation 2576.7108344336184  from sklearn.ensemble import RandomForestRegressor  forest_reg = RandomForestRegressor() forest_reg.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. \u0026quot;10 in version 0.20 to 100 in 0.22.\u0026quot;, FutureWarning) RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  forest_mse = mean_squared_error(housing_labels, forest_reg.predict(housing_prepared)) forest_rmse = np.sqrt(forest_mse) print(forest_rmse)  21923.44698175341  forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores)  display_scores(forest_rmse_scores)  Scores: [50985.57169248 49600.72682606 50759.78939086 54557.52337524 51089.83837069 55891.91762608 51010.98084994 50261.62522741 55934.1764328 52825.16027951] Mean: 52291.73100710661 Standard Deviation 2239.368467655192   Forest looks more promising (lower RMSE), but the sizeable difference between CV performance and training set performance indicates overfitting on the training set Could simplify it, regularize it, or get more training data Ideally you want to shortlist 2-5 models promising models total can use joblib library to save promising models for experimentation/later iterations  Fine-Tune Your Model  fine tune the shortlist of models  Grid Search\n grid search will automate cross-validation on all combinations of hyperparameters specified  from sklearn.model_selection import GridSearchCV  param_grid = [ {'n_estimators': [3,10,30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]}, ]  forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) GridSearchCV(cv='warn', error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid='warn', n_jobs=None, param_grid=[{'max_features': [2, 4, 6, 8], 'n_estimators': [3, 10, 30]}, {'bootstrap': [False], 'max_features': [2, 3, 4], 'n_estimators': [3, 10]}], pre_dispatch='2*n_jobs', refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=0)   try powers of 10 if you have no clue what a hyperparameter should have different dictionaries in param_grid specify separate grid searches  grid_search.best_params_  {'max_features': 8, 'n_estimators': 30}  grid_search.best_estimator_  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)   refit=True argument for GridSearchCV will retrain on the entire training set after finding best estimator hyperparams  cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[\u0026quot;mean_test_score\u0026quot;], cvres[\u0026quot;params\u0026quot;]): print(np.sqrt(-mean_score), params)  65307.87885616995 {'max_features': 2, 'n_estimators': 3} 56745.41295641033 {'max_features': 2, 'n_estimators': 10} 52896.1050644663 {'max_features': 2, 'n_estimators': 30} 59681.45476015563 {'max_features': 4, 'n_estimators': 3} 52735.85674503226 {'max_features': 4, 'n_estimators': 10} 50892.22744742235 {'max_features': 4, 'n_estimators': 30} 59799.42942971745 {'max_features': 6, 'n_estimators': 3} 52650.77632649698 {'max_features': 6, 'n_estimators': 10} 50570.40999405222 {'max_features': 6, 'n_estimators': 30} 59094.68896450388 {'max_features': 8, 'n_estimators': 3} 52492.707418330094 {'max_features': 8, 'n_estimators': 10} 50432.33406200777 {'max_features': 8, 'n_estimators': 30} 62530.835324885855 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 55183.05939812397 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 60507.19412492283 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 53097.84240758412 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 59391.108929740585 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 52293.331430755774 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}   can include data preparation steps as hyperparameters in grid search  Randomized Search\n when hyperparameter search space is large, randomized usually gets comparable results faster explores $n$ different values for each hyperparameter for $n$ total iterations gives you more control over computing budget  Ensemble Methods\n Can combine models that perform best Will often perform better than the best individual model, especially if the models make different types of errors  Analyze the Best Models and their Errors\n Can gain insights by inspecting best models Random Forest can give feature importances:  feature_importances = grid_search.best_estimator_.feature_importances_  extra_attribs = [\u0026quot;rooms_per_hhold\u0026quot;, \u0026quot;pop_per_hhold\u0026quot;, \u0026quot;bedrooms_per_room\u0026quot;] cat_encoder = full_pipeline.named_transformers_[\u0026quot;cat\u0026quot;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True)  [(0.3938202057578966, 'median_income'), (0.15133729393112902, 'INLAND'), (0.1029603450675752, 'bedrooms_per_room'), (0.0754751387108862, 'pop_per_hhold'), (0.06620720663844283, 'longitude'), (0.06338183338235212, 'latitude'), (0.04206999831411881, 'housing_median_age'), (0.03242004947783398, 'rooms_per_hhold'), (0.015482413139961633, 'total_bedrooms'), (0.014766900750197388, 'population'), (0.014573461291699393, 'total_rooms'), (0.014411203134276336, 'households'), (0.006778298384461372, '\u0026lt;1H OCEAN'), (0.0032380720741344592, 'NEAR OCEAN'), (0.002983526044293852, 'NEAR BAY'), (9.40539007408315e-05, 'ISLAND')]   could drop less important features and try retraining could also analyze the specific errors the system makes and try to understand why they\u0026rsquo;re occuring and what could fix (extra features, cleaning outliers, removing uninformative features)  Evaluate Your Sytem on the Test Set  just apply pipeline to the test set (run transform, not fit_transform - we do not want to fit it to the training set!)  final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(\u0026quot;median_house_value\u0026quot;, axis=1) y_test = strat_test_set[\u0026quot;median_house_value\u0026quot;].copy()  X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared)  final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse)  print(final_rmse)  48026.91757298586   in some situations a point estimate of the generalization error will not be enough evidence to launch you can get an idea of precision by computing a 95% confidence interval for the generalization error:  from scipy import stats  confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))  array([46060.77003181, 49915.67977301])   performance will likely be a bit worse than what was measured in cross-validation resist temptation to tweak numbers on the test set present solution, highlighting assumptions, methodology, system limitations create presentations with clear visuals and keep points summarized  Launch, Monitor, and Maintain Your System   launch in prod, polishing code, writing tests and documentation, etc\n  can deploy model by loading it in production environment or wrapping it in a REST api (i.e. cloud functions, microservices)\n  this offers easier updating of system without disrupting main application, and can handle load balancing\n  can deploy in cloud (GCP)\n  need to write monitoring code after deployment to make sure performance doesn\u0026rsquo;t rot over time\n  models decay as the world changes and old data becomes less relevant, need to feed with new data\n  model performance can be inferred from downstream metrics (e.g. how many sales your recommender system is generating)\n  can use human testers to verfiy model outputs (experts, or crowdsourced from platforms like Amazon Mechanical Turk)\n  putting models in prod and maintaining them can be more work than actually creating them\n  automate as much of the process as possible:\n  collecting fresh data regularly and labelling (e.g. using human raters)\n  writing scripts to train the model and fine-tune hyperparameters automatically\n  writing scripts to evaluate new model vs. old model on updated test sets, and deploying the new model if performance has not decreased\n  monitor data quality as well, poor quality data leaking in can be hard to detect\n  create backups of every model and have infrastructure to rollback previous models quickly, in case new ones start failing badly for some reason\n  backups help for investigation/comparison\n  keep backups of datasets as well\n  Exercises  Try a Support Vector Machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\u0026quot;linear\u0026rdquo; (with various values for the C hyperparameter) or kernel=\u0026quot;rbf\u0026rdquo; (with various values for the C and gamma hyperparameters). Donâ€™t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?  from sklearn.svm import SVR  svr = SVR() param_grid = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1.0, 10.0, 100.0]}, {'kernel': ['rbf'], 'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'gamma': ['scale', 'auto']} ] grid_search = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) GridSearchCV(cv='warn', error_score='raise-deprecating', estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False), iid='warn', n_jobs=None, param_grid=[{'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'kernel': ['linear']}, {'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'gamma': ['scale', 'auto'], 'kernel': ['rbf']}], pre_dispatch='2*n_jobs', refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=0)  best_svr = grid_search.best_estimator_  best_svr  SVR(C=100.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated', kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)  np.sqrt(-grid_search.best_score_)  72427.44454616884  Try replacing GridSearchCV with RandomizedSearchCV.  from sklearn.model_selection import RandomizedSearchCV from scipy.stats import uniform from scipy.stats import norm from pprint import pprint  random_forest = RandomForestRegressor()  print(\u0026quot;Parameters currently in use:\\n\u0026quot;) pprint(random_forest.get_params())  Parameters currently in use: {'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}  random_forest = RandomForestRegressor() n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)] max_features = ['auto', 'sqrt'] max_depth = [int(x) for x in np.linspace(10,110, num=11)] max_depth.append(None) min_samples_split = [2, 5, 10] min_samples_leaf = [1,2,4] bootstrap = [True, False] random_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap} random_search = RandomizedSearchCV(estimator=random_forest, param_distributions=random_grid, scoring='neg_mean_squared_error', verbose=1, n_iter=10, cv=3, n_jobs=-1, return_train_score=True)  random_search.fit(housing_prepared, housing_labels)  Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 31.4min finished RandomizedSearchCV(cv=3, error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_sta... param_distributions={'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=1)  best_forest = random_search.best_estimator_  best_forest  RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=110, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  np.sqrt(-random_search.best_score_)  49359.61219717077  Try adding a transformer in the preparation pipeline to select only the most important attributes.  class FeatureImportancesFilter(BaseEstimator, TransformerMixin): def __init__(self, feature_importances, importance_cutoff = 0.9): # no *args or **kargs self.feature_importances = feature_importances feature_indices = [i for i in range(len(self.feature_importances))] self.sorted_feature_importances = sorted(zip(self.feature_importances, feature_indices), reverse=True) self.importance_cutoff = importance_cutoff def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): importance_sum = 0 included_features = list() for importance, ix in self.sorted_feature_importances: if not importance_sum \u0026gt;= self.importance_cutoff: importance_sum += importance included_features.append(ix) else: break return X[:, included_features]   note: might need to fit to actual random forest regressor, or breaks if input shape changes  housing_prepared.shape  (16512, 16)  importance_filter = FeatureImportancesFilter(feature_importances, importance_cutoff=0.9)  importance_filter.transform(housing_prepared).shape  (16512, 8)  Try creating a single pipeline that does the full data preparation plus the final prediction.  e2e_pipeline = Pipeline([ ('data_prep', full_pipeline), ('filter', FeatureImportancesFilter(feature_importances)), ('clf', best_forest) ])  Automatically explore some preparation options using GridSearchCV.  e2e_grid_params = {'filter__importance_cutoff': (0.5, 0.6, 0.65)}  e2e_grid_search = GridSearchCV(e2e_pipeline, e2e_grid_params, scoring='neg_mean_squared_error', verbose=1, cv=3)  e2e_grid_search.fit(housing, housing_labels)  Fitting 3 folds for each of 3 candidates, totalling 9 fits [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 9 out of 9 | elapsed: 1.4min finished GridSearchCV(cv=3, error_score='raise-deprecating', estimator=Pipeline(memory=None, steps=[('data_prep', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3, transformer_weights=None, transformers=[('num', Pipeline(memory=None, steps=[('imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy='median'... min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False), iid='warn', n_jobs=None, param_grid={'filter__importance_cutoff': (0.5, 0.6, 0.65)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring='neg_mean_squared_error', verbose=1)  np.sqrt(-e2e_grid_search.best_score_)  66661.60437351998  e2e_grid_search.best_params_  {'filter__importance_cutoff': 0.65} ","date":"2020-08-17","permalink":"https://billwarker.com/posts/handson-ml-ch2/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 2 Notes"},{"content":"Notes Types of ML\n  ML: algorithms learning from data and improving performance on a task\n  advantage over rule based systems is that machine can update parameters/logic with new/more data (refreshing the model)\n  applying ML to discover/understand patterns in the data: data mining\n  Supervised learning: training a model with labelled examples. can be used for classification tasks (predict a discrete category) or regression (predict a continuous number)\n  Unsupervised learning algos: clustering unlabelled data, visualizing and dimensionality reduction, association rule learning\n  hierarchical clustering: sub-dividing clusters into smaller groups\n  dimensionality reduction: simplify data without losing too much information. i.e. merging correlated features. can help performance, takes up less disk and memory space\n  anomoly detection\n  association rule learning: relations between attributes (similar to data mining)\n  Semi-supervised learning. i.e. Google Photos, looks at unlabelled pictures to find common faces. once given a label for a person it can name everyone in the photo. mixed hierarchical models\n  Reinforcement learning: agent learns its environment and selects a policy (actions, strategy) to optimize some reward\n  Batch vs. Online Learning\n  Batch: training a model on all available data. train the system and then put it in prod. retrain new versions of the model with new data\n  requires a lot of computing resources, can be expensive. not suitable for autonomous systems with limited space for data (i.e. Mars rover)\n  Online: train system incrementally on mini-batches of data\n  great for continuous flows of data or limited storage resources\n  can be used to train on datasets that won\u0026rsquo;t fit in memory (out-of-core learning)\n  learning rate: how fast model adapts to new data\n  too high, forgets old data rapidly, too low, system has inertia (also less sensitive to noisey data)\n  need to monitor online systems if garbage data starts coming in\n  Instance-Based vs. Model-Based Learning\n  how a system generalizes (i.e. answers examples its never seen before)\n  Instance-based: generalizes to new examples by comparing similarity to training examples\n  i.e. KNN\n  Model-based: builds a model to predict new data\n  select a model (i.e. linear model) to represent the data\u0026rsquo;s pattern\n  tune model on a utility or cost function\n  if a model doesn\u0026rsquo;t generalize well, you can try again with better quality training data, more features, or a stronger model (e.g. polynomial vs. basic linear)\n  adding more data tends to get better results on all kinds of algos, to the point where performance can be identical with enough data\n  data needs to be representitive of the problem space trying to model, more data can eliminate noise but procedure needs to be solid or it risks sampling bias\n  Feature Engineering\n feature selection, picking the most useful features feature extraction, combining existing features to produce more useful ones + dimensionality reduction creating new features with the intro of new data  Performance\n  overfitting: performing well on the training data but not generalizing well\n  machine learning can pick up on noise in the data and sometimes even irrelevant features/useless metadata (i.e. data ids/index), detecting false patterns\n  overfitting happens when the model is too complex relative to the amount and noiseness of the data\n  regularization is constraining a model to make it simpler can help overfitting\n  controlled by model hyperparameters (knobs to tweak on the model itself, such as learning rate)\n  underfitting is opposite problem, model is too simple\n  fix it with a more complex model, more/better features, reducing regularization constraints\n  test models to see if they generalize well\n  split data into training and test sets to get error rate on new cases (out of sample/generalization error)\n  if training error is now but oos error is high, overfitting\n  use a third validation set to compare models/hyperparameters, then select the best one and use it one the test\n  cross validation splits the training set into subsets which are used for validation\n  need to make assumptions about the data to pick models reasonably\n  Questions How would you define ML?\n Algorithms that allow a computer to learn from data to improve on a task and generalize to new examples well  Four types of problems where it shines?\n Problems that traditionally require too many rules or hand-tuning Complex problems with no easy logic based model Problems where underlying strategy/solutions change over time, so new model can help adapt Data mining, learning underlying patterns in data  What is a labelled training set?\n Data that has the variable of interest classified (independent variable), and information about it in other attribures (dependent variables)  Two most common supervised tasks?\n Regression: predicting a continuous number for the target variable Classification: predicting a discrete category of target variable  Four kinds of unsupervised tasks?\n Clustering: creating groups in unstructured data Dimesionality Reduction: reducing the number of attributes in the training set while keeping most of the variance(underlying signal) Anomoly Dection: finding outliers Visualization Association Rule learning: data mining, learning patterns in the data  What type of ML algo would you use to allow a robot to walk in various unknown terrains?\n Reinforcement Learning  What type of algo would you use to segment your customers into multiple groups?\n Clustering, Unsupervised Learning  Would you frame the problem of spam detection as a supervised learning problem?\n Supervised; we can use examples of spam and ham (labelled training data) to create a model that identifies which is which  What is an online learning system?\n A system that can update with new data as it comes in, ingesting as it comes in through mini-batches  What is an out-of-core learning system?\n Using online learning to train the model on a dataset that wouldn\u0026rsquo;t fit inside the computers memory if you tried to train it in one giant batch.  What type of learning algo relies on similarity measures to make predictions?\n Instanced-based models, i.e. KNN  Difference between a model parameter and a hyperparameter?\n A model parameter is the coefficient determined by the algorithm to apply to a attribute/feature in the data when making predictions, a hyperparameter is an aspect of the model that you can adjust to change how it is trained  What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n These algos look to fit the model to the data (i.e. represent the problem with some simplified version of it). The strategy is to improve performance in respect to some cost/utility function. They make new predictions by applying the policy/parameters determined through training on the new data\u0026rsquo;s features  Four main challenges in ML?\n Not enough data Non-representative data Poor quality data Overfitting/Underfitting  If a model performs well on the training data but generalizes poorly to new examples, what is happening? 3 possible solutions?\n Model is overfitting to the training data and can\u0026rsquo;t generalize well to the new examples You can regularize the model (i.e. constrain it) to be less representative of the training data Train the model on more data/more representative data Tune the model on a validation set  What is a test set and why would you want to use it?\n Test the performance of the model on new examples to understand how well it generalizes to new data (i.e. the whole point)  What can go wrong if you tune hyperparameters on the test set?\n You fit the model to work well on the testing set, overfitting it and reducing the chance of generalizing well  What is cross validation, why is it better than a validation set?\n Cross validation takes different chunks of the training data and uses them iteratively to train and validate the model, creating a more robust model (training on different samples) and is a more economic use of data. ","date":"2020-08-02","permalink":"https://billwarker.com/posts/handson-ml-ch1/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 1 Notes"},{"content":"The Standard Error of the Mean The Standard Error of the Mean ($SE$) is the standard deviation of the sample distribution of the sample mean. To understand what this means, let\u0026rsquo;s break that sentence down in reverse order (i.e. chronologically):\nSample Mean: we have some probability density function $P$ for a population. We take a sample of $N$ instances from it and calculate our statistic of interest - in this case the mean, $\\bar{x}$. We have to take samples because it is hard/impossible to look at every instance in an entire population to calculate the true mean $\\mu$ hidden to us in nature (i.e. say we were interested in the average weight of all monkeys on the planet).\nSample Distribution: we take many samples (the number of which denoted by $M$) from the population\u0026rsquo;s probability density function $P$ and calculate the sample mean $\\bar{x}$ for each one. All of these sample means can be lumped together into a distribution $S$, which approximates a normal distribution the higher $M$ is due to the Central Limit Theorum. We want to create a sampling distribution because it allows us to reason about the likelihood of different values the sample mean can be. By doing so, we can look at the mean of this distribution and conclude that its the most likely value for $\\mu$ given the data we\u0026rsquo;ve used.\nStandard Deviation: The standard deviation is a metric that describes the dispersion of a dataset/distribution in relation to its mean. It speaks to how close/far the density of the distribution is spread in relation to its mean. The standard deviation of a sample distribution - our standard error $SE$ - is an indication of how representative the distribution is of our true mean $\\mu$. The smaller it is, the more representative it can be said to be; the larger it is, the harder it is to trust.\nTLDR: The Standard Error matters because it allows us to better understand how representative our sampling distribution is (i.e. our model of the true mean).\n$SE$ can be calculated with this formula:\n$$SE = \\frac{\\sigma}{\\sqrt{N}}$$\nwhere ${\\sigma}$ is the standard deviation of the population\u0026rsquo;s probability density function $P$ and $N$ is the number of instances in a sample.\nUnderstanding the Standard Error through Simulation import numpy as np import scipy as sp import pandas as pd import matplotlib.pyplot as plt  # create a random probability distribution function to model our population # in this case a Maxwell continuous random variable (picked randomly) x = np.linspace(sp.stats.maxwell.ppf(0.01), sp.stats.maxwell.ppf(0.99), 100) plt.plot(x, sp.stats.maxwell.pdf(x), 'b-', lw=5, alpha=0.6, label='maxwell pdf') plt.show() mean, var, skew, kurt = sp.stats.maxwell.stats(moments='mvsk') print(f'The true mean of the population is {mean} and its standard deviation is {var**(1/2)}')  The true mean of the population is 1.5957691216057308 and its standard deviation is 0.6734396116428514  # to create our sampling distribution S we take M samples of N instances each # we calculate the mean of each sample and add it to a list, which we can make a histogram with m_samples = 5000 n_instances = 250 sample_means = [] for m in range(m_samples): sample = sp.stats.maxwell.rvs(size=n_instances) # draw a sample from the population sample_means.append(sample.mean()) # add it to our sampling distribution sample_dist = pd.Series(sample_means) # our sample distribution as a pandas series  sample_dist.hist(bins=30) # visualizing our sample distribution of the sample mean  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1341e8fd0\u0026gt;  print(f\u0026quot;The mean of our sample distribution is {sample_dist.mean()}\u0026quot;) print(f\u0026quot;Its standard deviation (The Standard Error) is {sample_dist.std()}\u0026quot;)  The mean of our sample distribution is 1.5960355829136528 Its standard deviation (The Standard Error) is 0.04222284486122923  # calculating the same SE from the formula above se = np.sqrt(var)/np.sqrt(n_instances) print(f\u0026quot;The standard error calculated with the formula is: {se}\u0026quot;)  The standard error calculated with the formula is: 0.042592060787413163 ","date":"2020-07-12","permalink":"https://billwarker.com/posts/standard-error-of-the-mean/","tags":["stats"],"title":"The Standard Error of the Mean"},{"content":"Permutations of a Set Permutations of a set are particular orderings of its elements. To calculate the number of permutations (i.e. the possible orderings) of a subset $k$ distinct elements from a set of size $n$, the formula is:\n$$N(n, k) = \\frac{n!}{(n-k)!}$$\n$n!$ represents all orderings for every element in the set. $(n-k)!$ represents the orderings of the elements we\u0026rsquo;re not including in the subset of $k$ elements - dividing by this number allows us to factor them out of $n!$ in the numerator and give us the number of permutations.\nimport itertools import math S = ['A', 'B', 'C', 'D'] permutations = itertools.permutations(S, 2) # P(4,2) for p in permutations: print(p)  ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'A') ('B', 'C') ('B', 'D') ('C', 'A') ('C', 'B') ('C', 'D') ('D', 'A') ('D', 'B') ('D', 'C')  def permutations(n, k): return int(math.factorial(n)/math.factorial(n-k)) permutations(4, 2)  12  Combinations of a Set Combinations don\u0026rsquo;t care about order - they are just the different subsets of elements in the set. To calculate the number of combinations (i.e. subsets) of $k$ distinct elements from a set of size $n$, the formula is:\n$$C(n, k) = \\frac{n!}{k!(n-k)!}$$\nAn easy way to understand combinations is in relation to permutations - basically, we are calculating the number of permutations of a subset created by $P(n, k)$ and then just dividing that number by the possible ways to order its elements (since combinations don\u0026rsquo;t care about that).\n$$C(n, k) = \\frac{P(n, k)}{k!} = \\frac{n!}{(n-k)!}*\\frac{1}{k!}$$\nS = ['A', 'B', 'C', 'D'] combinations = itertools.combinations(S, 2) # P(4,2) for c in combinations: print(c)  ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'C') ('B', 'D') ('C', 'D')  def combinations(n, k): return int(permutations(n, k)/math.factorial(k)) combinations(4, 2)  6 ","date":"2020-07-07","permalink":"https://billwarker.com/posts/permutations-and-combinations/","tags":["stats"],"title":"Permutations and Combinations"}]