[{"content":"Well, for starters, a hash is an important part of a very useful and fast data structure called a hash table. Let\u0026rsquo;s reframe the question and ask what a hash table is instead.\nThe idea behind a Hash Table A hash table is a data structure that allows for the fast retrieval of data within it, regardless of how much is being stored. It\u0026rsquo;s used to do many things, from efficient caching to database indexing and error checking. The central idea of a hash table can be explained with an example. Let\u0026rsquo;s say we have a big array filled with random elements, one of which we want to retrieve:\nfrom numpy.random import randint def create_array_with_hidden_string(array_size, hidden_string): random_numbers = randint(0, array_size**2, size=(array_size,)) array = list(random_numbers) element_ix = randint(0, array_size) array.insert(element_ix, hidden_string) return array, element_ix hidden_string = 'yeet' array, hidden_string_ix = create_array_with_hidden_string(10**3, hidden_string)  array[:10] # first 10 elements of the array  [438536, 563467, 983737, 888887, 142932, 801221, 364091, 488761, 87409, 695441]  In the code above we\u0026rsquo;ve created an array with 1001 strings, one of which we want to find. Our string of interest, yeet, has been inserted at random index in our array. Let\u0026rsquo;s find it. One way we could do so is through a linear search - simply iterating through the elements and stopping once we find yeet. Let\u0026rsquo;s create a function that implements a linear search and reports how long it takes:\nimport time def linear_search_and_time(array, hidden_string): t_start = time.time() for i in range(len(array)): if array[i] == hidden_string: t_finish = time.time() t_delta = t_finish - t_start print(f\u0026quot;\u0026quot;\u0026quot;found {array[i]} in array of size {len(array)} at index {i} in {round(t_delta,4)} seconds\u0026quot;\u0026quot;\u0026quot;)  linear_search_and_time(array, hidden_string)  found yeet in array of size 1001 at index 81 in 0.0004 seconds  Linear search certainly does the trick, but what if our array was of a much bigger size? A consideration of linear search is that the time it takes to complete scales, well, linearly with the size of the array being searched. In Big O Notation, the time complexity of a linear search is O(N):\narray_sizes = [10**4, 10**5, 10**6, 10**7] for size in array_sizes: array, hidden_string_ix = create_array_with_hidden_string(size, hidden_string) linear_search_and_time(array, hidden_string)  found yeet in array of size 10001 at index 2145 in 0.0085 seconds found yeet in array of size 100001 at index 28500 in 0.1093 seconds found yeet in array of size 1000001 at index 543310 in 2.1833 seconds found yeet in array of size 10000001 at index 9412368 in 36.1402 seconds  As we can see, the linear search gets slower as the size of the array grows.\nSearch is basically instantaneous if you know the position/index of the element you want to find in the array - no matter its size, an array lookup always takes a fast, constant time. In Big O terms, this is O(1):\ndef index_lookup_and_time(array, hidden_string_ix): t_start = time.time() hidden_string = array[hidden_string_ix] t_finish = time.time() t_delta = t_finish - t_start print(f\u0026quot;\u0026quot;\u0026quot;found {hidden_string} in array of size {len(array)} at index {hidden_string_ix} in {round(t_delta,4)} seconds\u0026quot;\u0026quot;\u0026quot;)  for size in array_sizes: array, hidden_string_ix = create_array_with_hidden_string(size, hidden_string) index_lookup_and_time(array, hidden_string_ix)  found yeet in array of size 10001 at index 4934 in 0.0 seconds found yeet in array of size 100001 at index 72609 in 0.0 seconds found yeet in array of size 1000001 at index 827164 in 0.0 seconds found yeet in array of size 10000001 at index 2401275 in 0.0 seconds  This idea of using the index of an element to access it is at the core of hash tables. Independent of the size of the hash table or the position of any of its elements, actions such as adding, accessing, or deleting have a constant O(1) time complexity.\nHashing Algorithms To achieve this, a unique index is created for each value in the table using a hashing algorithm. When a value is \u0026ldquo;hashed\u0026rdquo;, it means that it is being passed through an algorithm that transforms it into a memory address (i.e. an index position). This address should never change over the lifetime of the element and is calculated using aspects of both the element itself and the hash table it is being placed into. In Python\u0026rsquo;s implementation of object hashing, this requirement means that only immutable (i.e. unchangeable) data types like strings, ints, and tuples can be hashed. If mutable objects were able to be hashed, then the hashing algorithm would generate different indexes for them as they changed and internal consistency would be lost.\nWhen an element is added, deleted, or retrieved from a hash table it is first passed into a hashing algorithm to find its location. Whatever action needs to happen then takes place after that.\nThere are different ways to implement hashing algorithms, but in principle they should all be easy to calculate as to keep things fast. Algorithms can also differ based on the intended inputs\u0026rsquo; data types. For numeric keys, a simple hashing function might divide the key by the number of available addresses within an array (i.e. its size) and take the remainder:\n# first, let's create an empty array of a certain size def make_empty_hash_table(size: int): return [None] * size  array = make_empty_hash_table(5) print(array)  [None, None, None, None, None]  # create our number hashing algorithm def simple_numeric_hash(num: int, array_size: int): return num % array_size  # check indices generated by elements print(simple_numeric_hash(30, len(array))) print(simple_numeric_hash(21, len(array))) print(simple_numeric_hash(9, len(array)))  0 1 4  # define function to insert the elements at the indices provided by the hashing algorithm def insert_number_into_hash_table(num, array): ix = simple_numeric_hash(num, len(array)) array[ix] = num return array  array = insert_number_into_hash_table(30, array) array = insert_number_into_hash_table(21, array) array = insert_number_into_hash_table(9, array) print(array)  [30, 21, None, None, 9]  For a string key, it might sum the ASCII codes for each character in the string and do the same division by the array size, taking the remainder:\n# create another empty array array = make_empty_hash_table(6)  # create our string hashing algorithm def simple_string_hash(string: str, array_size: int): ascii_sum = 0 for char in string: ascii_sum += ord(char) return ascii_sum % array_size  # check indices generated by pet hamster names print(simple_string_hash('Squeeky', len(array))) print(simple_string_hash('Squishy', len(array))) print(simple_string_hash('Squirrelly', len(array)))  5 2 4  # define function to insert the elements at the indices provided by the hashing algorithm def insert_string_into_hash_table(string, array): ix = simple_string_hash(string, len(array)) array[ix] = string return array  # store some pet hamsters in a hash table array = insert_string_into_hash_table('Squeeky', array) array = insert_string_into_hash_table('Squishy', array) array = insert_string_into_hash_table('Squirrelly', array) print(array)  [None, None, 'Squishy', None, 'Squirrelly', 'Squeeky']  Managing Collisions A hashing algorithm is an elegant way to sort data and have it be quickly accessible. There is one problem that needs to be solved with this approach, however - what if two elements generate the same hash?\nprint(simple_string_hash('Squishy', len(array))) print(simple_string_hash('Smokey', len(array)))  2 2  This is what\u0026rsquo;s known as a collision - when multiple elements yield the same hash. Positions in a hash table can\u0026rsquo;t belong (at least directly) to more than a single element, so available positions are used up as elements are added in. Fortunately, there are a variety of solutions that hash tables can use to manage collisions. These solutions fall into two categories: open addressing and closed addressing.\nOpen addressing solutions will place a collided element in another open position within the hash table. One implementation of this is linear probing, which will do a linear search from the collided index for the next available spot and place the element there. When the element needs to be accessed, the hash table will compute its hash, find the index position, see that position is occupied by something else, and then do a linear search from that spot to find it. Other open addressing algorithms behave similarly, just switching up how the search is performed: plus 3 rehashing checks every third available position, quadratic probing squares the number of failed placement attempts and checks that many positions away, and double hashing applies a second hash function after the first yields a collision. With all of these approaches the goal is find an available position further away from the collision - uniformly distributing across the table will reduce the likelihood of further collisions happening.\nHere\u0026rsquo;s a simple implementation of open addressing with linear probing:\ndef linear_probing_insert(element, ix, array): if array[ix] is None: array[ix] = element else: print(f'collision at index {ix}') lin_search_positions = list(range(ix + 1, len(array))) + list(range(0, ix)) for pos in lin_search_positions: if array[pos] is None: array[pos] = element print(f'added {element} at index {pos} instead') return array print(f'nowhere to put {element}') return array def insert_with_open_addressing(string, array): ix = simple_string_hash(string, len(array)) return linear_probing_insert(string, ix, array)  open_example = array  insert_with_open_addressing('Smokey', open_example)  collision at index 2 added Smokey at index 3 instead [None, None, 'Squishy', 'Smokey', 'Squirrelly', 'Squeeky']  Closed addressing solutions create linked lists at occupied positions in the hash table and add elements sequentially within them. Whereas open addressing solutions aim to place elements across the entire hash table\u0026rsquo;s available positions, closed addressing solutions opt to nest multiple elements within a sub-level list at the positions themselves (think going deep instead of going wide). Either way, both approaches end up having to do some sort of search after hashing an element and getting a collided position.\nHere\u0026rsquo;s a simple implementation of closed addressing with linked lists:\ndef closed_addressing_insert(element, ix, array): if array[ix] is None: array[ix] = [element] else: print(f'collision at index {ix}') array[ix].append(element) print(f'added {element} in a nested list at index {ix}') def insert_with_closed_addressing(string, array): ix = simple_string_hash(string, len(array)) return closed_addressing_insert(string, ix, array)  closed_example = make_empty_hash_table(6)  insert_with_closed_addressing('Squishy', closed_example) insert_with_closed_addressing('Squirrelly', closed_example) insert_with_closed_addressing('Squeeky', closed_example) insert_with_closed_addressing('Smokey', closed_example)  collision at index 2 added Smokey in a nested list at index 2  closed_example  [None, None, ['Squishy', 'Smokey'], None, ['Squirrelly'], ['Squeeky']]  One can aim to avoid collisions entirely by using a larger hash table with more available positions than values to occupy them. The ratio between stored elements and available positions in a hash table is called its load factor, and the lower this is the lower the likelihood of collisions (and subsequent linear searches). If a hash table contains too many collisions, its performance will worsen as more searches with increased time complexity are needed. No collisions means that access time will always be a speedy O(1) time complexity. A good hashing algorithm should aim to minimize collisions, resolve them quickly if they do happen, and generate a uniform distribution of hash values across the table.\nThe Difference between a Hash Table and a Hash Map A map is an association between a key and a value - for key K remember value V.\nMapping between keys and values can be done in many different ways, and one way of doing so is with a hash table. A hash table is just a structure for storing data, but when it stores data that is in the form of a distinct key-value pairing, it\u0026rsquo;s called a hash map.\nIn a hash table that simply stores values like the one above, there are no key-value pairs; each value is its own key with no association with another object.\nIn essence, a hash table is a data structure for quickly storing and accessing a data. A hash map is a particular kind of hash table that stores key-value pairs.\nHash Map Implementation with Lists There are three components to this Hash Map:\n Hash Table: a simple array Hashing Algorithm: a simple hash function that sums the ASCII values in a string, divides it by the size of the array, and returns the remainder Collision Handling: A closed addressing solution for adding elements into a list at each position, and then performing a linear search at the time of accessing  class HashMap: def __init__(self, size): self.size = size self.map = [None] * self.size def _get_hash(self, key): hash_ = 0 for char in str(key): hash_ += ord(char) return hash_ % self.size def add(self, key, value): key_hash = self._get_hash(key) key_value = [key, value] if self.map[key_hash] is None: self.map[key_hash] = list([key_value]) return True else: for pair in self.map[key_hash]: if pair[0] == key: pair[1] = value return True self.map[key_hash].append(key_value) return True def get(self, key): key_hash = self._get_hash(key) if self.map[key_hash] is not None: for pair in self.map[key_hash]: if pair[0] == key: return pair[1] return None def delete(self, key): key_hash = self._get_hash(key) if self.map[key_hash] is None: return False for i in range(0, len(self.map[key_hash])): if self.map[key_hash][i][0] == key: self.map[key_hash].pop(i) return True def display(self): for item in self.map: if item is not None: print(str(item))  # lists of hamsters and their favourite foods - the key value pairs to be inserted into the hash map hamsters = ['Squeeky', 'Squishy', 'Speedy', 'Smokey', 'Squirrelly', 'Snowy', 'Sleepy', 'Smoochy', 'Smarty', 'Snoozy', 'Smoothy', 'Shiny'] fav_foods = ['Pizza', 'Corn', 'Baguettes', 'Beef Jerky', 'Pho', 'Pad Thai', 'Shawarma', 'Chocolate', 'Carrots', 'Wine', 'Smoothies', 'Apples']  To demonstrate the idea of a hash table\u0026rsquo;s load factor, we\u0026rsquo;ll create two hash maps - one with a size equalling the number of elements, and one with a size that is the square of the number of elements.\nhamster_hash_map = HashMap(len(hamsters)) for hamster, food in zip(hamsters, fav_foods): hamster_hash_map.add(hamster, food)  hamster_hash_map.display()  [['Squishy', 'Corn'], ['Sleepy', 'Shawarma']] [['Squirrelly', 'Pho'], ['Snowy', 'Pad Thai'], ['Smarty', 'Carrots']] [['Speedy', 'Baguettes'], ['Smoochy', 'Chocolate']] [['Shiny', 'Apples']] [['Smokey', 'Beef Jerky']] [['Snoozy', 'Wine']] [['Squeeky', 'Pizza'], ['Smoothy', 'Smoothies']]  When the hash table is small relative to the number of elements being stored, collisions can occur. Not a big deal with only a few elements, but at much larger scales this would begin to hurt performance.\nbig_hamster_hash_map = HashMap(len(hamsters)**2) for hamster, food in zip(hamsters, fav_foods): big_hamster_hash_map.add(hamster, food)  big_hamster_hash_map.display()  [['Smoochy', 'Chocolate']] [['Squeeky', 'Pizza']] [['Smoothy', 'Smoothies']] [['Squishy', 'Corn']] [['Speedy', 'Baguettes']] [['Sleepy', 'Shawarma']] [['Smokey', 'Beef Jerky']] [['Smarty', 'Carrots']] [['Squirrelly', 'Pho']] [['Snoozy', 'Wine']] [['Shiny', 'Apples']] [['Snowy', 'Pad Thai']]  When the size of the hash table is much larger than the number of elements, there\u0026rsquo;s a reduced chance of collisions occuring. This is an ideal scenario where the table is highly performant an increased likelihood of O(1) access times.\n","date":"2020-09-28","permalink":"https://billwarker.com/posts/what-is-a-hash/","tags":["comp-sci"],"title":"What is a Hash?"},{"content":"Notes Classification: predicting classes/categories\nIntroducing MNIST  handwritten digits  from sklearn.datasets import fetch_openml  mnist = fetch_openml('mnist_784', version=1)  mnist.keys()  dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])  X, y = mnist['data'], mnist['target']  X.shape  (70000, 784)   70K samples, 784 features (28x28 pixels)  y.shape  (70000,)  import matplotlib as mpl import matplotlib.pyplot as plt  some_digit = X[0] some_digit_image = some_digit.reshape(28,28) plt.imshow(some_digit_image, cmap=\u0026quot;binary\u0026quot;) plt.axis(\u0026quot;off\u0026quot;) plt.show()  y[0]  '5'   cast target variable to integers  import numpy as np  y = y.astype(np.uint8)  X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:],   dataset is already shuffled, some CV folds will be similar (no missing digits)  Training a binary classifier  simplify the problem to classifying a single digit Stochastic Gradient Descent (SGD) classifier: good for large datasets, deals with training instances independently, one at a time relies on randomness  y_train_5 = (y_train == 5) y_test_5 = (y_test == 5)  from sklearn.linear_model import SGDClassifier  sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5)  SGDClassifier(random_state=42)  sgd_clf.predict([some_digit])  array([ True])  Performance Measures  Can use cross validation Custom implementation of CV:  from sklearn.model_selection import StratifiedKFold from sklearn.base import clone  skfolds = StratifiedKFold(n_splits=3, random_state=42) for train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_5[train_index] X_test_fold = X_train[test_index] y_test_fold = y_train_5[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct/len(y_pred))  /opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True. FutureWarning 0.95035 0.96035 0.9604   performs stratified sampling to get a representative ratio of each class  from sklearn.model_selection import cross_val_score  cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.95035, 0.96035, 0.9604 ])   accuracy doesn\u0026rsquo;t mean much here, since about 10% of the samples are 5s. If you guess non-5 for every sample you\u0026rsquo;ll get around 90% accuracy  from sklearn.base import BaseEstimator  class Never5Classifier(BaseEstimator): def fit(self, X, y=None): return self def predict(self, X): return np.zeros((len(X), 1), dtype=bool)  never_5_clf = Never5Classifier() cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')  array([0.91125, 0.90855, 0.90915])   accuracy is generally not the preferred performance measure, especially with skewed datasets  Confusion Matrix\n count the number of times instances of class A are classfied as class B  from sklearn.model_selection import cross_val_predict   cross_val_predict returns the predictions for each test fold  y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)  y_train_pred.shape  (60000,)  from sklearn.metrics import confusion_matrix  confusion_matrix(y_train_5, y_train_pred)  array([[53892, 687], [ 1891, 3530]])    each row represents an actual class\n  each column represents a predicted class\n  first row of this matrix is non-5s, second row is 5s\n  row 1 col 1: true negatives\n  row 1 col 2: false positives\n  row 2 col 1: false negatives\n  row 2 col 2: true positives\n  a perfect classifier would only have true positives and true negatives\n  # pretending we reached perfection y_train_perfect_predictions = y_train_5 confusion_matrix(y_train_5, y_train_perfect_predictions)  array([[54579, 0], [ 0, 5421]])  Precision and Recall\nPrecision: accuracy of positive predictions $$ precision = \\frac{TP}{TP+FP} $$\nRecall (AKA sensitivity or True Positive Rate): $$ recall = \\frac{TP}{TP+FN} $$\nfrom sklearn.metrics import precision_score, recall_score  precision_score(y_train_5, y_train_pred)  0.8370879772350012  recall_score(y_train_5, y_train_pred)  0.6511713705958311   higher precision than recall: when the classifier predicted a 5, it was likely to be correct. however, this came at the cost of incorrectly predicting non-5 on samples that it was less sure about its convenient to combine precision and recall into a single metric, the F1 score:  $$ F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = 2\\times\\frac{precision\\times{recall}}{precision + recall} = \\frac{TP}{TP + \\frac{FN+TP}{2}}$$\n this is the harmonic mean; while regular mean treats all values equally, harmonic mean gives much more weight to low values F1 is only high if both precision and recall are  from sklearn.metrics import f1_score  f1_score(y_train_5, y_train_pred)  0.7325171197343846   while F1 is a metric that favours equally good precision and recall, there are instances when prioritizing one of the two is more valuable e.g. a classifier that detects whether videos are safe for kids: you want high precision because the cost of being wrong is very high, and its better to reject potentially safe videos if it ensures that no unsafe videos are recommended e.g. a classifier for catching shoplifting can favour recall; maximizing the potential for catching all instances of shoplifting is well worth the potential for a few false positives here and there there is a tradeoff between precision and recall, can\u0026rsquo;t have it both ways  Precision/Recall Trade-Off\n controlled by the threshold at which a sample is classified as true can control this threshold by calling decision_function() instead of predict() for sklearn estimators and setting a threshold yourself:  y_scores = sgd_clf.decision_function([some_digit]) y_scores  array([2164.22030239])  threshold = 0 y_some_digit_pred = (y_scores \u0026gt; threshold) y_some_digit_pred  array([ True])  threshold = 8000 y_some_digit_pred = (y_scores \u0026gt; threshold) y_some_digit_pred  array([False])   raising the threshold increases precision and lowers recall lowering the threshold increases recall and lowers precision  y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function')  from sklearn.metrics import precision_recall_curve  precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)  import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline sns.set_style('whitegrid')  def plot_precision_recall_vs_threshold(precisions, recalls, thresholds, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(thresholds, precisions[:-1], 'b--', label='Precision') plt.plot(thresholds, recalls[:-1], 'g--', label='Recall') plt.legend()  plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.show()   precision can go down when the threshold increases e.g. getting 4/5 (80%) correct, then raising threshold and getting 3/4 correct (75%) can also plot precision vs. recall directly:  def plot_precision_vs_recall(precisions, recalls, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(recalls, precisions, 'b--') plt.xlabel('Recall') plt.ylabel('Precision')  plot_precision_vs_recall(precisions, recalls) plt.show()   precision really starts to dip around 70% recall might make sense to set the threshold before that drop, but depends on the context of your project to set the threshold at a specific precision: np.argmax() gives the first index of the maximum value; in this case, the first instance where precision \u0026gt; 90 is true  threshold_90_precision = thresholds[np.argmax(precisions \u0026gt;= 0.9)] threshold_90_precision  3370.0194991439557  y_train_pred_90 = (y_scores \u0026gt;= threshold_90_precision)  precision_score(y_train_5, y_train_pred_90)  0.9000345901072293  recall_score(y_train_5, y_train_pred_90)  0.4799852425751706  The ROC Curve\n Receiver Operating Characteristic plots true positive rate (TPR) vs false positive rate (FPR) FPR is ratio of false positives, i.e. 1 - True Negative Rate (TNR) TNR is also known as specificity ROC is plotting sensitivity (recall) vs 1 - specificity  from sklearn.metrics import roc_curve  fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)  def plot_roc_curve(fpr, tpr, label=None, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0,1], [0,1], 'k--') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate (Recall)') plt.legend()  plot_roc_curve(fpr, tpr) plt.show()  No handles with labels found to put in legend.   dotted line represents purely random classifier good classifiers have the highest true positive rate with the lowest false positive rate (top left corner) can compare ROC scores of different classifiers by measuring area under the curve AUC a purely random classifier will have an AUC of 0.5 (think the integral of the purely random classifier, the linear line) false positive rate is so high here because there aren\u0026rsquo;t that many 5s in the data (only about 10%), so don\u0026rsquo;t get misled by the great looking ROC curve  from sklearn.metrics import roc_auc_score  roc_auc_score(y_train_5, y_scores)  0.9604938554008616   use the precision/recall curve when the positive class is rare or you care more about the false positives than the false negatives otherwise use the ROC curve for skewed datasets where the positive class is rare, you can see that precision really suffers when recall increases (a result of having less training data on the positive class, the classifier is trying to catch every positive class despite not having a lot to go off of). paints a different picture than ROC Try calculating ROC, AUC, Precision and Recall for Random Forest estimator:  from sklearn.ensemble import RandomForestClassifier  forest_clf = RandomForestClassifier(random_state=42)  y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba')  y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class  fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)  plot_roc_curve(fpr_forest, tpr_forest, label=\u0026quot;Random Forest\u0026quot;) plt.plot(fpr, tpr, \u0026quot;b:\u0026quot;, label=\u0026quot;SGD\u0026quot;) plt.legend() plt.show()   Closer to top left than SGD, more AUC, better performance Explaining the ROC curve: to achieve an almost 99% recall/TPR rate (we correctly predict positive for 99% of all the real positive samples in the dataset), it seems like we will have to accept the tradeoff of getting 15% of the negative samples incorrectly predicted as positive (the false positive rate)  roc_auc_score(y_train_5, y_scores_forest)  0.9983436731328145  y_preds_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)  precision_score(y_train_5, y_preds_forest)  0.9905083315756169  recall_score(y_train_5, y_preds_forest)  0.8662608374838591  Multiclass Classification  AKA multinomial Logistic Regression, Random Forest, naive Bayes classifiers are examples of algorithms that can make multiclass classifications natively Can get around this with algos that only work as binary; in the example of MNIST train 10 binary classifiers, pick the class with the highest decision score. Known as one-versus-rest/one-versus-all strategy (OvR). You could also train a binary classifier for every pair of digits. i.e. 1 vs 2, 1 vs 3, 2 vs 5, etc. This is one-versus-one (OvO) strategy, and you pick the classifier that wins the most duels. Advantage of this strategy is that you only train the classifiers on subsets of the entire target variable space Some algos scale poorly with the size of the training set, so OvO strategy is preferred In general however, OvR is preferred, as its more straightforward Sklearn classifiers will automatically detecty multiclass classifications from the target variable and will use a strategy based on the algorithm used  from sklearn.svm import SVC  # svm_clf = SVC() # svm_clf.fit(X_train, y_train) # all digits in target variable # svm_clf.predict([some_digit])    SVC uses OvO strategy; it actually trained 45 binary classifiers\n  NOTE: This takes forever to run. Sklearn can\u0026rsquo;t use GPUs to speed up training; GPUs are only useful for training deep learning models with architectures like Tensorflow or PyTorch\n  SVMs have a quadratic time complexity, calculating the distance between each point in the dataset: $$ O({n_{features}}\\times{n_{observations}^2}) $$\n  Caches common points but this kills memory regardless\n  This doesn\u0026rsquo;t scale well over a couple 10k features\n  decision_function() returns 10 scores per instance, and classifier picked the highest one:\n  # some_digit_scores = svm_clf.decision_function([some_digit]) # some_digit_scores  # np.argmax(some_digit_scores)  # svm_clf.classes[np.argmax(some_digit_scores)]   index just happened to match the class itself, but this is just luck Sklearn can be forced to use either OvO or OvR using OneVsOneClassifier or OneVsRestClassifier classes; just create an instance with the classifier you want passed as constructor  from sklearn.multiclass import OneVsRestClassifier  # ovr_clf = OneVsRestClassifier(SVC()) # ovr_clf.fit(X_train, y_train) # ovr_clf.predict([some_digit])  # len(ovr_clf.estimators_)   SGD Classifiers can directly classify instances into multiple classes Decision function returns a score per class  sgd_clf.fit(X_train, y_train) sgd_clf.predict([some_digit])  array([3], dtype=uint8)  sgd_clf.decision_function([some_digit])  array([[-31893.03095419, -34419.69069632, -9530.63950739, 1823.73154031, -22320.14822878, -1385.80478895, -26188.91070951, -16147.51323997, -4604.35491274, -12050.767298 ]])  cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.87365, 0.85835, 0.8689 ])   84% on all testing folds is decent - if you were to use a random classifier you\u0026rsquo;d get 10% accuracy simply scaling inputs can increase score:  from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))  cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.8983, 0.891 , 0.9018])  Error Analysis  can improve shortlisted models by analyzing the errors they make make predictions using the cross_val_predict() function, then call confusion_matrix():  y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) conf_mx = confusion_matrix(y_train, y_train_pred)  conf_mx  array([[5577, 0, 22, 5, 8, 43, 36, 6, 225, 1], [ 0, 6400, 37, 24, 4, 44, 4, 7, 212, 10], [ 27, 27, 5220, 92, 73, 27, 67, 36, 378, 11], [ 22, 17, 117, 5227, 2, 203, 27, 40, 403, 73], [ 12, 14, 41, 9, 5182, 12, 34, 27, 347, 164], [ 27, 15, 30, 168, 53, 4444, 75, 14, 535, 60], [ 30, 15, 42, 3, 44, 97, 5552, 3, 131, 1], [ 21, 10, 51, 30, 49, 12, 3, 5684, 195, 210], [ 17, 63, 48, 86, 3, 126, 25, 10, 5429, 44], [ 25, 18, 30, 64, 118, 36, 1, 179, 371, 5107]])   As a heatmap:  plt.matshow(conf_mx, cmap=plt.cm.gray) plt.xlabel(\u0026quot;Predicted\u0026quot;) plt.ylabel(\u0026quot;Actual Values\u0026quot;) plt.show()    Main diagonal: predicted rate of actual digit\n  Rows are actual classes, Columns are predicted classes\n  Lower rate could mean that less of that class in the dataset or lower performance on it\n  Can divide absolute prediction sums by total number of instances for each class to get error rates:\n  row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums np.fill_diagonal(norm_conf_mx, 0) # fill in diagonals to look at errors only plt.matshow(norm_conf_mx, cmap=plt.cm.gray) plt.xlabel(\u0026quot;Predicted\u0026quot;) plt.ylabel(\u0026quot;Actual Values\u0026quot;) plt.show()   Many different numbers get misclassified for 8s, but 8s themselves seem to get classified properly Confusion matrix isn\u0026rsquo;t symmetrical necessarily General confusion around 5s and 3s too If we wanted to fix the model, focusing on improving scores on 8s would be beneficial: Could collect more data on numbers that look like 8s but aren\u0026rsquo;t 8s Could add extra features, like writing an algorithm to count closed loops, or preprocessing the image to make some patterns like closed loops stand out more Analyzing individual errors is good too but time consuming  def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = mpl.cm.binary, **options) plt.axis(\u0026quot;off\u0026quot;)  cl_a, cl_b = 3, 5 X_aa = X_train[(y_train == cl_a) \u0026amp; (y_train_pred == cl_a)] X_ab = X_train[(y_train == cl_a) \u0026amp; (y_train_pred == cl_b)] X_ba = X_train[(y_train == cl_b) \u0026amp; (y_train_pred == cl_a)] X_bb = X_train[(y_train == cl_b) \u0026amp; (y_train_pred == cl_b)] plt.figure(figsize=(8,8)) plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5) plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5) plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5) plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)    row 1 col 1: 3s that were classified as 3s\n  row 1 col 2: 3s that were classified as 5s\n  row 2 col 1: 5s that were classified as 3s\n  row 2 col 2: 5s that were classified as 5s\n  Hard to understand why SGD classifier made the errors it did; as a linear model, it just assigned a weight per pixel and when it sees a new image it sums up the weighted pixel intensities to get a score for each class\n  3s are different from 5s mainly with the vertical line that connects the top horizontal line and the bottom arc; this means the classifier would be quite sensitive to image shifting and rotation\n  Could preprocess images to make sure they\u0026rsquo;re centered/not too rotated\n  Multilabel Classification  Assigning multiple labels to one sample E.g. detecting multiple people\u0026rsquo;s faces in a photo, or whether a digit is even or odd Outputs multiple binary tags. E.g. y_pred = [1, 0, 1] (yes to classes 1 and 3, no to 2)  from sklearn.neighbors import KNeighborsClassifier   Side note: % (Modulus) yields the remainder when the first operand is divided by the second  5 % 3 # 3 goes into 5 once, with remainder 2  2  10 % 3 # 3 goes into 10 three times, with remainder 1  1  y_train_large = (y_train \u0026gt;= 7) y_train_odd = (y_train % 2 == 1) y_multilabel = np.c_[y_train_large, y_train_odd]  y_multilabel  array([[False, True], [False, False], [False, False], ..., [False, True], [False, False], [ True, False]])   label 1: digit is 7 or above label 2: digit is odd  knn_clf = KNeighborsClassifier() knn_clf.fit(X_train, y_multilabel)  KNeighborsClassifier()   KNeighbors supports multilabel classification, though not all classifiers do  knn_clf.predict([some_digit]) # some_digit = 5  array([[False, True]])   One approach to measure multilabel performance is measure F1 score for each individual label and compute the average score across them:  y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)  f1_score(y_multilabel, y_train_knn_pred, average=\u0026quot;macro\u0026quot;)  0.976410265560605   this assumes that all labels are equally important, which might not always be the case can give labels weight according to number of appeareances in training data by setting average=\u0026quot;weighted\u0026quot;  Multioutput Classification  Generalization of multilabel classification where each label can be multiclass (i.e. have more than two possible values) following example denoises images by predicting what the pixel intensitiy should be for each pixel in a sample (multiple classes for multiple labels) this somewhat blurs the line between classification and regression (predicting pixel intensity is more of a regression task)  noise = np.random.randint(0, 100, (len(X_train), 784)) X_train_mod = X_train + noise noise = np.random.randint(0, 100, (len(X_test), 784)) X_test_mod = X_test + noise y_train_mod = X_train y_test_mod = X_test  def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.binary, interpolation=\u0026quot;nearest\u0026quot;) plt.axis(\u0026quot;off\u0026quot;)  knn_clf.fit(X_train_mod, y_train_mod)  KNeighborsClassifier()  some_index = np.random.randint(0, len(X_test_mod)) clean_digit = knn_clf.predict([X_test_mod[some_index]])  plt.figure(figsize=(8,4)) plt.subplot(121); plot_digit(X_test_mod[some_index]) plt.subplot(122); plot_digit(clean_digit)  Exercises in a Separate Notebook","date":"2020-09-26","permalink":"https://billwarker.com/posts/handson-ml-ch3/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 3 Notes"},{"content":"Notes Main Steps:\n Frame the problem Get data EDA (exploratory data analysis) Prepare the data for ML Model Selection Tune the model Present solution Launch, monitor, iterate  Look at the big picture  predict the median housing price for a district in CA what\u0026rsquo;s the business objective (not building a model for fun) ask: what is the current, non-ML solution? why can\u0026rsquo;t we use that start thinking about/designing the system  Pipelines\n sequence of data processing components typically runs asynchronously: When you execute something synchronously, you wait for it to finish before moving on to another task. When you execute something asynchronously, you can move on to another task before it finishes. components are self contained, downstream components can keep working for a while by just using the last output from the broken component (async)  Types of Regression\n multiple regression: multiple features to make a prediction univariate regression: only trying to predict a single value  Selecting a performance measure\n Root Mean Square Error (RMSE):  $$ RMSE(X,h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^{2}} $$\n gives higher weight to larger errors lowercase italic font for scalars, lowercase bold for vectors, uppercase bold for matrices if there are many outliers, then Mean Absolute Error (MAE) might be a better cost function:  $$ MAE(X,h) = \\frac{1}{m} \\sum_{i=1}^{m} \\lvert h(x^{(i)}) - y^{(i)}\\rvert$$\n different ways to measure difference between vectors RMSE is euclidian distance, $l_2$ norm computing sum of absolutes is $l_1$ norm, measures the distance between two points if you can only travel along orthogonal (perpendicular) lines $l_0$ just gives the number of non-zero elements in vector higher the norm index, the more it focuses on large values and neglects smaller ones. this is why RMSE is more sensitive to outliers than MAE  Verify Assumptions\n list and verify assumptions e.g. what does the output exactly need to be, what do we think is true about the problem/solution we\u0026rsquo;ve proposed  Get the Data  usually data is in DBs or spread across many files, so common first step is jumping through the hoops of access, getting used to schemas, legal precautions, etc. best to automate process of fetching data, future proof against changes  import os import tarfile from six.moves import urllib  DOWNLOAD_ROOT = \u0026quot;https://raw.githubusercontent.com/ageron/handson-ml/master/\u0026quot; HOUSING_PATH = os.path.join(\u0026quot;datasets\u0026quot;, \u0026quot;housing\u0026quot;) HOUSING_URL = DOWNLOAD_ROOT + \u0026quot;datasets/housing/housing.tgz\u0026quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.makedirs(housing_path) tgz_path = os.path.join(housing_path, \u0026quot;housing.tgz\u0026quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close()  fetch_housing_data()  import pandas as pd  def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(HOUSING_PATH, \u0026quot;housing.csv\u0026quot;) return pd.read_csv(csv_path)  housing = load_housing_data() housing.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY     housing.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total_rooms 20640 non-null float64 total_bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB  housing[\u0026quot;ocean_proximity\u0026quot;].value_counts()  \u0026lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64  housing.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value     count 20640.000000 20640.000000 20640.000000 20640.000000 20433.000000 20640.000000 20640.000000 20640.000000 20640.000000   mean -119.569704 35.631861 28.639486 2635.763081 537.870553 1425.476744 499.539680 3.870671 206855.816909   std 2.003532 2.135952 12.585558 2181.615252 421.385070 1132.462122 382.329753 1.899822 115395.615874   min -124.350000 32.540000 1.000000 2.000000 1.000000 3.000000 1.000000 0.499900 14999.000000   25% -121.800000 33.930000 18.000000 1447.750000 296.000000 787.000000 280.000000 2.563400 119600.000000   50% -118.490000 34.260000 29.000000 2127.000000 435.000000 1166.000000 409.000000 3.534800 179700.000000   75% -118.010000 37.710000 37.000000 3148.000000 647.000000 1725.000000 605.000000 4.743250 264725.000000   max -114.310000 41.950000 52.000000 39320.000000 6445.000000 35682.000000 6082.000000 15.000100 500001.000000      percentiles: what percentage of the data falls beneath this point. i.e. if 50th percentile is 100 for an attribute that means half of all the samples have a value less than 100 for that attribute if mean varies a lot from median that speaks to the presence of outliers pulling it up/down  %matplotlib inline import matplotlib.pyplot as plt  housing.hist(bins=50, figsize=(20,15)) plt.show()   Median income doesn\u0026rsquo;t seem to be expressed as USD Median house value and age seem to be capped features have different scales (needs feature scaling)  Create a test set\n before anything else, set aside test set this will avoid data snooping bias; i.e. fitting the model to better generalize on the test set  import numpy as np  def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices]  train_set, test_set = split_train_test(housing, 0.2)  len(train_set)  16512  len(test_set)  4128   this function will regenerate different sets on each run you can seed the random number generator, but this breaks if you add new data to the dataset (regenerates new train/test) eventually data you\u0026rsquo;ve trained on before will make it into the test set on multiple reruns with this pipeline can use each instance\u0026rsquo;s indentifier (assuming unique and immutable) to decide whether or not it should go in the test set code below computes a hash of each instance\u0026rsquo;s indentifier and puts that instance in the test set if hash is lower or equal to 20% of the maximum hash value. ensures consistency across multiple runs, and the test set will always contain 20% of the new data, but never any instance that was previously in the training set  from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) \u0026amp; 0xffffffff \u0026lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set]  housing_with_id = housing.reset_index() train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026quot;index\u0026quot;)  train_set   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   3 3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY   6 6 -122.25 37.84 52.0 2535.0 489.0 1094.0 514.0 3.6591 299200.0 NEAR BAY   ... ... ... ... ... ... ... ... ... ... ... ...   20635 20635 -121.09 39.48 25.0 1665.0 374.0 845.0 330.0 1.5603 78100.0 INLAND   20636 20636 -121.21 39.49 18.0 697.0 150.0 356.0 114.0 2.5568 77100.0 INLAND   20637 20637 -121.22 39.43 17.0 2254.0 485.0 1007.0 433.0 1.7000 92300.0 INLAND   20638 20638 -121.32 39.43 18.0 1860.0 409.0 741.0 349.0 1.8672 84700.0 INLAND   20639 20639 -121.24 39.37 16.0 2785.0 616.0 1387.0 530.0 2.3886 89400.0 INLAND    16512 rows × 11 columns\n   note: this approach looks fugazi, indices aren\u0026rsquo;t shuffled\n  the problem still exists but would need a better implementation than what\u0026rsquo;s here\n  if you use row index as a unique identifier, you must make sure that new data always gets appended to the end of the dataset and a row is never deleted\n  instead, you can try to engineer a unique ID for each row by combining some of the (ideally most stable/constant) features\n  e.g. a district\u0026rsquo;s latitude/longitude is guaranteed to be stable for a few million years lol:\n  housing_with_id['id'] = housing[\u0026quot;longitude\u0026quot;] * 1000 + housing[\u0026quot;latitude\u0026quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026quot;id\u0026quot;)  len(housing_with_id)  20640  len(housing_with_id[\u0026quot;id\u0026quot;].unique()) # also fugazi  12590   the approach above in the book obviously doesn\u0026rsquo;t work haha  from sklearn.model_selection import train_test_split  train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)  train_set   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     14196 -117.03 32.71 33.0 3126.0 627.0 2300.0 623.0 3.2596 103000.0 NEAR OCEAN   8267 -118.16 33.77 49.0 3382.0 787.0 1314.0 756.0 3.8125 382100.0 NEAR OCEAN   17445 -120.48 34.66 4.0 1897.0 331.0 915.0 336.0 4.1563 172600.0 NEAR OCEAN   14265 -117.11 32.69 36.0 1421.0 367.0 1418.0 355.0 1.9425 93400.0 NEAR OCEAN   2271 -119.80 36.78 43.0 2382.0 431.0 874.0 380.0 3.5542 96500.0 INLAND   ... ... ... ... ... ... ... ... ... ... ...   11284 -117.96 33.78 35.0 1330.0 201.0 658.0 217.0 6.3700 229200.0 \u0026lt;1H OCEAN   11964 -117.43 34.02 33.0 3084.0 570.0 1753.0 449.0 3.0500 97800.0 INLAND   5390 -118.38 34.03 36.0 2101.0 569.0 1756.0 527.0 2.9344 222100.0 \u0026lt;1H OCEAN   860 -121.96 37.58 15.0 3575.0 597.0 1777.0 559.0 5.7192 283500.0 \u0026lt;1H OCEAN   15795 -122.42 37.77 52.0 4226.0 1315.0 2619.0 1242.0 2.5755 325000.0 NEAR BAY    16512 rows × 10 columns\n  the above are random sampling methods which work well enough on a large dataset if the dataset is small then you should do stratified sampling (i.e. take steps to ensure that the sample is representative of the whole pop.) divide the data into different groups (strata) and randomly sample from those in a way which is representative e.g. stratify by median income:  # use pd.cut to bin the median income into categories housing['income_cat'] = pd.cut(housing['median_income'], bins=[0., 1.5, 3., 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])  housing['income_cat'].hist()  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc82c0cf90\u0026gt;  from sklearn.model_selection import StratifiedShuffleSplit  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing['income_cat']): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index]  # checking to see if it worked as expected strat_test_set[\u0026quot;income_cat\u0026quot;].value_counts()/len(strat_test_set)  3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64   the stratified sampling matches the proportions seen in the histogram; the test set in this instance is representative of the income_cat distribution found in the whole dataset now remove income_cat attribute to restore data to original form  for set_ in (strat_train_set, strat_test_set): set_.drop(\u0026quot;income_cat\u0026quot;, axis=1, inplace=True)  Discover and Visualize the Data to Gain Insights (EDA)  only explore the training set, pust the test set aside if training set is large, you might want to just explore a sample to make manipulations faster create a copy of the training set so you can play around without harming it  Visualizing Geographical Data:\nhousing = strat_train_set.copy()  housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc822c8990\u0026gt;   Can visualize density in the geographical viz with lower alpha  housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=housing['population']/100, label='population', figsize=(10,7), c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True) plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7fcc80e608d0\u0026gt;   prices are higher along the coast (duh), with some hotspots in the bay area and around LA could use a clustering algo to find many clusters, and then create features that measure proximity to the cluster centers Ocean proxmity is useful feature  Look for correlations:\n standard correlation coefficient only measures linear relationships, misses any other kinds  corr_matrix = housing.corr()  corr_matrix['median_house_value'].sort_values(ascending=False)  median_house_value 1.000000 median_income 0.687160 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population -0.026920 longitude -0.047432 latitude -0.142724 Name: median_house_value, dtype: float64  from pandas.plotting import scatter_matrix  attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age'] scatter_matrix(housing[attributes], figsize=(12,8)) plt.show()   diagonal lines are histograms of attributes  # focus on median income housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc83a74110\u0026gt;   clear correlation can also see the cap at $500k quite clearly other less visible lines at \\$350K, $450K; might want to remove from data to stop model from picking up on quirks  Experimenting with Attribute Combinations\n EDA shows some interesting correlations so far some attributes are tail heavy, so could transform to normal distribution by taking their log can take combinations of features to get relationships; some attributes make less sense when considered independently e.g. compute avg. rooms per household as rooms/households  housing['rooms_per_household'] = housing['total_rooms']/housing['households'] housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms'] housing['population_per_household'] = housing['population']/housing['households']  # check corr matrix again corr_matrix = housing.corr() corr_matrix['median_house_value'].sort_values(ascending=False)  median_house_value 1.000000 median_income 0.687160 rooms_per_household 0.146285 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population_per_household -0.021985 population -0.026920 longitude -0.047432 latitude -0.142724 bedrooms_per_room -0.259984 Name: median_house_value, dtype: float64   bedrooms per room is much more correlated (negatively) to median house value than total rooms/bedrooms houses with a lower bedroom/room ratio seem to be more expensive EDA doesn\u0026rsquo;t need to be completely thorough, point is to get insights that will get you a reasonably decent prototype iterative process  Prepare the Data for Machine Learning Algorithms write functions to prepare data because:\n can reproduce on fresh data build a library of transformation functions can use in live system easier experimentation  housing = strat_train_set.drop(\u0026quot;median_house_value\u0026quot;, axis=1) housing_labels = strat_train_set[\u0026quot;median_house_value\u0026quot;].copy()  Handling Missing Values\n 3 options: get rid of of corresponding samples get rid of the entire attribute impute values to (mean, median, etc)  from sklearn.impute import SimpleImputer  imputer = SimpleImputer(strategy=\u0026quot;median\u0026quot;)  # create copy of numerical features housing_num = housing.drop(\u0026quot;ocean_proximity\u0026quot;, axis=1)  imputer.fit(housing_num) imputer.statistics_  array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])  X = imputer.transform(housing_num)  housing_tr = pd.DataFrame(X, columns=housing_num.columns)  Handling Text and Categorical Attributes\n encode to numerical values  housing_cat = housing[[\u0026quot;ocean_proximity\u0026quot;]]  from sklearn.preprocessing import OrdinalEncoder  ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)  housing_cat_encoded[:5]  array([[0.], [0.], [4.], [1.], [0.]])  ordinal_encoder.categories_  [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]   ordinal encoding places ranking on the attributes, so some algos might interpret nearby values as similar (4\u0026amp;3 are more similar than 1\u0026amp;7, although these are all just arbitrary indices) for nominal categories its better to one-hot encode them; create a binary attribute per category also known as creating dummy variables  from sklearn.preprocessing import OneHotEncoder  cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat)  housing_cat_1hot  \u0026lt;16512x5 sparse matrix of type '\u0026lt;class 'numpy.float64'\u0026gt;' with 16512 stored elements in Compressed Sparse Row format\u0026gt;   sparse matrices don\u0026rsquo;t use memory to store zero elements  cat_encoder.categories_  [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]   if a categorical attribute has a large number of categories, one hot encoding will create a large number of input features and may hurt performance. in this case embeddings are useful (denser representations)  Custom Transformers\n write to align with Scikit-Learn so you can use pipelines add hyperparameters to gate any data preparation steps you aren\u0026rsquo;t sure about  from sklearn.base import BaseEstimator, TransformerMixin  rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6  class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, population_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household]  attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs = attr_adder.transform(housing.values)  np.c_ stacks arrays on their last axis (turns column vectors into a matrix)\nnp.c_[np.array([1,2,3]), np.array([4,5,6])]  array([[1, 4], [2, 5], [3, 6]])  Feature Scaling:\n  ML algos don\u0026rsquo;t perform well when features have different scales\n  Two ways to feature scale:\n  min-max scaling: attributes are shifted and rescaled so they range from 0-1. $$x_{scaled} = \\frac{x_n - x_{min}}{x_{max} - x_{min}}$$\n  standardization: subtract the mean and divide by standard deviation\n  standardization is less affected by outliers in the data $$x_{scaled} = \\frac{x_n - {\\bar{x}}}{\\sigma}$$\n  fit all transformers to the training set, then use them on the test\n  Transformation Pipelines\nfrom sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler  num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy=\u0026quot;median\u0026quot;)), ('attribs_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()) ])  housing_num_tr = num_pipeline.fit_transform(housing_num)   calls fit_transform() on all of the transformers sequentially  from sklearn.compose import ColumnTransformer  num_attribs = list(housing_num) cat_attribs = [\u0026quot;ocean_proximity\u0026quot;] full_pipeline = ColumnTransformer([ (\u0026quot;num\u0026quot;, num_pipeline, num_attribs), (\u0026quot;cat\u0026quot;, OneHotEncoder(), cat_attribs), ])  housing_prepared = full_pipeline.fit_transform(housing)  Select and Train a Model Training and Evaluating on the Training Set\n preprocessing steps make training models simple  from sklearn.linear_model import LinearRegression  lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels)  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)  some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(f\u0026quot;Predictions: {lin_reg.predict(some_data_prepared)}\u0026quot;) print(f\u0026quot;Labels: {list(some_labels)}\u0026quot;)  Predictions: [211944.80589799 321295.84907457 210851.33029021 62359.51850965 194954.19182968] Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]  from sklearn.metrics import mean_squared_error  housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) print(lin_rmse)  68898.54780411992   error of \\$68K here isn\u0026rsquo;t very satisfying (housing prices range from \\$120K to \\$265K) if a model is underfitting the data it means that the features don\u0026rsquo;t provide enough information to make good decisions, or model is too weak  from sklearn.tree import DecisionTreeRegressor  tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels)  DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')  housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse  0.0  Better Evaluation with Cross Validation\n K-fold Cross Validation splits the training set into K folds, training the model K times by testing on one of the folds and composing the training set on the remaining K-1 Sklearns CV feature expects a utility function (greater is better) as opposed to a cost function (lower is better), so the scoring function is actually opposite of MSE, which is why the following code uses -scores in the square root  from sklearn.model_selection import cross_val_score  scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) tree_rmse_scores = np.sqrt(-scores)  def display_scores(scores): print(\u0026quot;Scores:\u0026quot;, scores) print(\u0026quot;Mean:\u0026quot;, scores.mean()) print(\u0026quot;Standard Deviation\u0026quot;, scores.std())  display_scores(tree_rmse_scores)  Scores: [67971.85204287 67101.92229431 69341.35567834 66956.22248918 69739.80377843 72468.59865874 66736.84144169 67241.24548885 72220.02744352 70187.38922156] Mean: 68996.52585375118 Standard Deviation 2037.77518366374   CV not only allows you to get an averaged estimate of model\u0026rsquo;s performance, but allows you to get an idea of how precise this estimate is through the standard deviation  lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores)  Scores: [67500.31361237 68404.48325957 68239.95757613 74813.56736728 68419.88576794 71632.92651865 65216.31837467 68702.06708289 71793.11060978 68131.30099374] Mean: 69285.3931163006 Standard Deviation 2576.7108344336184  from sklearn.ensemble import RandomForestRegressor  forest_reg = RandomForestRegressor() forest_reg.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. \u0026quot;10 in version 0.20 to 100 in 0.22.\u0026quot;, FutureWarning) RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  forest_mse = mean_squared_error(housing_labels, forest_reg.predict(housing_prepared)) forest_rmse = np.sqrt(forest_mse) print(forest_rmse)  21923.44698175341  forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores)  display_scores(forest_rmse_scores)  Scores: [50985.57169248 49600.72682606 50759.78939086 54557.52337524 51089.83837069 55891.91762608 51010.98084994 50261.62522741 55934.1764328 52825.16027951] Mean: 52291.73100710661 Standard Deviation 2239.368467655192   Forest looks more promising (lower RMSE), but the sizeable difference between CV performance and training set performance indicates overfitting on the training set Could simplify it, regularize it, or get more training data Ideally you want to shortlist 2-5 models promising models total can use joblib library to save promising models for experimentation/later iterations  Fine-Tune Your Model  fine tune the shortlist of models  Grid Search\n grid search will automate cross-validation on all combinations of hyperparameters specified  from sklearn.model_selection import GridSearchCV  param_grid = [ {'n_estimators': [3,10,30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]}, ]  forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) GridSearchCV(cv='warn', error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid='warn', n_jobs=None, param_grid=[{'max_features': [2, 4, 6, 8], 'n_estimators': [3, 10, 30]}, {'bootstrap': [False], 'max_features': [2, 3, 4], 'n_estimators': [3, 10]}], pre_dispatch='2*n_jobs', refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=0)   try powers of 10 if you have no clue what a hyperparameter should have different dictionaries in param_grid specify separate grid searches  grid_search.best_params_  {'max_features': 8, 'n_estimators': 30}  grid_search.best_estimator_  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)   refit=True argument for GridSearchCV will retrain on the entire training set after finding best estimator hyperparams  cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[\u0026quot;mean_test_score\u0026quot;], cvres[\u0026quot;params\u0026quot;]): print(np.sqrt(-mean_score), params)  65307.87885616995 {'max_features': 2, 'n_estimators': 3} 56745.41295641033 {'max_features': 2, 'n_estimators': 10} 52896.1050644663 {'max_features': 2, 'n_estimators': 30} 59681.45476015563 {'max_features': 4, 'n_estimators': 3} 52735.85674503226 {'max_features': 4, 'n_estimators': 10} 50892.22744742235 {'max_features': 4, 'n_estimators': 30} 59799.42942971745 {'max_features': 6, 'n_estimators': 3} 52650.77632649698 {'max_features': 6, 'n_estimators': 10} 50570.40999405222 {'max_features': 6, 'n_estimators': 30} 59094.68896450388 {'max_features': 8, 'n_estimators': 3} 52492.707418330094 {'max_features': 8, 'n_estimators': 10} 50432.33406200777 {'max_features': 8, 'n_estimators': 30} 62530.835324885855 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 55183.05939812397 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 60507.19412492283 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 53097.84240758412 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 59391.108929740585 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 52293.331430755774 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}   can include data preparation steps as hyperparameters in grid search  Randomized Search\n when hyperparameter search space is large, randomized usually gets comparable results faster explores $n$ different values for each hyperparameter for $n$ total iterations gives you more control over computing budget  Ensemble Methods\n Can combine models that perform best Will often perform better than the best individual model, especially if the models make different types of errors  Analyze the Best Models and their Errors\n Can gain insights by inspecting best models Random Forest can give feature importances:  feature_importances = grid_search.best_estimator_.feature_importances_  extra_attribs = [\u0026quot;rooms_per_hhold\u0026quot;, \u0026quot;pop_per_hhold\u0026quot;, \u0026quot;bedrooms_per_room\u0026quot;] cat_encoder = full_pipeline.named_transformers_[\u0026quot;cat\u0026quot;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True)  [(0.3938202057578966, 'median_income'), (0.15133729393112902, 'INLAND'), (0.1029603450675752, 'bedrooms_per_room'), (0.0754751387108862, 'pop_per_hhold'), (0.06620720663844283, 'longitude'), (0.06338183338235212, 'latitude'), (0.04206999831411881, 'housing_median_age'), (0.03242004947783398, 'rooms_per_hhold'), (0.015482413139961633, 'total_bedrooms'), (0.014766900750197388, 'population'), (0.014573461291699393, 'total_rooms'), (0.014411203134276336, 'households'), (0.006778298384461372, '\u0026lt;1H OCEAN'), (0.0032380720741344592, 'NEAR OCEAN'), (0.002983526044293852, 'NEAR BAY'), (9.40539007408315e-05, 'ISLAND')]   could drop less important features and try retraining could also analyze the specific errors the system makes and try to understand why they\u0026rsquo;re occuring and what could fix (extra features, cleaning outliers, removing uninformative features)  Evaluate Your Sytem on the Test Set  just apply pipeline to the test set (run transform, not fit_transform - we do not want to fit it to the training set!)  final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(\u0026quot;median_house_value\u0026quot;, axis=1) y_test = strat_test_set[\u0026quot;median_house_value\u0026quot;].copy()  X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared)  final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse)  print(final_rmse)  48026.91757298586   in some situations a point estimate of the generalization error will not be enough evidence to launch you can get an idea of precision by computing a 95% confidence interval for the generalization error:  from scipy import stats  confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))  array([46060.77003181, 49915.67977301])   performance will likely be a bit worse than what was measured in cross-validation resist temptation to tweak numbers on the test set present solution, highlighting assumptions, methodology, system limitations create presentations with clear visuals and keep points summarized  Launch, Monitor, and Maintain Your System   launch in prod, polishing code, writing tests and documentation, etc\n  can deploy model by loading it in production environment or wrapping it in a REST api (i.e. cloud functions, microservices)\n  this offers easier updating of system without disrupting main application, and can handle load balancing\n  can deploy in cloud (GCP)\n  need to write monitoring code after deployment to make sure performance doesn\u0026rsquo;t rot over time\n  models decay as the world changes and old data becomes less relevant, need to feed with new data\n  model performance can be inferred from downstream metrics (e.g. how many sales your recommender system is generating)\n  can use human testers to verfiy model outputs (experts, or crowdsourced from platforms like Amazon Mechanical Turk)\n  putting models in prod and maintaining them can be more work than actually creating them\n  automate as much of the process as possible:\n  collecting fresh data regularly and labelling (e.g. using human raters)\n  writing scripts to train the model and fine-tune hyperparameters automatically\n  writing scripts to evaluate new model vs. old model on updated test sets, and deploying the new model if performance has not decreased\n  monitor data quality as well, poor quality data leaking in can be hard to detect\n  create backups of every model and have infrastructure to rollback previous models quickly, in case new ones start failing badly for some reason\n  backups help for investigation/comparison\n  keep backups of datasets as well\n  Exercises  Try a Support Vector Machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\u0026quot;linear\u0026rdquo; (with various values for the C hyperparameter) or kernel=\u0026quot;rbf\u0026rdquo; (with various values for the C and gamma hyperparameters). Don’t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?  from sklearn.svm import SVR  svr = SVR() param_grid = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1.0, 10.0, 100.0]}, {'kernel': ['rbf'], 'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'gamma': ['scale', 'auto']} ] grid_search = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) GridSearchCV(cv='warn', error_score='raise-deprecating', estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False), iid='warn', n_jobs=None, param_grid=[{'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'kernel': ['linear']}, {'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'gamma': ['scale', 'auto'], 'kernel': ['rbf']}], pre_dispatch='2*n_jobs', refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=0)  best_svr = grid_search.best_estimator_  best_svr  SVR(C=100.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated', kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)  np.sqrt(-grid_search.best_score_)  72427.44454616884  Try replacing GridSearchCV with RandomizedSearchCV.  from sklearn.model_selection import RandomizedSearchCV from scipy.stats import uniform from scipy.stats import norm from pprint import pprint  random_forest = RandomForestRegressor()  print(\u0026quot;Parameters currently in use:\\n\u0026quot;) pprint(random_forest.get_params())  Parameters currently in use: {'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}  random_forest = RandomForestRegressor() n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)] max_features = ['auto', 'sqrt'] max_depth = [int(x) for x in np.linspace(10,110, num=11)] max_depth.append(None) min_samples_split = [2, 5, 10] min_samples_leaf = [1,2,4] bootstrap = [True, False] random_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap} random_search = RandomizedSearchCV(estimator=random_forest, param_distributions=random_grid, scoring='neg_mean_squared_error', verbose=1, n_iter=10, cv=3, n_jobs=-1, return_train_score=True)  random_search.fit(housing_prepared, housing_labels)  Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 31.4min finished RandomizedSearchCV(cv=3, error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_sta... param_distributions={'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=1)  best_forest = random_search.best_estimator_  best_forest  RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=110, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  np.sqrt(-random_search.best_score_)  49359.61219717077  Try adding a transformer in the preparation pipeline to select only the most important attributes.  class FeatureImportancesFilter(BaseEstimator, TransformerMixin): def __init__(self, feature_importances, importance_cutoff = 0.9): # no *args or **kargs self.feature_importances = feature_importances feature_indices = [i for i in range(len(self.feature_importances))] self.sorted_feature_importances = sorted(zip(self.feature_importances, feature_indices), reverse=True) self.importance_cutoff = importance_cutoff def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): importance_sum = 0 included_features = list() for importance, ix in self.sorted_feature_importances: if not importance_sum \u0026gt;= self.importance_cutoff: importance_sum += importance included_features.append(ix) else: break return X[:, included_features]   note: might need to fit to actual random forest regressor, or breaks if input shape changes  housing_prepared.shape  (16512, 16)  importance_filter = FeatureImportancesFilter(feature_importances, importance_cutoff=0.9)  importance_filter.transform(housing_prepared).shape  (16512, 8)  Try creating a single pipeline that does the full data preparation plus the final prediction.  e2e_pipeline = Pipeline([ ('data_prep', full_pipeline), ('filter', FeatureImportancesFilter(feature_importances)), ('clf', best_forest) ])  Automatically explore some preparation options using GridSearchCV.  e2e_grid_params = {'filter__importance_cutoff': (0.5, 0.6, 0.65)}  e2e_grid_search = GridSearchCV(e2e_pipeline, e2e_grid_params, scoring='neg_mean_squared_error', verbose=1, cv=3)  e2e_grid_search.fit(housing, housing_labels)  Fitting 3 folds for each of 3 candidates, totalling 9 fits [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 9 out of 9 | elapsed: 1.4min finished GridSearchCV(cv=3, error_score='raise-deprecating', estimator=Pipeline(memory=None, steps=[('data_prep', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3, transformer_weights=None, transformers=[('num', Pipeline(memory=None, steps=[('imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy='median'... min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False), iid='warn', n_jobs=None, param_grid={'filter__importance_cutoff': (0.5, 0.6, 0.65)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring='neg_mean_squared_error', verbose=1)  np.sqrt(-e2e_grid_search.best_score_)  66661.60437351998  e2e_grid_search.best_params_  {'filter__importance_cutoff': 0.65} ","date":"2020-08-17","permalink":"https://billwarker.com/posts/handson-ml-ch2/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 2 Notes"},{"content":"Notes Types of ML\n  ML: algorithms learning from data and improving performance on a task\n  advantage over rule based systems is that machine can update parameters/logic with new/more data (refreshing the model)\n  applying ML to discover/understand patterns in the data: data mining\n  Supervised learning: training a model with labelled examples. can be used for classification tasks (predict a discrete category) or regression (predict a continuous number)\n  Unsupervised learning algos: clustering unlabelled data, visualizing and dimensionality reduction, association rule learning\n  hierarchical clustering: sub-dividing clusters into smaller groups\n  dimensionality reduction: simplify data without losing too much information. i.e. merging correlated features. can help performance, takes up less disk and memory space\n  anomoly detection\n  association rule learning: relations between attributes (similar to data mining)\n  Semi-supervised learning. i.e. Google Photos, looks at unlabelled pictures to find common faces. once given a label for a person it can name everyone in the photo. mixed hierarchical models\n  Reinforcement learning: agent learns its environment and selects a policy (actions, strategy) to optimize some reward\n  Batch vs. Online Learning\n  Batch: training a model on all available data. train the system and then put it in prod. retrain new versions of the model with new data\n  requires a lot of computing resources, can be expensive. not suitable for autonomous systems with limited space for data (i.e. Mars rover)\n  Online: train system incrementally on mini-batches of data\n  great for continuous flows of data or limited storage resources\n  can be used to train on datasets that won\u0026rsquo;t fit in memory (out-of-core learning)\n  learning rate: how fast model adapts to new data\n  too high, forgets old data rapidly, too low, system has inertia (also less sensitive to noisey data)\n  need to monitor online systems if garbage data starts coming in\n  Instance-Based vs. Model-Based Learning\n  how a system generalizes (i.e. answers examples its never seen before)\n  Instance-based: generalizes to new examples by comparing similarity to training examples\n  i.e. KNN\n  Model-based: builds a model to predict new data\n  select a model (i.e. linear model) to represent the data\u0026rsquo;s pattern\n  tune model on a utility or cost function\n  if a model doesn\u0026rsquo;t generalize well, you can try again with better quality training data, more features, or a stronger model (e.g. polynomial vs. basic linear)\n  adding more data tends to get better results on all kinds of algos, to the point where performance can be identical with enough data\n  data needs to be representitive of the problem space trying to model, more data can eliminate noise but procedure needs to be solid or it risks sampling bias\n  Feature Engineering\n feature selection, picking the most useful features feature extraction, combining existing features to produce more useful ones + dimensionality reduction creating new features with the intro of new data  Performance\n  overfitting: performing well on the training data but not generalizing well\n  machine learning can pick up on noise in the data and sometimes even irrelevant features/useless metadata (i.e. data ids/index), detecting false patterns\n  overfitting happens when the model is too complex relative to the amount and noiseness of the data\n  regularization is constraining a model to make it simpler can help overfitting\n  controlled by model hyperparameters (knobs to tweak on the model itself, such as learning rate)\n  underfitting is opposite problem, model is too simple\n  fix it with a more complex model, more/better features, reducing regularization constraints\n  test models to see if they generalize well\n  split data into training and test sets to get error rate on new cases (out of sample/generalization error)\n  if training error is now but oos error is high, overfitting\n  use a third validation set to compare models/hyperparameters, then select the best one and use it one the test\n  cross validation splits the training set into subsets which are used for validation\n  need to make assumptions about the data to pick models reasonably\n  Questions How would you define ML?\n Algorithms that allow a computer to learn from data to improve on a task and generalize to new examples well  Four types of problems where it shines?\n Problems that traditionally require too many rules or hand-tuning Complex problems with no easy logic based model Problems where underlying strategy/solutions change over time, so new model can help adapt Data mining, learning underlying patterns in data  What is a labelled training set?\n Data that has the variable of interest classified (independent variable), and information about it in other attribures (dependent variables)  Two most common supervised tasks?\n Regression: predicting a continuous number for the target variable Classification: predicting a discrete category of target variable  Four kinds of unsupervised tasks?\n Clustering: creating groups in unstructured data Dimesionality Reduction: reducing the number of attributes in the training set while keeping most of the variance(underlying signal) Anomoly Dection: finding outliers Visualization Association Rule learning: data mining, learning patterns in the data  What type of ML algo would you use to allow a robot to walk in various unknown terrains?\n Reinforcement Learning  What type of algo would you use to segment your customers into multiple groups?\n Clustering, Unsupervised Learning  Would you frame the problem of spam detection as a supervised learning problem?\n Supervised; we can use examples of spam and ham (labelled training data) to create a model that identifies which is which  What is an online learning system?\n A system that can update with new data as it comes in, ingesting as it comes in through mini-batches  What is an out-of-core learning system?\n Using online learning to train the model on a dataset that wouldn\u0026rsquo;t fit inside the computers memory if you tried to train it in one giant batch.  What type of learning algo relies on similarity measures to make predictions?\n Instanced-based models, i.e. KNN  Difference between a model parameter and a hyperparameter?\n A model parameter is the coefficient determined by the algorithm to apply to a attribute/feature in the data when making predictions, a hyperparameter is an aspect of the model that you can adjust to change how it is trained  What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n These algos look to fit the model to the data (i.e. represent the problem with some simplified version of it). The strategy is to improve performance in respect to some cost/utility function. They make new predictions by applying the policy/parameters determined through training on the new data\u0026rsquo;s features  Four main challenges in ML?\n Not enough data Non-representative data Poor quality data Overfitting/Underfitting  If a model performs well on the training data but generalizes poorly to new examples, what is happening? 3 possible solutions?\n Model is overfitting to the training data and can\u0026rsquo;t generalize well to the new examples You can regularize the model (i.e. constrain it) to be less representative of the training data Train the model on more data/more representative data Tune the model on a validation set  What is a test set and why would you want to use it?\n Test the performance of the model on new examples to understand how well it generalizes to new data (i.e. the whole point)  What can go wrong if you tune hyperparameters on the test set?\n You fit the model to work well on the testing set, overfitting it and reducing the chance of generalizing well  What is cross validation, why is it better than a validation set?\n Cross validation takes different chunks of the training data and uses them iteratively to train and validate the model, creating a more robust model (training on different samples) and is a more economic use of data. ","date":"2020-08-02","permalink":"https://billwarker.com/posts/handson-ml-ch1/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 1 Notes"},{"content":"The Standard Error of the Mean The Standard Error of the Mean ($SE$) is the standard deviation of the sample distribution of the sample mean. To understand what this means, let\u0026rsquo;s break that sentence down in reverse order (i.e. chronologically):\nSample Mean: we have some probability density function $P$ for a population. We take a sample of $N$ instances from it and calculate our statistic of interest - in this case the mean, $\\bar{x}$. We have to take samples because it is hard/impossible to look at every instance in an entire population to calculate the true mean $\\mu$ hidden to us in nature (i.e. say we were interested in the average weight of all monkeys on the planet).\nSample Distribution: we take many samples (the number of which denoted by $M$) from the population\u0026rsquo;s probability density function $P$ and calculate the sample mean $\\bar{x}$ for each one. All of these sample means can be lumped together into a distribution $S$, which approximates a normal distribution the higher $M$ is due to the Central Limit Theorum. We want to create a sampling distribution because it allows us to reason about the likelihood of different values the sample mean can be. By doing so, we can look at the mean of this distribution and conclude that its the most likely value for $\\mu$ given the data we\u0026rsquo;ve used.\nStandard Deviation: The standard deviation is a metric that describes the dispersion of a dataset/distribution in relation to its mean. It speaks to how close/far the density of the distribution is spread in relation to its mean. The standard deviation of a sample distribution - our standard error $SE$ - is an indication of how representative the distribution is of our true mean $\\mu$. The smaller it is, the more representative it can be said to be; the larger it is, the harder it is to trust.\nTLDR: The Standard Error matters because it allows us to better understand how representative our sampling distribution is (i.e. our model of the true mean).\n$SE$ can be calculated with this formula:\n$$SE = \\frac{\\sigma}{\\sqrt{N}}$$\nwhere ${\\sigma}$ is the standard deviation of the population\u0026rsquo;s probability density function $P$ and $N$ is the number of instances in a sample.\nUnderstanding the Standard Error through Simulation import numpy as np import scipy as sp import pandas as pd import matplotlib.pyplot as plt  # create a random probability distribution function to model our population # in this case a Maxwell continuous random variable (picked randomly) x = np.linspace(sp.stats.maxwell.ppf(0.01), sp.stats.maxwell.ppf(0.99), 100) plt.plot(x, sp.stats.maxwell.pdf(x), 'b-', lw=5, alpha=0.6, label='maxwell pdf') plt.show() mean, var, skew, kurt = sp.stats.maxwell.stats(moments='mvsk') print(f'The true mean of the population is {mean} and its standard deviation is {var**(1/2)}')  The true mean of the population is 1.5957691216057308 and its standard deviation is 0.6734396116428514  # to create our sampling distribution S we take M samples of N instances each # we calculate the mean of each sample and add it to a list, which we can make a histogram with m_samples = 5000 n_instances = 250 sample_means = [] for m in range(m_samples): sample = sp.stats.maxwell.rvs(size=n_instances) # draw a sample from the population sample_means.append(sample.mean()) # add it to our sampling distribution sample_dist = pd.Series(sample_means) # our sample distribution as a pandas series  sample_dist.hist(bins=30) # visualizing our sample distribution of the sample mean  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1341e8fd0\u0026gt;  print(f\u0026quot;The mean of our sample distribution is {sample_dist.mean()}\u0026quot;) print(f\u0026quot;Its standard deviation (The Standard Error) is {sample_dist.std()}\u0026quot;)  The mean of our sample distribution is 1.5960355829136528 Its standard deviation (The Standard Error) is 0.04222284486122923  # calculating the same SE from the formula above se = np.sqrt(var)/np.sqrt(n_instances) print(f\u0026quot;The standard error calculated with the formula is: {se}\u0026quot;)  The standard error calculated with the formula is: 0.042592060787413163 ","date":"2020-07-12","permalink":"https://billwarker.com/posts/standard-error-of-the-mean/","tags":["stats"],"title":"The Standard Error of the Mean"},{"content":"Permutations of a Set Permutations of a set are particular orderings of its elements. To calculate the number of permutations (i.e. the possible orderings) of a subset $k$ distinct elements from a set of size $n$, the formula is:\n$$N(n, k) = \\frac{n!}{(n-k)!}$$\n$n!$ represents all orderings for every element in the set. $(n-k)!$ represents the orderings of the elements we\u0026rsquo;re not including in the subset of $k$ elements - dividing by this number allows us to factor them out of $n!$ in the numerator and give us the number of permutations.\nimport itertools import math S = ['A', 'B', 'C', 'D'] permutations = itertools.permutations(S, 2) # P(4,2) for p in permutations: print(p)  ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'A') ('B', 'C') ('B', 'D') ('C', 'A') ('C', 'B') ('C', 'D') ('D', 'A') ('D', 'B') ('D', 'C')  def permutations(n, k): return int(math.factorial(n)/math.factorial(n-k)) permutations(4, 2)  12  Combinations of a Set Combinations don\u0026rsquo;t care about order - they are just the different subsets of elements in the set. To calculate the number of combinations (i.e. subsets) of $k$ distinct elements from a set of size $n$, the formula is:\n$$C(n, k) = \\frac{n!}{k!(n-k)!}$$\nAn easy way to understand combinations is in relation to permutations - basically, we are calculating the number of permutations of a subset created by $P(n, k)$ and then just dividing that number by the possible ways to order its elements (since combinations don\u0026rsquo;t care about that).\n$$C(n, k) = \\frac{P(n, k)}{k!} = \\frac{n!}{(n-k)!}*\\frac{1}{k!}$$\nS = ['A', 'B', 'C', 'D'] combinations = itertools.combinations(S, 2) # P(4,2) for c in combinations: print(c)  ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'C') ('B', 'D') ('C', 'D')  def combinations(n, k): return int(permutations(n, k)/math.factorial(k)) combinations(4, 2)  6 ","date":"2020-07-07","permalink":"https://billwarker.com/posts/permutations-and-combinations/","tags":["stats"],"title":"Permutations and Combinations"}]