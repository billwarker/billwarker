[{"content":"Part 3 of 3 on a series of notes covering margin of error, power, and sample size calculations.\nNotes, with questions and examples, taken from the following reading: https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html\nEnsuring a Test has High Power Power values of 80-90% are commonly accepted as norms when running hypothesis tests. Certain power levels can guaranteed in studies by including enough samples to control the variability in the parameter of interest.\nThe input for the sample size formulas include desired power, the level of significance, and the effect size. Effect size is selected to represent a meaningful or practically important difference in the parameter of interest.\nThe formulas below produce minimum sample sizes to ensure that their associated flavours of hypothesis tests will have a specified probability of rejecting the null hypothesis when it is false (i.e. power). Additionally, for certain studies one may need to factor in the likelihood of attrition or lose in the samples afterwards.\nSample Size for One Sample, Continuous Outcome In a hypothesis test comparing the mean of a continuous normal variable in a single population to a known mean, the hypotheses are:\n$$ H_0: \\mu = \\mu_0 $$ $$ H_A: \\mu \\neq \\mu_0 $$\nWhere $\\mu_0$ is the known mean (e.g. historical control).\nThe formula for determining sample size to ensure the test has a specified power is:\n$$ n = \\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\n $\\alpha$ is the selected level of significance and $Z_{1-\\alpha/2}$ is the value from the standard normal distribution holding $1-\\alpha/2$ below it $1-\\beta$ is the selected power, and $Z_{1-\\beta}$ is the value from the standard normal distribution holding $1-\\beta$ below it  For 80% power, this associated Z value is $Z_{0.80} = 0.84$. For 90% power, it is $Z_{0.90} = 1.282$.\nfrom scipy.stats import norm import numpy as np  norm.ppf(0.8)  0.8416212335729143  norm.ppf(0.9)  1.2815515655446004  $ES$ is the effect size, defined as follows:\n$$ ES = \\frac{\\lvert\\mu_1 - \\mu_0\\rvert}{\\sigma} $$\n $\\mu_0$ is the mean under $H_0$ $\\mu_1$ is the mean under $H_1$ $\\sigma$ is the standard deviation of the outcome of interest  The numerator of the effect size is the absolute difference in means, $\\lvert\\mu_1 - \\mu_0\\rvert$, representing what is considered a meaningful or important difference in the population.\nIt can be difficult to underestimate $\\sigma$ at the outset of a test - in sample size calculations it is common to use a value from a previous study or a study performed on a comparable population. Regardless, $\\sigma$ should always be conservative (i.e. reasonably large), so that the resultant sample size isn\u0026rsquo;t too small.\nExample  An investigator hypothesizes that in people free of diabetes, fasting blood glucose, a risk factor for coronary heart disease, is higher in those who drink at least 2 cups of coffee per day. A cross-sectional study is planned to assess the mean fasting blood glucose levels in people who drink at least two cups of coffee per day. The mean fasting blood glucose level in people free of diabetes is reported as 95.0 mg/dL with a standard deviation of 9.8 mg/dL.7 If the mean blood glucose level in people who drink at least 2 cups of coffee per day is 100 mg/dL, this would be important clinically. How many patients should be enrolled in the study to ensure that the power of the test is 80% to detect this difference? A two sided test will be used with a 5% level of significance.  two_cups_glucose = 100.0 mean_glucose = 95.0 std_glucose = 9.8 effect_size = (two_cups_glucose - mean_glucose)/std_glucose effect_size  0.5102040816326531  The effect size represents a meaningful standardized difference in the population mean - 95 mg/dL vs. 100 mg/dL, or 0.51 standard deviation units different.\nZ_significance = norm.ppf(1 - (0.05/2)) beta = 0.2 Z_power = norm.ppf(1 - beta) n_patients = ((Z_significance + Z_power)/effect_size)**2 n_patients  30.152256387475454  Therefore a sample size of n=31 (rounding up) will ensure that a two-sided test with $\\alpha = 0.05$ has 80% power to detect a 5 mg/dL difference in mean fasting blood glucose levels.\n In the planned study, participants will be asked to fast overnight and to provide a blood sample for analysis of glucose levels. Based on prior experience, the investigators hypothesize that 10% of the participants will fail to fast or will refuse to follow the study protocol.  Factoring in 10% attritition to hit the needed 31 participants:\n31 / (1 - 0.1)  34.44444444444444  Factoring in an attrition rate of 10%, 35 participants should be enrolled in the study.\nSample Size for One Sample, Bernoulli Outcome In studies where the plan is to perform a hypothesis test comparing the proportion of successes in a bernoulli variable in a single population to a known proportion, the hypotheses become:\n$$ H_0: p = p_0 $$ $$ H_A: p \\neq p_0 $$\nThe formula for calculating sample size remains the same as the one for one sample, continuous outcome. This is because a bernoulli random variable approximates to a normal distribution across many trials due to CLT:\n$$ n = \\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\nThe effect size $ES$ is calculated as:\n$$ ES = \\frac{\\lvert p_A - p_0\\rvert}{\\sqrt{p_0(1-p_0)}} $$\n where $p_0$ is the proportion under $H_0$ and $p_A$ is the proportion under $H_A$ the numerator is again a meaningful difference in proportions  We use $p_0$ for the standard deviation calculation in the denominator because we want to measure the effect size of $p_A$, the proportion in our alternate hypothesis, in relation to what we already know/have observed about the population.\nExample  A medical device manufacturer produces implantable stents. During the manufacturing process, approximately 10% of the stents are deemed to be defective. The manufacturer wants to test whether the proportion of defective stents is more than 10%. If the process produces more than 15% defective stents, then corrective action must be taken. Therefore, the manufacturer wants the test to have 90% power to detect a difference in proportions of this magnitude. How many stents must be evaluated? For you computations, use a two-sided test with a 5% level of significance.  p0_stents = 0.1 pA_stents = 0.15 effect_size = np.abs(pA_stents - p0_stents)/\\ np.sqrt(p0_stents * (1 - p0_stents)) effect_size  0.1666666666666666  We could round this effect size up to 0.17 - doing so would simplify our understanding of what the standardized difference we\u0026rsquo;re looking for is, but it would lower the number of samples in our test by way of increasing the denominator.\nalpha = 0.05 beta = 0.1 Z_significance = norm.ppf(1 - (alpha/2)) Z_power = norm.ppf(1 - beta) n_stents = np.square((Z_significance + Z_power)/effect_size) n_stents  378.26723021186257  Therefore, 379 stents should be evaluated to ensure that a two-sided test with $\\alpha = 0.05$ and 90% power would detect a 5% difference (the delta between a 10% and 15% defective rate) in the proportion of defective stents produced.\nSample Sizes for Two Independent Samples, Continuous Outcomes When the plan is to perform a hypothesis test on the mean difference of a continuous random variable (CRV) in two independent populations, the hypotheses of interest are:\n$$ H_0: \\mu_1 = \\mu_2 $$ $$ H_A: \\mu_1 \\neq \\mu_2 $$\nwhere $\\mu_1$ and $\\mu_2$ are the means in the two comparison populations.\nThe formulas for determining sample size and effect size:\n$$ n_i = 2\\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\n$$ ES = \\frac{\\lvert\\mu_1 - \\mu_0\\rvert}{\\sigma} $$\nWhere $n_i$ is the sample size required in each group (i=1,2). When doing a hypothesis test on two independent groups, the pooled estimate of standard deviation $S_p$ is used:\n$$ S_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{(n_1 + n_2 - 2)}} $$\nIf the variability of each of the two populations is known at the outset of the test, then we can use algebra to get the sample sizes by reversing the equation (and setting $n_1 = n_2$, generating samples of equal size). However, it is usually the case that data on the variability of the outcome will only be available for a single population that you\u0026rsquo;re testing an alternative against. This can be used as a substitute for the standard deviation in the effect size to plan the test.\nExample  An investigator is planning a study to assess the association between alcohol consumption and grade point average among college seniors. The plan is to categorize students as heavy drinkers or not using 5 or more drinks on a typical drinking day as the criterion for heavy drinking. Mean grade point averages will be compared between students classified as heavy drinkers versus not using a two independent samples test of means. The standard deviation in grade point averages is assumed to be 0.42 and a meaningful difference in grade point averages (relative to drinking status) is 0.25 units. How many college seniors should be enrolled in the study to ensure that the power of the test is 80% to detect a 0.25 unit difference in mean grade point averages? Use a two-sided test with a 5% level of significance.  gpa_std = 0.42 gpa_delta = 0.25 alpha = 0.05 beta = 0.2  Since variability is only known for average GPA (not for the two populaitons, heavy drinkers vs. non heavy drinkers), we\u0026rsquo;ll use it to plan the study.\neffect_size = gpa_delta/gpa_std effect_size  0.5952380952380952  Pretty large effect size (i.e. we\u0026rsquo;re testing for a pretty obvious difference between the two populations), so we won\u0026rsquo;t need as many samples to achieve our desired power.\nZ_significance = norm.ppf(1 - (alpha/2)) Z_power = norm.ppf(1 - beta) n_students = 2 * np.square((Z_significance + Z_power)/effect_size) n_students  44.30535632445374  Therefore in each group we would need 44 students - 44 heavy drinkers, 44 who aren\u0026rsquo;t heavy drinkers, 88 students total.\nSample Size for Matched Samples, Continuous Outcome  in studies where the plan is to perform a hypothesis test on the mean difference in a continuous outcome variable based on matched data:  $$ H_0: \\mu_d = 0 $$ $$ H_A: \\mu_d \\neq 0 $$\nWhere $\\mu_d$ is the mean difference in the population.\nThe formula for sample size is again:\n$$ n_i = \\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\nWhile effect size is calculated as:\n$$ ES = \\frac{\\mu_d}{\\sigma_d} $$\nWhere $\\sigma_d$ is the standard deviation of the difference in the outcome (i.e. difference based on measurements over time/between matched pairs)\nExample  An investigator wants to evaluate the efficacy of an acupuncture treatment for reducing pain in patients with chronic migraine headaches. The plan is to enroll patients who suffer from migraine headaches. Each will be asked to rate the severity of the pain they experience with their next migraine before any treatment is administered. Pain will be recorded on a scale of 1-100 with higher scores indicative of more severe pain. Each patient will then undergo the acupuncture treatment. On their next migraine (post-treatment), each patient will again be asked to rate the severity of the pain. The difference in pain will be computed for each patient. A two sided test of hypothesis will be conducted, at α =0.05, to assess whether there is a statistically significant difference in pain scores before and after treatment. How many patients should be involved in the study to ensure that the test has 80% power to detect a difference of 10 units on the pain scale? Assume that the standard deviation in the difference scores is approximately 20 units.  pain_std = 20 pain_delta = 10 alpha = 0.05 beta = 0.2 effect_size = pain_delta/pain_std effect_size  0.5  Z_significance = norm.ppf(1 - (alpha/2)) Z_power = norm.ppf(1 - beta) n_samples = np.square((Z_significance + Z_power) / effect_size) n_samples  31.395518937396353  Therefore a sample size n=32 patients with migraines will ensure that a two sided test with $\\alpha=0.05$ has 80% power to detect a mean difference of 10% pain before and after the treatment, assuming all patients complete the treatment.\nSample Sizes for Two Independent Samples, Dichotomous Outcomes In studies where the plan is to perform a hypothesis test comparing the proportions of successes in two independent populations, the hypotheses of interest are:\n$$ H_0: p_1 = p_2 $$ $$ H_A: p_1 \\neq p_2 $$\nWhere $p_1$ and $p_2$ are the proportions in the two comparison populations.\nThe formulas for determining sample size and effect size are:\n$$ n_i = 2\\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\n$$ ES = \\frac{\\lvert{p_1 - p_2}\\rvert}{\\sqrt{p(1-p)}} $$\n $n_i$ is the sample size required for each group (i=1,2). $\\lvert{p_1 - p_2}\\rvert$ is the absolute value of the difference in proportions between the two groups expected under the alternate hypothesis $H_A$. $p$ is the overall proportion, based on pooling the data from the two comparison groups (can be computed by taking the mean of the proportions in the two groups, assuming that the groups will be of approximately equal size.  Example   *Clostridium difficile (also referred to as \u0026ldquo;C. difficile\u0026rdquo; or \u0026ldquo;C. diff.\u0026quot;) is a bacterial species that can be found in the colon of humans, although its numbers are kept in check by other normal flora in the colon. Antibiotic therapy sometimes diminishes the normal flora in the colon to the point that C. difficile flourishes and causes infection with symptoms ranging from diarrhea to life-threatening inflammation of the colon. Illness from C. difficile most commonly affects older adults in hospitals or in long term care facilities and typically occurs after use of antibiotic medications.*\n  *In recent years, C. difficile infections have become more frequent, more severe and more difficult to treat. Ironically, C. difficile is first treated by discontinuing antibiotics, if they are still being prescribed. If that is unsuccessful, the infection has been treated by switching to another antibiotic. However, treatment with another antibiotic frequently does not cure the C. difficile infection. There have been sporadic reports of successful treatment by infusing feces from healthy donors into the duodenum of patients suffering from C. difficile. (Yuk!) This re-establishes the normal microbiota in the colon, and counteracts the overgrowth of C. diff.*\n  The efficacy of this approach was tested in a randomized clinical trial reported in the New England Journal of Medicine (Jan. 2013). The investigators planned to randomly assign patients with recurrent C. difficile infection to either antibiotic therapy or to duodenal infusion of donor feces. In order to estimate the sample size that would be needed, the investigators assumed that the feces infusion would be successful 90% of the time, and antibiotic therapy would be successful in 60% of cases. How many subjects will be needed in each group to ensure that the power of the study is 80% with a level of significance α = 0.05?\n  p_feces = 0.9 p_anti = 0.6 p = np.mean([p_feces, p_anti]) effect_size = np.abs(p_feces - p_anti)/np.sqrt(p * (1 - p)) effect_size  0.692820323027551  Again, pretty sizeable effect size.\nalpha = 0.05 beta = 0.2 Z_significance = norm.ppf(1 - alpha/2) Z_power = norm.ppf(1 - beta) n_subjects = 2 * np.square((Z_significance + Z_power) / effect_size) n_subjects  32.70366555978786  Each group would need about 33 subjects, so (66 subjects total) to detect a 30% difference in the two methods with $\\alpha = 0.05$ and 80% power\nConcluding Notes Determining the appropriate design of a study is more important than the analysis; you can always re-analzye the data, you can\u0026rsquo;t always just redo studies. We need a sample size large enough to answer the research question, byachieving acceptable margins of error and powers for the results.\n","date":"2020-11-29","permalink":"https://billwarker.com/posts/calculating-sample-size-to-ensure-high-power/","tags":["stats"],"title":"Calculating Sample Size to Ensure High Power"},{"content":"Part 2 of 3 on a series of notes covering margin of error, power, and sample size calculations.\nNotes, with questions and examples, taken from the following reading: https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html\nType I and Type II Errors, and their relationship to Power In hypothesis testing there are two kinds of errors that can be made when deciding whether to reject the null hypothesis or not:\nType I Error A type I error is falsely rejecting $h_0$ (the null hypothesis) when it is actually true, i.e. a false positive. Imagine a doctor looking at a man and telling him he\u0026rsquo;s pregnant. The level of significance in a hypothesis test, $\\alpha$ (alpha), is the probability of a type I error occuring:\n$\\alpha$ = P(Type I Error) = P(Rejct $H_0$ | $H_0$ is True)\nWe can use alpha as a control for the probability of making a type I error.\nType II Error A type II error is not rejecting $h_0$ when it it\u0026rsquo;s actually false, i.e. a false negative. This time, the doctor looks at a pregnant lady and tells her she\u0026rsquo;s got to start exercising to get rid of that giant bump in her belly. The probability of making a type II error is denoted as $\\beta$ (beta):\n$\\beta$ = P(Type II Error) = P(Do not reject $H_0$ | $H_0$ is False)\nStatistical Power The power of a hypothesis test is the probability that $H_0$ will be correctly rejected when it is false. In other words, its the probability of detecting an effect if it actually exists. This is the probability of not making a type II error:\nPower = 1 - $\\beta$ = 1 - P(Do not reject $H_0$ | $H_0$ is False)\nA good hypothesis tests has a low significance threshold (small $\\alpha$) and high power (small $\\beta$). Power is a single piece in a puzzle of four interconnected parts:\n The chosen significance level of the hyptothesis test, $\\alpha$ The desired power of the test, (1 - $\\beta$) The sample size, which determines the variability of the parameter of interest The effect size, the difference observed in the parameter of interest that denotes a meaningful difference (determined through domain knowledge)  Power analysis involves estimating one of these variables given we know the values of the other three.\nExample Say we have some parameter of interest in a population that we want to make an inference on. We want to test the following hypothesis about this parameter:\nThe Null Hypothesis $H_0$: the population mean $\\mu$ for the parameter is 90. $\\mu$ = 90\nThe Alternative Hypothesis $H_A$: the population mean $\\mu$ for the parameter is not 90. $\\mu$ $\\neq$ 90\nHere are the conditions for the test:\n We want to test a hypothesis with a significance level of $\\alpha$ = 0.05, i.e. the probability of a false positive is only 5%. The test is two sided, meaning we are testing to see if the parameter of interest is lower or higher than our null hypothesis From previous tests on the population we can safely estimate that the standard deviation $\\sigma$ of the parameter is 20. $\\sigma$ = 20 To conduct this test we select a sample of n = 100  To conduct the test we compute the parameter\u0026rsquo;s sample mean $\\bar{X}$ and then decide whether it provides enough evidence to support the alternative hypothesis. To do this we compute a test statistic and compare it to the appropriate critical value; since we know the variability of the parameter we can use a Z test.\nIf the null hypothesis is true ($\\mu$ = 90) then we are likely to select a sample whose mean is close to 90. However it\u0026rsquo;s possible to have a sample mean that is much larger or smaller than 90.\nWe can use the Central Limit Theorum here: when n is sufficiently large (in this case n=100 is enough), the distribution of sample means is approximately normal with a mean of:\n$$ \\mu_X = \\mu $$\nThe standard error of our sample can be calculated as:\n$$ SE = \\frac{\\sigma}{\\sqrt{n}} = \\frac{20}{\\sqrt{100}} = 2.0 $$\nIf the null hypothesis is true, then it is possible to observe any sample mean from the sampling distribution below:\nimport numpy as np from scipy.stats import norm import matplotlib.pyplot as plt  h0_true_mean = 90 standard_deviation = 20 sample_size = 100 standard_error = standard_deviation / np.sqrt(sample_size) h0_sample_dist = norm(h0_true_mean, standard_error) x_range = h0_sample_dist.ppf(np.linspace(0.0001, 0.9999, num=1000)) plt.figure(figsize=(12,6)) plt.plot(x_range, h0_sample_dist.pdf(x_range)) plt.title(\u0026quot;Sampling Distribution for $\\\\bar{X}$ given $H_0$: $\\mu$ = 90 is True\u0026quot;, fontsize=16, pad=10) plt.xlabel(\u0026quot;Values for $\\\\bar{X}$\u0026quot;, fontsize=12) plt.ylabel(\u0026quot;$\\\\bar{X}$ Probability (PDF)\u0026quot;, fontsize=12) plt.show()  Given this sampling distribution, we determine critical lower and upper values at which we reject $H_0$ based on our chosen significance ($\\alpha = 0.05$) and the decision that this will be a two-sided test:\nupper_rejection_cutoff = h0_sample_dist.ppf(0.975) # 2.5% probability of occuring at or after this threshold lower_rejection_cutoff = h0_sample_dist.ppf(0.025) # 2.5% probability of occuring at or before this threshold print(f\u0026quot;Upper rejection cutoff: {upper_rejection_cutoff}\u0026quot;) print(f\u0026quot;Lower rejection cutoff: {lower_rejection_cutoff}\u0026quot;)  Upper rejection cutoff: 93.9199279690801 Lower rejection cutoff: 86.0800720309199  Speaking in terms of the Z test, we would take the calculated sample mean $\\bar{X}$ and convert it into a Z score. We\u0026rsquo;d then find this Z score\u0026rsquo;s probability on a standard normal distribution and if it was less than 5% (i.e. outside of our lower and upper rejection cutoffs), we would reject $H_0$.\nIn this example the critical values for a two-sided test with $\\alpha$ = 0.05 are 86.06 and 93.92 (-1.96 and 1.96 on the Z scale), so the decision rule becomes reject $H_0$ if $\\bar{X}$ $\\leq$ 86.06 or if $\\bar{X}$ $\\geq$ 93.92.\nplt.figure(figsize=(12,6)) plt.plot(x_range, h0_sample_dist.pdf(x_range)) plt.axvline(lower_rejection_cutoff, color='r', linestyle='--', label=f'{lower_rejection_cutoff}') plt.axvline(upper_rejection_cutoff, color='r', linestyle='--', label=f'{upper_rejection_cutoff}') lower_rejection_range = np.linspace(h0_sample_dist.ppf(0.0001), lower_rejection_cutoff, num=1000) upper_rejection_range = np.linspace(upper_rejection_cutoff, h0_sample_dist.ppf(0.9999), num=1000) non_rejection_range = np.linspace(lower_rejection_cutoff, upper_rejection_cutoff, num=1000) plt.fill_between(non_rejection_range, 0, h0_sample_dist.pdf(non_rejection_range), color='green', alpha=0.5) plt.fill_between(lower_rejection_range, 0, h0_sample_dist.pdf(lower_rejection_range), color='red', alpha=0.5) plt.fill_between(upper_rejection_range, 0, h0_sample_dist.pdf(upper_rejection_range), color='red', alpha=0.5) plt.title(\u0026quot;Rejection Region for Test $H_0$: $\\\\mu$ = 90 vs. $H_A$: $\\\\neq$ 90 at $\\\\alpha$ = 0.05\u0026quot;, fontsize=16, pad=10) plt.xlabel(\u0026quot;Values for $\\\\bar{X}$\u0026quot;, fontsize=12) plt.ylabel(\u0026quot;$\\\\bar{X}$ Probability (PDF)\u0026quot;, fontsize=12) plt.legend() plt.show()  The red areas that aren\u0026rsquo;t between the two rejection lines represent the probability of a Type I error, which are the values for sample mean $\\bar{X}$ whose probabilities sum to $\\alpha$ = 0.05. If $\\bar{X}$ is in these regions, we reject $H_0$ with a 5% probability of making a type I error. The green area represents the chosen range where $\\bar{X}$ supports the null hypothesis, so we do not reject $h_0$.\nIf we suppose the alternative hypothesis, $H_A$ is true ($\\mu$ $\\neq$ 90) and that the true mean is actually 94, this is what the distributions of the sample mean look like for the null and alternate hypotheses:\ntrue_mean = 94 x_range = np.linspace(80,100, num=1000) hA_sample_dist = norm(true_mean, standard_error) plt.figure(figsize=(12,6)) plt.axvline(true_mean, linestyle='--', label='$\\mu$ = 94') plt.axvline(lower_rejection_cutoff, color='r', linestyle='--', label=f'{lower_rejection_cutoff}') plt.axvline(upper_rejection_cutoff, color='r', linestyle='--', label=f'{upper_rejection_cutoff}') plt.plot(x_range, h0_sample_dist.pdf(x_range), color='b', label='$H_0$') plt.plot(x_range, hA_sample_dist.pdf(x_range), color='r', label='$H_A$') false_neg_range = np.linspace(lower_rejection_cutoff, upper_rejection_cutoff, num=1000) plt.fill_between(false_neg_range, 0, hA_sample_dist.pdf(false_neg_range), color='red', alpha=0.5) power_range = np.linspace(upper_rejection_cutoff, 100, num=1000) plt.fill_between(power_range, 0, hA_sample_dist.pdf(power_range), color='green', alpha=0.5) plt.title(\u0026quot;Distribution of $\\\\bar{X}$ under $H_0$: $\\\\mu$ = 90 and under $H_A$: $\\\\mu$ = 94 \u0026quot;) plt.xlabel(\u0026quot;Values for $\\\\bar{X}$\u0026quot;, fontsize=12) plt.ylabel(\u0026quot;$\\\\bar{X}$ Probability (PDF)\u0026quot;, fontsize=12) plt.legend() plt.show()  If the true mean is 94, then the alternative hypothesis $H_A$ is true. The probability of a type II error, $\\beta$, is the red shaded area: this is the overlap between the alternate hypothesis\u0026rsquo; distribution has with the \u0026ldquo;do not reject region\u0026rdquo; of the null hypothesis.\nThe test\u0026rsquo;s power, i.e. the probability of a true positive, rejecting $h_0$ when it is truly false, is the green shaded area to the right of the null hypothesis\u0026rsquo; upper rejection cutoff (as set by $\\alpha$). It can be calculated as the probability of $\\bar{X}$ being a value greater than $H_0$'s upper rejection cutoff of 93.91, given $H_A$ is true (1 - probability of beta).\nTo do this we can put the upper rejection cutoff $\\bar{X}$ = 93.91 in terms of its associated Z statistic:\n$$ Power = 1 - \\beta = P(\\bar{X} \u0026gt; 93.91 | H_A) $$\n$$ Power = P\\left(Z \u0026gt; \\frac{93.92 - 94}{\\frac{20}{\\sqrt{100}}}\\right) $$\n$$ Power = P\\left(Z \u0026gt; -0.04\\right) $$\nWe can convert the Z score of -0.04 using the cumulative density function, which will represent the probablility of drawing values less than or equal to -0.04 on a standard normal distribution\nbeta_calc = (upper_rejection_cutoff - true_mean)/standard_error  norm.cdf(beta_calc)  0.4840322065576678  This gives us a beta $\\beta$ of 0.484 (the probability of $\\bar{X}$ being less or equal to 93.91, giving us a false negative). From there we can just subtract this value from 1 to get the probability of that not happening (true positive):\n$$ Power = 1 - \\beta = P(\\bar{X} \u0026gt; 93.91 | H_A) = 1 - 0.484 = 0.516 $$\nTherefore, the given power of this test between $H_0$ and $H_A$ is 51.6% (not great). $\\beta$ can also be calculated from our $H_A$ distribution object, by obtaining the CDF at $H_0$'s upper rejection region:\nbeta_from_dist = hA_sample_dist.cdf(upper_rejection_cutoff) power = round(1 - beta_from_dist, 4) * 100 print(f\u0026quot;The power of this hypothesis test is {power}%\u0026quot;)  The power of this hypothesis test is 51.6%  $\\beta$ and power are related to $\\alpha$, the variance of the outcome and the effect size (i.e. the difference in the parameter of interest between $H_0$ and $H_A$). If we increased $\\alpha$ from 0.05 to 0.10, the upper rejection limit of $H_0$ would shift to the left and be larger, increasing the test\u0026rsquo;s power. While this would give the test higher power, it would also reduce the confidence we could have in the test.\nThe effect size and variance of the outcome affect power in clear ways:\n Increase the desired effect size between $H_0$ and $H_A$ to move their respective distributions further away from each other, reducing their overlap Gathering more samples and reducing the variance of $H_0$ and $H_A$'s distributions will also reduce their overlap  Using the exact same components as the plot above, here is what the test\u0026rsquo;s power becomes when $H_0$: $\\mu$ = 90 and $H_A$: $\\mu$ = 98, an effect size of 8 units:\nhA_mean = 98 x_range = np.linspace(80,110, num=1000) hA_sample_dist = norm(hA_mean, standard_error) plt.figure(figsize=(12,6)) plt.axvline(lower_rejection_cutoff, color='r', linestyle='--', label=f'{lower_rejection_cutoff}') plt.axvline(upper_rejection_cutoff, color='r', linestyle='--', label=f'{upper_rejection_cutoff}') plt.plot(x_range, h0_sample_dist.pdf(x_range), color='b', label='$H_0$') plt.plot(x_range, hA_sample_dist.pdf(x_range), color='r', label='$H_A$') false_neg_range = np.linspace(lower_rejection_cutoff, upper_rejection_cutoff, num=1000) plt.fill_between(false_neg_range, 0, hA_sample_dist.pdf(false_neg_range), color='blue', alpha=0.5) power_range = np.linspace(upper_rejection_cutoff, 110, num=1000) plt.fill_between(power_range, 0, hA_sample_dist.pdf(power_range), color='green', alpha=0.5) plt.title(\u0026quot;Distribution of $\\\\bar{X}$ under $H_0$: $\\\\mu$ = 90 and under $H_A$: $\\\\mu$ = 94 \u0026quot;) plt.xlabel(\u0026quot;Values for $\\\\bar{X}$\u0026quot;, fontsize=12) plt.ylabel(\u0026quot;$\\\\bar{X}$ Probability (PDF)\u0026quot;, fontsize=12) plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7ff6971a7390\u0026gt;  Calculating the Power for this test by obtaining $\\beta$ from $H_A$'s distribution variable:\nbeta_from_dist = hA_sample_dist.cdf(upper_rejection_cutoff) power = round(1 - beta_from_dist, 4) * 100 print(f\u0026quot;The power of this hypothesis test is {power}%\u0026quot;)  The power of this hypothesis test is 97.92999999999999%  Note to be continued in Part 3: Ensuring a Test has High Power","date":"2020-11-22","permalink":"https://billwarker.com/posts/understanding-statistical-power/","tags":["stats"],"title":"Understanding Statistical Power"},{"content":"Part 1 of 3 on a series of notes covering margin of error, power, and sample size calculations.\nNotes, with questions and examples, taken from the following reading: https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html\nConfidence Interals, Margins of Error, and Sample Sizes Confidence intervals take the following general form: Point Estimate $\\pm$ Margin of Error\nFor confidence intervals based on normal data, this looks like:\n$$ \\bar{X} \\pm E $$\n $\\bar{X}$ is the sample mean generated through the experiment (our point estimate) $E$ is the margin of error, calculated as: $$ E=Z\\frac{\\sigma}{\\sqrt{n}} $$\n $Z$ is the Z statistic of a standard normal distribution for a desired confidence level (Z = 1.96 for 95% confidence) $\\sigma$ is the standard deviation of the population $\\mu$ (as best as we know/can estimate it) $\\sqrt{n}$ is the square root of the sample size  In planning experiments we need to determine the sample size required to achieve a sufficiently small margin of error. If the margin of error is too wide then the test is fairly uninformative. To determine the sample size needed first we need to define the desired margin of error, and then we can use algebra to solve:\n$$ E = Z\\frac{\\sigma}{\\sqrt{n}} $$\n$$ \\sqrt{n}E = Z\\sigma $$\n$$ \\sqrt{n} = \\frac{Z\\sigma}{E} $$\n$$ n = \\left(\\frac{Z\\sigma}{E}\\right)^2 $$\n$\\sigma$ can be difficult to estimate at the outset of a experiment, so it can be appropriate to use a value for the standard deviation from a previous study done to a comparable population. However it\u0026rsquo;s determined, $\\sigma$ should be a conservative estimate (i.e. as large as is reasonable) so that the resulting sample size isn\u0026rsquo;t too small.\nThe following examples demonstrate these sample size calculations for different scenarios and random variables.\nSample Size for One Sample, Continuous Outcome Example 1:\n An investigator wants to estimate the mean systolic blood pressure in children with congenital heart disease who are between the ages of 3 and 5. How many children should be enrolled in the study? The investigator plans on using a 95% confidence interval (so Z=1.96) and wants a margin of error of 5 units. The standard deviation of systolic blood pressure is unknown, but the investigators conduct a literature search and find that the standard deviation of systolic blood pressures in children with other cardiac defects is between 15 and 20.  from scipy.stats import norm  Z = norm.ppf(0.975) std = 20 E = 5  n = ((Z*std)/E)**2  n  61.46334113110599  In order to ensure a 95% confidence interval the study will need 62 participants (rounding up). Selecting a smaller sample size could potentially produce a confidence interval with a larger margin of error.\nQuestion 1:\n An investigator wants to estimate the mean birth weight of infants born full term (approximately 40 weeks gestation) to mothers who are 19 years of age and under. The mean birth weight of infants born full-term to mothers 20 years of age and older is 3,510 grams with a standard deviation of 385 grams. How many women 19 years of age and under must be enrolled in the study to ensure that a 95% confidence interval estimate of the mean birth weight of their infants has a margin of error not exceeding 100 grams? If 5% of women are expected to deliver prematurely, how many participants should there be to account for this possibility?  Z = norm.ppf(0.975) std = 385 E = 100  n = ((Z*std)/E)**2  n  56.94002336973868  In order to ensure a 95% confidence interval the study will need 57 participants. If 5% of women are expected to deliver prematurely then we would need $\\frac{n}{0.95} = 60$ participants\nexpected_premature = 0.05 n = (n/(1 - expected_premature))  n  59.93686670498809  Sample Size for One Sample, Binary Outcome (Bernoulli) In experiments to estimate the proportion of successes in a variable with a binary outcome (yes/no, AKA a bernoulli random variable), the formula becomes:\n$$ n = p(1-p)\\left(\\frac{Z}{E}\\right)^2 $$\n $n$ is equal to the variance of a bernoulli trial multiplied by the square of the desired confidence Z score over the margin of error  Working backwards to get the margin of error:\n$$ n = p(1-p)\\left(\\frac{Z}{E}\\right)^2 = \\sigma^2\\left(\\frac{Z}{E}\\right)^2 $$\n$$ \\frac{n}{\\sigma^2} = \\left(\\frac{Z}{E}\\right)^2 $$\n$$ \\sqrt{\\frac{n}{\\sigma^2}} = \\frac{Z}{E} $$\n$$ \\frac{\\sqrt{n}}{\\sigma} = \\frac{Z}{E} $$\n$$ E\\frac{\\sqrt{n}}{\\sigma} = Z $$\n$$ \\frac{E}{\\sigma} = \\frac{Z}{\\sqrt{n}} $$\n$$ E = Z\\frac{\\sigma}{\\sqrt{n}} $$\nIn planning an experiment, $p$ is our estimate of the propensity for the binary outcome to be a success and $1-p$ is the propensity for it to be failure. If no knowledge is known for an estimate of $p$, using 0.5 (50/50 chance) will maximize the variance and the sample size.\nExample 2:\n An investigator wants to estimate the proportion of freshmen at his University who currently smoke cigarettes (i.e., the prevalence of smoking). How many freshmen should be involved in the study to ensure that a 95% confidence interval estimate of the proportion of freshmen who smoke is within 5% of the true proportion?  Since we have no information of the proportion of freshmen who smoke, we use 0.5 to estimate the sample size as follows:\np = 0.5 Z = norm.ppf(0.975) E = 0.05  n = p*(1-p)*(Z/E)**2  n  384.14588206941244  To ensure a 95% confidence interval estimate of the proportion of freshmen who smoke is within 5% of the true population, a sample size of 385 is needed.\nQuestion 2:\n Suppose that a similar study was conducted 2 years ago and found that the prevalence of smoking was 27% among freshmen. If the investigator believes that this is a reasonable estimate of prevalence 2 years later, it can be used to plan the next study. Using this estimate of p, what sample size is needed (assuming that again a 95% confidence interval will be used and we want the same level of precision)?  p = 0.27 Z = norm.ppf(0.975) E = 0.05  n = p*(1-p)*(Z/E)**2  n  302.86061342352474  To ensure a 95% confidence interval estimate of the proportion of freshmen who smoke is within 5% of the true population, a sample size of 303 is needed.\nExample 3:\n An investigator wants to estimate the prevalence of breast cancer among women who are between 40 and 45 years of age living in Boston. How many women must be involved in the study to ensure that the estimate is precise? National data suggest that 1 in 235 women are diagnosed with breast cancer by age 40. This translates to a proportion of 0.0043 (0.43%) or a prevalence of 43 per 10,000 women. Suppose the investigator wants the estimate to be within 10 per 10,000 women with 95% confidence.  p = 43/10000 Z = norm.ppf(0.975) E = 10/10000  n = p*(1-p)*(Z/E)**2  n  16447.244355390107  A sample size of n=16447 will ensure a 95% confidence interval estimate of the prevelance of breast cancer is within 0.10 (10 women per 10,000).\n Suppose this sample size isn\u0026rsquo;t feasible, and the investigators thought a sample size of 5,000 would be practical How precisely can we estimate the prevalence with a sample size of n=5,000?  The confidence interval formula to estimate prevalence is:\n$$ \\hat{p}\\pm Z\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}} $$\nThis is just the sample mean plus/minus the Z score multiplied the standard error of the mean. If we assume the prevalence of breast cancer in the sample will be close to that based on national data, we can expect the margin of error to be approximately:\n$$ Z\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}} = 1.96\\sqrt{\\frac{0.0043(1-0.00.43)}{5000}} = 0.0018 $$\nsample_size = 5000  E = Z*((p*(1-p))/sample_size)**(1/2)  E  0.0018136837847535663  With n=5,000 women in the sample, a 95% confidence interval would be expected to have a margin of error of 0.0018 (18 per 10,000). The investigators would need to decide if this is precise enough to answer the question. This comes with the assumption that the propensity for one to get breast cancer in Boston is similar to the propensity to get it nationally, which might be a stretch.\nSample Sizes for Two Independent Samples, Continuous Outcome For studies where the plan is to estimate the difference in means between two independent populations, the formula for determining sample sizes becomes:\n$$ n_i = 2\\left(\\frac{Z\\sigma}{ES_p}\\right)^2 $$\n $n_i$ is the sample size required in each group $Z$ is again the Z score from the standard normal distribution for the confidence level used $E$ is the desired margin of error $\\sigma$ is the standard deviation of the outcome variable $S_p$ is the pooled estimate of the common standard deviation between the two populations, calculated as:  $$ S_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{(n_1 + n_2 - 2)}} $$\nIf data is available on variability of the outcome in each population, then $S_p$ can be computed and used in the sample size formula. Usually though there\u0026rsquo;s only data on the variance in one group, usually the control.\nWhen planning an investigation data is often available from other trials that involved a placebo or control group, and a standard deviation from these trials can be used for the experimental (non-control) group in this trial. When this is the case we forget about $S_p$ and just use the following:\n$$ n_i = 2\\left(\\frac{Z\\sigma}{E}\\right)^2 $$\nNote: sample size formula generates estimates for samples of equal size, alternative formulas can be used for samples of different sizes\nSkipping Example 4 and going straight to Example 5:\n An investigator wants to compare two diet programs in children who are obese. One diet is a low fat diet, and the other is a low carbohydrate diet. The plan is to enroll children and weigh them at the start of the study. Each child will then be randomly assigned to either the low fat or the low carbohydrate diet. Each child will follow the assigned diet for 8 weeks, at which time they will again be weighed. The number of pounds lost will be computed for each child. Based on data reported from diet trials in adults, the investigator expects that 20% of all children will not complete the study. A 95% confidence interval will be estimated to quantify the difference in weight lost between the two diets and the investigator would like the margin of error to be no more than 3 pounds. How many children should be recruited into the study? Again, the issue is determining the variability in the outcome of interest (σ), here the standard deviation in pounds lost over 8 weeks. To plan this study, investigators use data from a published study in adults. Suppose one such study compared the same diets in adults and involved 100 participants in each diet group. The study reported a standard deviation in weight lost over 8 weeks on a low fat diet of 8.4 pounds and a standard deviation in weight lost over 8 weeks on a low carbohydrate diet of 7.7 pounds.  Can use the information in the second bullet to compute the pooled estimate of the standard deviation between low fat and low carbohydrate groups:\nn_fat = 100 n_carb = 100 std_fat = 8.4 std_carb = 7.7 std_pooled = (((n_fat - 1)*std_fat**2 \\ + (n_carb - 1)*std_carb**2) \\ / (n_fat + n_carb - 2))**(1/2)  std_pooled  8.057605103254938  $S_p = 8.1 $, rounding up. We will use this as $\\sigma$ in our experiment, and do not need to multiply the margin of error by the pooled variance (drop $ES_p$ in the denominator and just use $E$):\nZ = norm.ppf(0.975) E = 3 n = 2*((Z*std_pooled)/E)**2  n  55.423714207459135  $n = 56$, rounding up. This means that $2 \\times n_i = 2 \\times 56 = 112$ children should be recruited for the study (not counting attrition). If we factor in an attrition rate of 20%:\nattrition_rate = 0.2 n_total = (n * 2) / (1 - attrition_rate)  n_total  138.55928551864784  Factoring in attrition, about 140 children should participate in the study.\nSample Size for Matched Samples, Continuous Outcomes In studies where the plan is to estimate the mean difference of a continuous outcome based on matched (i.e. paired) data:\n$$ n = \\left(\\frac{Z\\sigma_d}{E}\\right)^2 $$\nIn this case, $\\sigma_d$ is the standard deviation of the difference scores. The standard deviation between the paired data points must be used here, you can\u0026rsquo;t estimate the difference using past trials.\nSample Sizes for Two Independent Samples, Binary Outcome In studies where the plan is to estimate the difference in proportions between two independent populations, the formula for determining the sample sizes required in each comparison group is:\n$$ n_i = {p_1(1-p_1) + p_2(1-p_2)}\\left(\\frac{Z}{E}\\right)^2 $$\n $n_i$ is the sample size required in each group ${p_1(1-p_1) + p_2(1-p_2)}$ is their pooled variance $Z$ is again the Z score from the standard normal distribution for the confidence level used $E$ is the desired margin of error $p_1$ and $p_2$ are the propensities for success in each group  To estimate the sample size we need to approximate $p_1$ and $p_2$, or if we have no prior intuitions and just want to generate the most conservative and largest sample sizes we can again use just 0.5.\nIf we\u0026rsquo;re comparing an unknown group with a group that we know already have data on (e.g. the control group), we can use its proportion for both $p_1$ and $p_2$. Alternative formulas can be used with groups with different sample sizes\nExample 6\n  An investigator wants to estimate the impact of smoking during pregnancy on premature delivery. Normal pregnancies last approximately 40 weeks and premature deliveries are those that occur before 37 weeks. The 2005 National Vital Statistics report indicates that approximately 12% of infants are born prematurely in the United States.5 The investigator plans to collect data through medical record review and to generate a 95% confidence interval for the difference in proportions of infants born prematurely to women who smoked during pregnancy as compared to those who did not. How many women should be enrolled in the study to ensure that the 95% confidence interval for the difference in proportions has a margin of error of no more than 4%?\n  The sample sizes (i.e., numbers of women who smoked and did not smoke during pregnancy) can be computed using the formula shown above. National data suggest that 12% of infants are born prematurely.\n  # using the proportion of p=0.12 for both groups (smoking and non-smoking) Z = norm.ppf(0.975) E = 0.04 p = 0.12 n = (p*(1-p) + p*(1-p))*(Z/E)**2  n  507.07256433162445  A sample size of $n_1=508$ women who smoked during pregnancy and $n_2=508$ who did not during pregnancy will ensure that the 95% confidence interval for the difference in proportions who deliver prematurely will have a margin of error of no more than 4%.\nAttrition could be a factor in this trial as confounding factors could happen to either group (someone stops/starts smoking or decides to drop out for whatever reason)\nNote to be continued in Part 2: Understanding Statistical Power","date":"2020-11-21","permalink":"https://billwarker.com/posts/understanding-margin-of-error-and-sample-size/","tags":["stats"],"title":"Understanding Margin of Error and Sample Size"},{"content":"Well, for starters, a hash is an important part of a very useful and fast data structure called a hash table. Let\u0026rsquo;s reframe the question and ask what a hash table is instead.\nThe idea behind a Hash Table A hash table is a data structure that allows for the fast retrieval of data within it, regardless of how much is being stored. It\u0026rsquo;s used to do many things, from efficient caching to database indexing and error checking. The central idea of a hash table can be explained with an example. Let\u0026rsquo;s say we have a big array filled with random elements, one of which we want to retrieve:\nfrom numpy.random import randint def create_array_with_hidden_string(array_size, hidden_string): random_numbers = randint(0, array_size**2, size=(array_size,)) array = list(random_numbers) element_ix = randint(0, array_size) array.insert(element_ix, hidden_string) return array, element_ix hidden_string = 'yeet' array, hidden_string_ix = create_array_with_hidden_string(10**3, hidden_string)  array[:10] # first 10 elements of the array  [438536, 563467, 983737, 888887, 142932, 801221, 364091, 488761, 87409, 695441]  In the code above we\u0026rsquo;ve created an array with 1001 strings, one of which we want to find. Our string of interest, yeet, has been inserted at random index in our array. Let\u0026rsquo;s find it. One way we could do so is through a linear search - simply iterating through the elements and stopping once we find yeet. Let\u0026rsquo;s create a function that implements a linear search and reports how long it takes:\nimport time def linear_search_and_time(array, hidden_string): t_start = time.time() for i in range(len(array)): if array[i] == hidden_string: t_finish = time.time() t_delta = t_finish - t_start print(f\u0026quot;\u0026quot;\u0026quot;found {array[i]} in array of size {len(array)} at index {i} in {round(t_delta,4)} seconds\u0026quot;\u0026quot;\u0026quot;)  linear_search_and_time(array, hidden_string)  found yeet in array of size 1001 at index 81 in 0.0004 seconds  Linear search certainly does the trick, but what if our array was of a much bigger size? A consideration of linear search is that the time it takes to complete scales, well, linearly with the size of the array being searched. In Big O Notation, the time complexity of a linear search is O(N):\narray_sizes = [10**4, 10**5, 10**6, 10**7] for size in array_sizes: array, hidden_string_ix = create_array_with_hidden_string(size, hidden_string) linear_search_and_time(array, hidden_string)  found yeet in array of size 10001 at index 2145 in 0.0085 seconds found yeet in array of size 100001 at index 28500 in 0.1093 seconds found yeet in array of size 1000001 at index 543310 in 2.1833 seconds found yeet in array of size 10000001 at index 9412368 in 36.1402 seconds  As we can see, the linear search gets slower as the size of the array grows.\nSearch is basically instantaneous if you know the position/index of the element you want to find in the array - no matter its size, an array lookup always takes a fast, constant time. In Big O terms, this is O(1):\ndef index_lookup_and_time(array, hidden_string_ix): t_start = time.time() hidden_string = array[hidden_string_ix] t_finish = time.time() t_delta = t_finish - t_start print(f\u0026quot;\u0026quot;\u0026quot;found {hidden_string} in array of size {len(array)} at index {hidden_string_ix} in {round(t_delta,4)} seconds\u0026quot;\u0026quot;\u0026quot;)  for size in array_sizes: array, hidden_string_ix = create_array_with_hidden_string(size, hidden_string) index_lookup_and_time(array, hidden_string_ix)  found yeet in array of size 10001 at index 4934 in 0.0 seconds found yeet in array of size 100001 at index 72609 in 0.0 seconds found yeet in array of size 1000001 at index 827164 in 0.0 seconds found yeet in array of size 10000001 at index 2401275 in 0.0 seconds  This idea of using the index of an element to access it is at the core of hash tables. Independent of the size of the hash table or the position of any of its elements, actions such as adding, accessing, or deleting have a constant O(1) time complexity.\nHashing Algorithms To achieve this, a unique index is created for each value in the table using a hashing algorithm. When a value is \u0026ldquo;hashed\u0026rdquo;, it means that it is being passed through an algorithm that transforms it into a memory address (i.e. an index position). This address should never change over the lifetime of the element and is calculated using aspects of both the element itself and the hash table it is being placed into. In Python\u0026rsquo;s implementation of object hashing, this requirement means that only immutable (i.e. unchangeable) data types like strings, ints, and tuples can be hashed. If mutable objects were able to be hashed, then the hashing algorithm would generate different indexes for them as they changed and internal consistency would be lost.\nWhen an element is added, deleted, or retrieved from a hash table it is first passed into a hashing algorithm to find its location. Whatever action needs to happen then takes place after that.\nThere are different ways to implement hashing algorithms, but in principle they should all be easy to calculate as to keep things fast. Algorithms can also differ based on the intended inputs\u0026rsquo; data types. For numeric keys, a simple hashing function might divide the key by the number of available addresses within an array (i.e. its size) and take the remainder:\n# first, let's create an empty array of a certain size def make_empty_hash_table(size: int): return [None] * size  array = make_empty_hash_table(5) print(array)  [None, None, None, None, None]  # create our number hashing algorithm def simple_numeric_hash(num: int, array_size: int): return num % array_size  # check indices generated by elements print(simple_numeric_hash(30, len(array))) print(simple_numeric_hash(21, len(array))) print(simple_numeric_hash(9, len(array)))  0 1 4  # define function to insert the elements at the indices provided by the hashing algorithm def insert_number_into_hash_table(num, array): ix = simple_numeric_hash(num, len(array)) array[ix] = num return array  array = insert_number_into_hash_table(30, array) array = insert_number_into_hash_table(21, array) array = insert_number_into_hash_table(9, array) print(array)  [30, 21, None, None, 9]  For a string key, it might sum the ASCII codes for each character in the string and do the same division by the array size, taking the remainder:\n# create another empty array array = make_empty_hash_table(6)  # create our string hashing algorithm def simple_string_hash(string: str, array_size: int): ascii_sum = 0 for char in string: ascii_sum += ord(char) return ascii_sum % array_size  # check indices generated by pet hamster names print(simple_string_hash('Squeeky', len(array))) print(simple_string_hash('Squishy', len(array))) print(simple_string_hash('Squirrelly', len(array)))  5 2 4  # define function to insert the elements at the indices provided by the hashing algorithm def insert_string_into_hash_table(string, array): ix = simple_string_hash(string, len(array)) array[ix] = string return array  # store some pet hamsters in a hash table array = insert_string_into_hash_table('Squeeky', array) array = insert_string_into_hash_table('Squishy', array) array = insert_string_into_hash_table('Squirrelly', array) print(array)  [None, None, 'Squishy', None, 'Squirrelly', 'Squeeky']  Managing Collisions A hashing algorithm is an elegant way to sort data and have it be quickly accessible. There is one problem that needs to be solved with this approach, however - what if two elements generate the same hash?\nprint(simple_string_hash('Squishy', len(array))) print(simple_string_hash('Smokey', len(array)))  2 2  This is what\u0026rsquo;s known as a collision - when multiple elements yield the same hash. Positions in a hash table can\u0026rsquo;t belong (at least directly) to more than a single element, so available positions are used up as elements are added in. Fortunately, there are a variety of solutions that hash tables can use to manage collisions. These solutions fall into two categories: open addressing and closed addressing.\nOpen addressing solutions will place a collided element in another open position within the hash table. One implementation of this is linear probing, which will do a linear search from the collided index for the next available spot and place the element there. When the element needs to be accessed, the hash table will compute its hash, find the index position, see that position is occupied by something else, and then do a linear search from that spot to find it. Other open addressing algorithms behave similarly, just switching up how the search is performed: plus 3 rehashing checks every third available position, quadratic probing squares the number of failed placement attempts and checks that many positions away, and double hashing applies a second hash function after the first yields a collision. With all of these approaches the goal is find an available position further away from the collision - uniformly distributing across the table will reduce the likelihood of further collisions happening.\nHere\u0026rsquo;s a simple implementation of open addressing with linear probing:\ndef linear_probing_insert(element, ix, array): if array[ix] is None: array[ix] = element else: print(f'collision at index {ix}') lin_search_positions = list(range(ix + 1, len(array))) + list(range(0, ix)) for pos in lin_search_positions: if array[pos] is None: array[pos] = element print(f'added {element} at index {pos} instead') return array print(f'nowhere to put {element}') return array def insert_with_open_addressing(string, array): ix = simple_string_hash(string, len(array)) return linear_probing_insert(string, ix, array)  open_example = array  insert_with_open_addressing('Smokey', open_example)  collision at index 2 added Smokey at index 3 instead [None, None, 'Squishy', 'Smokey', 'Squirrelly', 'Squeeky']  Closed addressing solutions create linked lists at occupied positions in the hash table and add elements sequentially within them. Whereas open addressing solutions aim to place elements across the entire hash table\u0026rsquo;s available positions, closed addressing solutions opt to nest multiple elements within a sub-level list at the positions themselves (think going deep instead of going wide). Either way, both approaches end up having to do some sort of search after hashing an element and getting a collided position.\nHere\u0026rsquo;s a simple implementation of closed addressing with linked lists:\ndef closed_addressing_insert(element, ix, array): if array[ix] is None: array[ix] = [element] else: print(f'collision at index {ix}') array[ix].append(element) print(f'added {element} in a nested list at index {ix}') def insert_with_closed_addressing(string, array): ix = simple_string_hash(string, len(array)) return closed_addressing_insert(string, ix, array)  closed_example = make_empty_hash_table(6)  insert_with_closed_addressing('Squishy', closed_example) insert_with_closed_addressing('Squirrelly', closed_example) insert_with_closed_addressing('Squeeky', closed_example) insert_with_closed_addressing('Smokey', closed_example)  collision at index 2 added Smokey in a nested list at index 2  closed_example  [None, None, ['Squishy', 'Smokey'], None, ['Squirrelly'], ['Squeeky']]  One can aim to avoid collisions entirely by using a larger hash table with more available positions than values to occupy them. The ratio between stored elements and available positions in a hash table is called its load factor, and the lower this is the lower the likelihood of collisions (and subsequent linear searches). If a hash table contains too many collisions, its performance will worsen as more searches with increased time complexity are needed. No collisions means that access time will always be a speedy O(1) time complexity. A good hashing algorithm should aim to minimize collisions, resolve them quickly if they do happen, and generate a uniform distribution of hash values across the table.\nThe Difference between a Hash Table and a Hash Map A map is an association between a key and a value - for key K remember value V.\nMapping between keys and values can be done in many different ways, and one way of doing so is with a hash table. A hash table is just a structure for storing data, but when it stores data that is in the form of a distinct key-value pairing, it\u0026rsquo;s called a hash map.\nIn a hash table that simply stores values like the one above, there are no key-value pairs; each value is its own key with no association with another object.\nIn essence, a hash table is a data structure for quickly storing and accessing a data. A hash map is a particular kind of hash table that stores key-value pairs.\nHash Map Implementation with Lists There are three components to this Hash Map:\n Hash Table: a simple array Hashing Algorithm: a simple hash function that sums the ASCII values in a string, divides it by the size of the array, and returns the remainder Collision Handling: A closed addressing solution for adding elements into a list at each position, and then performing a linear search at the time of accessing  class HashMap: def __init__(self, size): self.size = size self.map = [None] * self.size def _get_hash(self, key): hash_ = 0 for char in str(key): hash_ += ord(char) return hash_ % self.size def add(self, key, value): key_hash = self._get_hash(key) key_value = [key, value] if self.map[key_hash] is None: self.map[key_hash] = list([key_value]) return True else: for pair in self.map[key_hash]: if pair[0] == key: pair[1] = value return True self.map[key_hash].append(key_value) return True def get(self, key): key_hash = self._get_hash(key) if self.map[key_hash] is not None: for pair in self.map[key_hash]: if pair[0] == key: return pair[1] return None def delete(self, key): key_hash = self._get_hash(key) if self.map[key_hash] is None: return False for i in range(0, len(self.map[key_hash])): if self.map[key_hash][i][0] == key: self.map[key_hash].pop(i) return True def display(self): for item in self.map: if item is not None: print(str(item))  # lists of hamsters and their favourite foods - the key value pairs to be inserted into the hash map hamsters = ['Squeeky', 'Squishy', 'Speedy', 'Smokey', 'Squirrelly', 'Snowy', 'Sleepy', 'Smoochy', 'Smarty', 'Snoozy', 'Smoothy', 'Shiny'] fav_foods = ['Pizza', 'Corn', 'Baguettes', 'Beef Jerky', 'Pho', 'Pad Thai', 'Shawarma', 'Chocolate', 'Carrots', 'Wine', 'Smoothies', 'Apples']  To demonstrate the idea of a hash table\u0026rsquo;s load factor, we\u0026rsquo;ll create two hash maps - one with a size equalling the number of elements, and one with a size that is the square of the number of elements.\nhamster_hash_map = HashMap(len(hamsters)) for hamster, food in zip(hamsters, fav_foods): hamster_hash_map.add(hamster, food)  hamster_hash_map.display()  [['Squishy', 'Corn'], ['Sleepy', 'Shawarma']] [['Squirrelly', 'Pho'], ['Snowy', 'Pad Thai'], ['Smarty', 'Carrots']] [['Speedy', 'Baguettes'], ['Smoochy', 'Chocolate']] [['Shiny', 'Apples']] [['Smokey', 'Beef Jerky']] [['Snoozy', 'Wine']] [['Squeeky', 'Pizza'], ['Smoothy', 'Smoothies']]  When the hash table is small relative to the number of elements being stored, collisions can occur. Not a big deal with only a few elements, but at much larger scales this would begin to hurt performance.\nbig_hamster_hash_map = HashMap(len(hamsters)**2) for hamster, food in zip(hamsters, fav_foods): big_hamster_hash_map.add(hamster, food)  big_hamster_hash_map.display()  [['Smoochy', 'Chocolate']] [['Squeeky', 'Pizza']] [['Smoothy', 'Smoothies']] [['Squishy', 'Corn']] [['Speedy', 'Baguettes']] [['Sleepy', 'Shawarma']] [['Smokey', 'Beef Jerky']] [['Smarty', 'Carrots']] [['Squirrelly', 'Pho']] [['Snoozy', 'Wine']] [['Shiny', 'Apples']] [['Snowy', 'Pad Thai']]  When the size of the hash table is much larger than the number of elements, there\u0026rsquo;s a reduced chance of collisions occuring. This is an ideal scenario where the table is highly performant and has an increased likelihood of O(1) access times.\n","date":"2020-09-28","permalink":"https://billwarker.com/posts/what-is-a-hash/","tags":["comp-sci"],"title":"What is a Hash?"},{"content":"Notes Classification: predicting classes/categories\nIntroducing MNIST  handwritten digits  from sklearn.datasets import fetch_openml  mnist = fetch_openml('mnist_784', version=1)  mnist.keys()  dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])  X, y = mnist['data'], mnist['target']  X.shape  (70000, 784)   70K samples, 784 features (28x28 pixels)  y.shape  (70000,)  import matplotlib as mpl import matplotlib.pyplot as plt  some_digit = X[0] some_digit_image = some_digit.reshape(28,28) plt.imshow(some_digit_image, cmap=\u0026quot;binary\u0026quot;) plt.axis(\u0026quot;off\u0026quot;) plt.show()  y[0]  '5'   cast target variable to integers  import numpy as np  y = y.astype(np.uint8)  X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:],   dataset is already shuffled, some CV folds will be similar (no missing digits)  Training a binary classifier  simplify the problem to classifying a single digit Stochastic Gradient Descent (SGD) classifier: good for large datasets, deals with training instances independently, one at a time relies on randomness  y_train_5 = (y_train == 5) y_test_5 = (y_test == 5)  from sklearn.linear_model import SGDClassifier  sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5)  SGDClassifier(random_state=42)  sgd_clf.predict([some_digit])  array([ True])  Performance Measures  Can use cross validation Custom implementation of CV:  from sklearn.model_selection import StratifiedKFold from sklearn.base import clone  skfolds = StratifiedKFold(n_splits=3, random_state=42) for train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_5[train_index] X_test_fold = X_train[test_index] y_test_fold = y_train_5[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct/len(y_pred))  /opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True. FutureWarning 0.95035 0.96035 0.9604   performs stratified sampling to get a representative ratio of each class  from sklearn.model_selection import cross_val_score  cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.95035, 0.96035, 0.9604 ])   accuracy doesn\u0026rsquo;t mean much here, since about 10% of the samples are 5s. If you guess non-5 for every sample you\u0026rsquo;ll get around 90% accuracy  from sklearn.base import BaseEstimator  class Never5Classifier(BaseEstimator): def fit(self, X, y=None): return self def predict(self, X): return np.zeros((len(X), 1), dtype=bool)  never_5_clf = Never5Classifier() cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')  array([0.91125, 0.90855, 0.90915])   accuracy is generally not the preferred performance measure, especially with skewed datasets  Confusion Matrix\n count the number of times instances of class A are classfied as class B  from sklearn.model_selection import cross_val_predict   cross_val_predict returns the predictions for each test fold  y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)  y_train_pred.shape  (60000,)  from sklearn.metrics import confusion_matrix  confusion_matrix(y_train_5, y_train_pred)  array([[53892, 687], [ 1891, 3530]])    each row represents an actual class\n  each column represents a predicted class\n  first row of this matrix is non-5s, second row is 5s\n  row 1 col 1: true negatives\n  row 1 col 2: false positives\n  row 2 col 1: false negatives\n  row 2 col 2: true positives\n  a perfect classifier would only have true positives and true negatives\n  # pretending we reached perfection y_train_perfect_predictions = y_train_5 confusion_matrix(y_train_5, y_train_perfect_predictions)  array([[54579, 0], [ 0, 5421]])  Precision and Recall\nPrecision: accuracy of positive predictions $$ precision = \\frac{TP}{TP+FP} $$\nRecall (AKA sensitivity or True Positive Rate): $$ recall = \\frac{TP}{TP+FN} $$\nfrom sklearn.metrics import precision_score, recall_score  precision_score(y_train_5, y_train_pred)  0.8370879772350012  recall_score(y_train_5, y_train_pred)  0.6511713705958311   higher precision than recall: when the classifier predicted a 5, it was likely to be correct. however, this came at the cost of incorrectly predicting non-5 on samples that it was less sure about its convenient to combine precision and recall into a single metric, the F1 score:  $$ F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = 2\\times\\frac{precision\\times{recall}}{precision + recall} = \\frac{TP}{TP + \\frac{FN+TP}{2}}$$\n this is the harmonic mean; while regular mean treats all values equally, harmonic mean gives much more weight to low values F1 is only high if both precision and recall are  from sklearn.metrics import f1_score  f1_score(y_train_5, y_train_pred)  0.7325171197343846   while F1 is a metric that favours equally good precision and recall, there are instances when prioritizing one of the two is more valuable e.g. a classifier that detects whether videos are safe for kids: you want high precision because the cost of being wrong is very high, and its better to reject potentially safe videos if it ensures that no unsafe videos are recommended e.g. a classifier for catching shoplifting can favour recall; maximizing the potential for catching all instances of shoplifting is well worth the potential for a few false positives here and there there is a tradeoff between precision and recall, can\u0026rsquo;t have it both ways  Precision/Recall Trade-Off\n controlled by the threshold at which a sample is classified as true can control this threshold by calling decision_function() instead of predict() for sklearn estimators and setting a threshold yourself:  y_scores = sgd_clf.decision_function([some_digit]) y_scores  array([2164.22030239])  threshold = 0 y_some_digit_pred = (y_scores \u0026gt; threshold) y_some_digit_pred  array([ True])  threshold = 8000 y_some_digit_pred = (y_scores \u0026gt; threshold) y_some_digit_pred  array([False])   raising the threshold increases precision and lowers recall lowering the threshold increases recall and lowers precision  y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function')  from sklearn.metrics import precision_recall_curve  precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)  import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline sns.set_style('whitegrid')  def plot_precision_recall_vs_threshold(precisions, recalls, thresholds, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(thresholds, precisions[:-1], 'b--', label='Precision') plt.plot(thresholds, recalls[:-1], 'g--', label='Recall') plt.legend()  plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.show()   precision can go down when the threshold increases e.g. getting 4/5 (80%) correct, then raising threshold and getting 3/4 correct (75%) can also plot precision vs. recall directly:  def plot_precision_vs_recall(precisions, recalls, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(recalls, precisions, 'b--') plt.xlabel('Recall') plt.ylabel('Precision')  plot_precision_vs_recall(precisions, recalls) plt.show()   precision really starts to dip around 70% recall might make sense to set the threshold before that drop, but depends on the context of your project to set the threshold at a specific precision: np.argmax() gives the first index of the maximum value; in this case, the first instance where precision \u0026gt; 90 is true  threshold_90_precision = thresholds[np.argmax(precisions \u0026gt;= 0.9)] threshold_90_precision  3370.0194991439557  y_train_pred_90 = (y_scores \u0026gt;= threshold_90_precision)  precision_score(y_train_5, y_train_pred_90)  0.9000345901072293  recall_score(y_train_5, y_train_pred_90)  0.4799852425751706  The ROC Curve\n Receiver Operating Characteristic plots true positive rate (TPR) vs false positive rate (FPR) FPR is ratio of false positives, i.e. 1 - True Negative Rate (TNR) TNR is also known as specificity ROC is plotting sensitivity (recall) vs 1 - specificity  from sklearn.metrics import roc_curve  fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)  def plot_roc_curve(fpr, tpr, label=None, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0,1], [0,1], 'k--') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate (Recall)') plt.legend()  plot_roc_curve(fpr, tpr) plt.show()  No handles with labels found to put in legend.   dotted line represents purely random classifier good classifiers have the highest true positive rate with the lowest false positive rate (top left corner) can compare ROC scores of different classifiers by measuring area under the curve AUC a purely random classifier will have an AUC of 0.5 (think the integral of the purely random classifier, the linear line) false positive rate is so high here because there aren\u0026rsquo;t that many 5s in the data (only about 10%), so don\u0026rsquo;t get misled by the great looking ROC curve  from sklearn.metrics import roc_auc_score  roc_auc_score(y_train_5, y_scores)  0.9604938554008616   use the precision/recall curve when the positive class is rare or you care more about the false positives than the false negatives otherwise use the ROC curve for skewed datasets where the positive class is rare, you can see that precision really suffers when recall increases (a result of having less training data on the positive class, the classifier is trying to catch every positive class despite not having a lot to go off of). paints a different picture than ROC Try calculating ROC, AUC, Precision and Recall for Random Forest estimator:  from sklearn.ensemble import RandomForestClassifier  forest_clf = RandomForestClassifier(random_state=42)  y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba')  y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class  fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)  plot_roc_curve(fpr_forest, tpr_forest, label=\u0026quot;Random Forest\u0026quot;) plt.plot(fpr, tpr, \u0026quot;b:\u0026quot;, label=\u0026quot;SGD\u0026quot;) plt.legend() plt.show()   Closer to top left than SGD, more AUC, better performance Explaining the ROC curve: to achieve an almost 99% recall/TPR rate (we correctly predict positive for 99% of all the real positive samples in the dataset), it seems like we will have to accept the tradeoff of getting 15% of the negative samples incorrectly predicted as positive (the false positive rate)  roc_auc_score(y_train_5, y_scores_forest)  0.9983436731328145  y_preds_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)  precision_score(y_train_5, y_preds_forest)  0.9905083315756169  recall_score(y_train_5, y_preds_forest)  0.8662608374838591  Multiclass Classification  AKA multinomial Logistic Regression, Random Forest, naive Bayes classifiers are examples of algorithms that can make multiclass classifications natively Can get around this with algos that only work as binary; in the example of MNIST train 10 binary classifiers, pick the class with the highest decision score. Known as one-versus-rest/one-versus-all strategy (OvR). You could also train a binary classifier for every pair of digits. i.e. 1 vs 2, 1 vs 3, 2 vs 5, etc. This is one-versus-one (OvO) strategy, and you pick the classifier that wins the most duels. Advantage of this strategy is that you only train the classifiers on subsets of the entire target variable space Some algos scale poorly with the size of the training set, so OvO strategy is preferred In general however, OvR is preferred, as its more straightforward Sklearn classifiers will automatically detecty multiclass classifications from the target variable and will use a strategy based on the algorithm used  from sklearn.svm import SVC  # svm_clf = SVC() # svm_clf.fit(X_train, y_train) # all digits in target variable # svm_clf.predict([some_digit])    SVC uses OvO strategy; it actually trained 45 binary classifiers\n  NOTE: This takes forever to run. Sklearn can\u0026rsquo;t use GPUs to speed up training; GPUs are only useful for training deep learning models with architectures like Tensorflow or PyTorch\n  SVMs have a quadratic time complexity, calculating the distance between each point in the dataset: $$ O({n_{features}}\\times{n_{observations}^2}) $$\n  Caches common points but this kills memory regardless\n  This doesn\u0026rsquo;t scale well over a couple 10k features\n  decision_function() returns 10 scores per instance, and classifier picked the highest one:\n  # some_digit_scores = svm_clf.decision_function([some_digit]) # some_digit_scores  # np.argmax(some_digit_scores)  # svm_clf.classes[np.argmax(some_digit_scores)]   index just happened to match the class itself, but this is just luck Sklearn can be forced to use either OvO or OvR using OneVsOneClassifier or OneVsRestClassifier classes; just create an instance with the classifier you want passed as constructor  from sklearn.multiclass import OneVsRestClassifier  # ovr_clf = OneVsRestClassifier(SVC()) # ovr_clf.fit(X_train, y_train) # ovr_clf.predict([some_digit])  # len(ovr_clf.estimators_)   SGD Classifiers can directly classify instances into multiple classes Decision function returns a score per class  sgd_clf.fit(X_train, y_train) sgd_clf.predict([some_digit])  array([3], dtype=uint8)  sgd_clf.decision_function([some_digit])  array([[-31893.03095419, -34419.69069632, -9530.63950739, 1823.73154031, -22320.14822878, -1385.80478895, -26188.91070951, -16147.51323997, -4604.35491274, -12050.767298 ]])  cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.87365, 0.85835, 0.8689 ])   84% on all testing folds is decent - if you were to use a random classifier you\u0026rsquo;d get 10% accuracy simply scaling inputs can increase score:  from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))  cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.8983, 0.891 , 0.9018])  Error Analysis  can improve shortlisted models by analyzing the errors they make make predictions using the cross_val_predict() function, then call confusion_matrix():  y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) conf_mx = confusion_matrix(y_train, y_train_pred)  conf_mx  array([[5577, 0, 22, 5, 8, 43, 36, 6, 225, 1], [ 0, 6400, 37, 24, 4, 44, 4, 7, 212, 10], [ 27, 27, 5220, 92, 73, 27, 67, 36, 378, 11], [ 22, 17, 117, 5227, 2, 203, 27, 40, 403, 73], [ 12, 14, 41, 9, 5182, 12, 34, 27, 347, 164], [ 27, 15, 30, 168, 53, 4444, 75, 14, 535, 60], [ 30, 15, 42, 3, 44, 97, 5552, 3, 131, 1], [ 21, 10, 51, 30, 49, 12, 3, 5684, 195, 210], [ 17, 63, 48, 86, 3, 126, 25, 10, 5429, 44], [ 25, 18, 30, 64, 118, 36, 1, 179, 371, 5107]])   As a heatmap:  plt.matshow(conf_mx, cmap=plt.cm.gray) plt.xlabel(\u0026quot;Predicted\u0026quot;) plt.ylabel(\u0026quot;Actual Values\u0026quot;) plt.show()    Main diagonal: predicted rate of actual digit\n  Rows are actual classes, Columns are predicted classes\n  Lower rate could mean that less of that class in the dataset or lower performance on it\n  Can divide absolute prediction sums by total number of instances for each class to get error rates:\n  row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums np.fill_diagonal(norm_conf_mx, 0) # fill in diagonals to look at errors only plt.matshow(norm_conf_mx, cmap=plt.cm.gray) plt.xlabel(\u0026quot;Predicted\u0026quot;) plt.ylabel(\u0026quot;Actual Values\u0026quot;) plt.show()   Many different numbers get misclassified for 8s, but 8s themselves seem to get classified properly Confusion matrix isn\u0026rsquo;t symmetrical necessarily General confusion around 5s and 3s too If we wanted to fix the model, focusing on improving scores on 8s would be beneficial: Could collect more data on numbers that look like 8s but aren\u0026rsquo;t 8s Could add extra features, like writing an algorithm to count closed loops, or preprocessing the image to make some patterns like closed loops stand out more Analyzing individual errors is good too but time consuming  def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = mpl.cm.binary, **options) plt.axis(\u0026quot;off\u0026quot;)  cl_a, cl_b = 3, 5 X_aa = X_train[(y_train == cl_a) \u0026amp; (y_train_pred == cl_a)] X_ab = X_train[(y_train == cl_a) \u0026amp; (y_train_pred == cl_b)] X_ba = X_train[(y_train == cl_b) \u0026amp; (y_train_pred == cl_a)] X_bb = X_train[(y_train == cl_b) \u0026amp; (y_train_pred == cl_b)] plt.figure(figsize=(8,8)) plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5) plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5) plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5) plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)    row 1 col 1: 3s that were classified as 3s\n  row 1 col 2: 3s that were classified as 5s\n  row 2 col 1: 5s that were classified as 3s\n  row 2 col 2: 5s that were classified as 5s\n  Hard to understand why SGD classifier made the errors it did; as a linear model, it just assigned a weight per pixel and when it sees a new image it sums up the weighted pixel intensities to get a score for each class\n  3s are different from 5s mainly with the vertical line that connects the top horizontal line and the bottom arc; this means the classifier would be quite sensitive to image shifting and rotation\n  Could preprocess images to make sure they\u0026rsquo;re centered/not too rotated\n  Multilabel Classification  Assigning multiple labels to one sample E.g. detecting multiple people\u0026rsquo;s faces in a photo, or whether a digit is even or odd Outputs multiple binary tags. E.g. y_pred = [1, 0, 1] (yes to classes 1 and 3, no to 2)  from sklearn.neighbors import KNeighborsClassifier   Side note: % (Modulus) yields the remainder when the first operand is divided by the second  5 % 3 # 3 goes into 5 once, with remainder 2  2  10 % 3 # 3 goes into 10 three times, with remainder 1  1  y_train_large = (y_train \u0026gt;= 7) y_train_odd = (y_train % 2 == 1) y_multilabel = np.c_[y_train_large, y_train_odd]  y_multilabel  array([[False, True], [False, False], [False, False], ..., [False, True], [False, False], [ True, False]])   label 1: digit is 7 or above label 2: digit is odd  knn_clf = KNeighborsClassifier() knn_clf.fit(X_train, y_multilabel)  KNeighborsClassifier()   KNeighbors supports multilabel classification, though not all classifiers do  knn_clf.predict([some_digit]) # some_digit = 5  array([[False, True]])   One approach to measure multilabel performance is measure F1 score for each individual label and compute the average score across them:  y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)  f1_score(y_multilabel, y_train_knn_pred, average=\u0026quot;macro\u0026quot;)  0.976410265560605   this assumes that all labels are equally important, which might not always be the case can give labels weight according to number of appeareances in training data by setting average=\u0026quot;weighted\u0026quot;  Multioutput Classification  Generalization of multilabel classification where each label can be multiclass (i.e. have more than two possible values) following example denoises images by predicting what the pixel intensitiy should be for each pixel in a sample (multiple classes for multiple labels) this somewhat blurs the line between classification and regression (predicting pixel intensity is more of a regression task)  noise = np.random.randint(0, 100, (len(X_train), 784)) X_train_mod = X_train + noise noise = np.random.randint(0, 100, (len(X_test), 784)) X_test_mod = X_test + noise y_train_mod = X_train y_test_mod = X_test  def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.binary, interpolation=\u0026quot;nearest\u0026quot;) plt.axis(\u0026quot;off\u0026quot;)  knn_clf.fit(X_train_mod, y_train_mod)  KNeighborsClassifier()  some_index = np.random.randint(0, len(X_test_mod)) clean_digit = knn_clf.predict([X_test_mod[some_index]])  plt.figure(figsize=(8,4)) plt.subplot(121); plot_digit(X_test_mod[some_index]) plt.subplot(122); plot_digit(clean_digit)  Exercises in a Separate Notebook","date":"2020-09-26","permalink":"https://billwarker.com/posts/handson-ml-ch3/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 3 Notes"},{"content":"Notes Main Steps:\n Frame the problem Get data EDA (exploratory data analysis) Prepare the data for ML Model Selection Tune the model Present solution Launch, monitor, iterate  Look at the big picture  predict the median housing price for a district in CA what\u0026rsquo;s the business objective (not building a model for fun) ask: what is the current, non-ML solution? why can\u0026rsquo;t we use that start thinking about/designing the system  Pipelines\n sequence of data processing components typically runs asynchronously: When you execute something synchronously, you wait for it to finish before moving on to another task. When you execute something asynchronously, you can move on to another task before it finishes. components are self contained, downstream components can keep working for a while by just using the last output from the broken component (async)  Types of Regression\n multiple regression: multiple features to make a prediction univariate regression: only trying to predict a single value  Selecting a performance measure\n Root Mean Square Error (RMSE):  $$ RMSE(X,h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^{2}} $$\n gives higher weight to larger errors lowercase italic font for scalars, lowercase bold for vectors, uppercase bold for matrices if there are many outliers, then Mean Absolute Error (MAE) might be a better cost function:  $$ MAE(X,h) = \\frac{1}{m} \\sum_{i=1}^{m} \\lvert h(x^{(i)}) - y^{(i)}\\rvert$$\n different ways to measure difference between vectors RMSE is euclidian distance, $l_2$ norm computing sum of absolutes is $l_1$ norm, measures the distance between two points if you can only travel along orthogonal (perpendicular) lines $l_0$ just gives the number of non-zero elements in vector higher the norm index, the more it focuses on large values and neglects smaller ones. this is why RMSE is more sensitive to outliers than MAE  Verify Assumptions\n list and verify assumptions e.g. what does the output exactly need to be, what do we think is true about the problem/solution we\u0026rsquo;ve proposed  Get the Data  usually data is in DBs or spread across many files, so common first step is jumping through the hoops of access, getting used to schemas, legal precautions, etc. best to automate process of fetching data, future proof against changes  import os import tarfile from six.moves import urllib  DOWNLOAD_ROOT = \u0026quot;https://raw.githubusercontent.com/ageron/handson-ml/master/\u0026quot; HOUSING_PATH = os.path.join(\u0026quot;datasets\u0026quot;, \u0026quot;housing\u0026quot;) HOUSING_URL = DOWNLOAD_ROOT + \u0026quot;datasets/housing/housing.tgz\u0026quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.makedirs(housing_path) tgz_path = os.path.join(housing_path, \u0026quot;housing.tgz\u0026quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close()  fetch_housing_data()  import pandas as pd  def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(HOUSING_PATH, \u0026quot;housing.csv\u0026quot;) return pd.read_csv(csv_path)  housing = load_housing_data() housing.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY     housing.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total_rooms 20640 non-null float64 total_bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB  housing[\u0026quot;ocean_proximity\u0026quot;].value_counts()  \u0026lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64  housing.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value     count 20640.000000 20640.000000 20640.000000 20640.000000 20433.000000 20640.000000 20640.000000 20640.000000 20640.000000   mean -119.569704 35.631861 28.639486 2635.763081 537.870553 1425.476744 499.539680 3.870671 206855.816909   std 2.003532 2.135952 12.585558 2181.615252 421.385070 1132.462122 382.329753 1.899822 115395.615874   min -124.350000 32.540000 1.000000 2.000000 1.000000 3.000000 1.000000 0.499900 14999.000000   25% -121.800000 33.930000 18.000000 1447.750000 296.000000 787.000000 280.000000 2.563400 119600.000000   50% -118.490000 34.260000 29.000000 2127.000000 435.000000 1166.000000 409.000000 3.534800 179700.000000   75% -118.010000 37.710000 37.000000 3148.000000 647.000000 1725.000000 605.000000 4.743250 264725.000000   max -114.310000 41.950000 52.000000 39320.000000 6445.000000 35682.000000 6082.000000 15.000100 500001.000000      percentiles: what percentage of the data falls beneath this point. i.e. if 50th percentile is 100 for an attribute that means half of all the samples have a value less than 100 for that attribute if mean varies a lot from median that speaks to the presence of outliers pulling it up/down  %matplotlib inline import matplotlib.pyplot as plt  housing.hist(bins=50, figsize=(20,15)) plt.show()   Median income doesn\u0026rsquo;t seem to be expressed as USD Median house value and age seem to be capped features have different scales (needs feature scaling)  Create a test set\n before anything else, set aside test set this will avoid data snooping bias; i.e. fitting the model to better generalize on the test set  import numpy as np  def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices]  train_set, test_set = split_train_test(housing, 0.2)  len(train_set)  16512  len(test_set)  4128   this function will regenerate different sets on each run you can seed the random number generator, but this breaks if you add new data to the dataset (regenerates new train/test) eventually data you\u0026rsquo;ve trained on before will make it into the test set on multiple reruns with this pipeline can use each instance\u0026rsquo;s indentifier (assuming unique and immutable) to decide whether or not it should go in the test set code below computes a hash of each instance\u0026rsquo;s indentifier and puts that instance in the test set if hash is lower or equal to 20% of the maximum hash value. ensures consistency across multiple runs, and the test set will always contain 20% of the new data, but never any instance that was previously in the training set  from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) \u0026amp; 0xffffffff \u0026lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set]  housing_with_id = housing.reset_index() train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026quot;index\u0026quot;)  train_set   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   3 3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY   6 6 -122.25 37.84 52.0 2535.0 489.0 1094.0 514.0 3.6591 299200.0 NEAR BAY   ... ... ... ... ... ... ... ... ... ... ... ...   20635 20635 -121.09 39.48 25.0 1665.0 374.0 845.0 330.0 1.5603 78100.0 INLAND   20636 20636 -121.21 39.49 18.0 697.0 150.0 356.0 114.0 2.5568 77100.0 INLAND   20637 20637 -121.22 39.43 17.0 2254.0 485.0 1007.0 433.0 1.7000 92300.0 INLAND   20638 20638 -121.32 39.43 18.0 1860.0 409.0 741.0 349.0 1.8672 84700.0 INLAND   20639 20639 -121.24 39.37 16.0 2785.0 616.0 1387.0 530.0 2.3886 89400.0 INLAND    16512 rows × 11 columns\n   note: this approach looks fugazi, indices aren\u0026rsquo;t shuffled\n  the problem still exists but would need a better implementation than what\u0026rsquo;s here\n  if you use row index as a unique identifier, you must make sure that new data always gets appended to the end of the dataset and a row is never deleted\n  instead, you can try to engineer a unique ID for each row by combining some of the (ideally most stable/constant) features\n  e.g. a district\u0026rsquo;s latitude/longitude is guaranteed to be stable for a few million years lol:\n  housing_with_id['id'] = housing[\u0026quot;longitude\u0026quot;] * 1000 + housing[\u0026quot;latitude\u0026quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026quot;id\u0026quot;)  len(housing_with_id)  20640  len(housing_with_id[\u0026quot;id\u0026quot;].unique()) # also fugazi  12590   the approach above in the book obviously doesn\u0026rsquo;t work haha  from sklearn.model_selection import train_test_split  train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)  train_set   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     14196 -117.03 32.71 33.0 3126.0 627.0 2300.0 623.0 3.2596 103000.0 NEAR OCEAN   8267 -118.16 33.77 49.0 3382.0 787.0 1314.0 756.0 3.8125 382100.0 NEAR OCEAN   17445 -120.48 34.66 4.0 1897.0 331.0 915.0 336.0 4.1563 172600.0 NEAR OCEAN   14265 -117.11 32.69 36.0 1421.0 367.0 1418.0 355.0 1.9425 93400.0 NEAR OCEAN   2271 -119.80 36.78 43.0 2382.0 431.0 874.0 380.0 3.5542 96500.0 INLAND   ... ... ... ... ... ... ... ... ... ... ...   11284 -117.96 33.78 35.0 1330.0 201.0 658.0 217.0 6.3700 229200.0 \u0026lt;1H OCEAN   11964 -117.43 34.02 33.0 3084.0 570.0 1753.0 449.0 3.0500 97800.0 INLAND   5390 -118.38 34.03 36.0 2101.0 569.0 1756.0 527.0 2.9344 222100.0 \u0026lt;1H OCEAN   860 -121.96 37.58 15.0 3575.0 597.0 1777.0 559.0 5.7192 283500.0 \u0026lt;1H OCEAN   15795 -122.42 37.77 52.0 4226.0 1315.0 2619.0 1242.0 2.5755 325000.0 NEAR BAY    16512 rows × 10 columns\n  the above are random sampling methods which work well enough on a large dataset if the dataset is small then you should do stratified sampling (i.e. take steps to ensure that the sample is representative of the whole pop.) divide the data into different groups (strata) and randomly sample from those in a way which is representative e.g. stratify by median income:  # use pd.cut to bin the median income into categories housing['income_cat'] = pd.cut(housing['median_income'], bins=[0., 1.5, 3., 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])  housing['income_cat'].hist()  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc82c0cf90\u0026gt;  from sklearn.model_selection import StratifiedShuffleSplit  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing['income_cat']): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index]  # checking to see if it worked as expected strat_test_set[\u0026quot;income_cat\u0026quot;].value_counts()/len(strat_test_set)  3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64   the stratified sampling matches the proportions seen in the histogram; the test set in this instance is representative of the income_cat distribution found in the whole dataset now remove income_cat attribute to restore data to original form  for set_ in (strat_train_set, strat_test_set): set_.drop(\u0026quot;income_cat\u0026quot;, axis=1, inplace=True)  Discover and Visualize the Data to Gain Insights (EDA)  only explore the training set, pust the test set aside if training set is large, you might want to just explore a sample to make manipulations faster create a copy of the training set so you can play around without harming it  Visualizing Geographical Data:\nhousing = strat_train_set.copy()  housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc822c8990\u0026gt;   Can visualize density in the geographical viz with lower alpha  housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=housing['population']/100, label='population', figsize=(10,7), c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True) plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7fcc80e608d0\u0026gt;   prices are higher along the coast (duh), with some hotspots in the bay area and around LA could use a clustering algo to find many clusters, and then create features that measure proximity to the cluster centers Ocean proxmity is useful feature  Look for correlations:\n standard correlation coefficient only measures linear relationships, misses any other kinds  corr_matrix = housing.corr()  corr_matrix['median_house_value'].sort_values(ascending=False)  median_house_value 1.000000 median_income 0.687160 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population -0.026920 longitude -0.047432 latitude -0.142724 Name: median_house_value, dtype: float64  from pandas.plotting import scatter_matrix  attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age'] scatter_matrix(housing[attributes], figsize=(12,8)) plt.show()   diagonal lines are histograms of attributes  # focus on median income housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc83a74110\u0026gt;   clear correlation can also see the cap at $500k quite clearly other less visible lines at \\$350K, $450K; might want to remove from data to stop model from picking up on quirks  Experimenting with Attribute Combinations\n EDA shows some interesting correlations so far some attributes are tail heavy, so could transform to normal distribution by taking their log can take combinations of features to get relationships; some attributes make less sense when considered independently e.g. compute avg. rooms per household as rooms/households  housing['rooms_per_household'] = housing['total_rooms']/housing['households'] housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms'] housing['population_per_household'] = housing['population']/housing['households']  # check corr matrix again corr_matrix = housing.corr() corr_matrix['median_house_value'].sort_values(ascending=False)  median_house_value 1.000000 median_income 0.687160 rooms_per_household 0.146285 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population_per_household -0.021985 population -0.026920 longitude -0.047432 latitude -0.142724 bedrooms_per_room -0.259984 Name: median_house_value, dtype: float64   bedrooms per room is much more correlated (negatively) to median house value than total rooms/bedrooms houses with a lower bedroom/room ratio seem to be more expensive EDA doesn\u0026rsquo;t need to be completely thorough, point is to get insights that will get you a reasonably decent prototype iterative process  Prepare the Data for Machine Learning Algorithms write functions to prepare data because:\n can reproduce on fresh data build a library of transformation functions can use in live system easier experimentation  housing = strat_train_set.drop(\u0026quot;median_house_value\u0026quot;, axis=1) housing_labels = strat_train_set[\u0026quot;median_house_value\u0026quot;].copy()  Handling Missing Values\n 3 options: get rid of of corresponding samples get rid of the entire attribute impute values to (mean, median, etc)  from sklearn.impute import SimpleImputer  imputer = SimpleImputer(strategy=\u0026quot;median\u0026quot;)  # create copy of numerical features housing_num = housing.drop(\u0026quot;ocean_proximity\u0026quot;, axis=1)  imputer.fit(housing_num) imputer.statistics_  array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])  X = imputer.transform(housing_num)  housing_tr = pd.DataFrame(X, columns=housing_num.columns)  Handling Text and Categorical Attributes\n encode to numerical values  housing_cat = housing[[\u0026quot;ocean_proximity\u0026quot;]]  from sklearn.preprocessing import OrdinalEncoder  ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)  housing_cat_encoded[:5]  array([[0.], [0.], [4.], [1.], [0.]])  ordinal_encoder.categories_  [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]   ordinal encoding places ranking on the attributes, so some algos might interpret nearby values as similar (4\u0026amp;3 are more similar than 1\u0026amp;7, although these are all just arbitrary indices) for nominal categories its better to one-hot encode them; create a binary attribute per category also known as creating dummy variables  from sklearn.preprocessing import OneHotEncoder  cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat)  housing_cat_1hot  \u0026lt;16512x5 sparse matrix of type '\u0026lt;class 'numpy.float64'\u0026gt;' with 16512 stored elements in Compressed Sparse Row format\u0026gt;   sparse matrices don\u0026rsquo;t use memory to store zero elements  cat_encoder.categories_  [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]   if a categorical attribute has a large number of categories, one hot encoding will create a large number of input features and may hurt performance. in this case embeddings are useful (denser representations)  Custom Transformers\n write to align with Scikit-Learn so you can use pipelines add hyperparameters to gate any data preparation steps you aren\u0026rsquo;t sure about  from sklearn.base import BaseEstimator, TransformerMixin  rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6  class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, population_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household]  attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs = attr_adder.transform(housing.values)  np.c_ stacks arrays on their last axis (turns column vectors into a matrix)\nnp.c_[np.array([1,2,3]), np.array([4,5,6])]  array([[1, 4], [2, 5], [3, 6]])  Feature Scaling:\n  ML algos don\u0026rsquo;t perform well when features have different scales\n  Two ways to feature scale:\n  min-max scaling: attributes are shifted and rescaled so they range from 0-1. $$x_{scaled} = \\frac{x_n - x_{min}}{x_{max} - x_{min}}$$\n  standardization: subtract the mean and divide by standard deviation\n  standardization is less affected by outliers in the data $$x_{scaled} = \\frac{x_n - {\\bar{x}}}{\\sigma}$$\n  fit all transformers to the training set, then use them on the test\n  Transformation Pipelines\nfrom sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler  num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy=\u0026quot;median\u0026quot;)), ('attribs_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()) ])  housing_num_tr = num_pipeline.fit_transform(housing_num)   calls fit_transform() on all of the transformers sequentially  from sklearn.compose import ColumnTransformer  num_attribs = list(housing_num) cat_attribs = [\u0026quot;ocean_proximity\u0026quot;] full_pipeline = ColumnTransformer([ (\u0026quot;num\u0026quot;, num_pipeline, num_attribs), (\u0026quot;cat\u0026quot;, OneHotEncoder(), cat_attribs), ])  housing_prepared = full_pipeline.fit_transform(housing)  Select and Train a Model Training and Evaluating on the Training Set\n preprocessing steps make training models simple  from sklearn.linear_model import LinearRegression  lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels)  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)  some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(f\u0026quot;Predictions: {lin_reg.predict(some_data_prepared)}\u0026quot;) print(f\u0026quot;Labels: {list(some_labels)}\u0026quot;)  Predictions: [211944.80589799 321295.84907457 210851.33029021 62359.51850965 194954.19182968] Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]  from sklearn.metrics import mean_squared_error  housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) print(lin_rmse)  68898.54780411992   error of \\$68K here isn\u0026rsquo;t very satisfying (housing prices range from \\$120K to \\$265K) if a model is underfitting the data it means that the features don\u0026rsquo;t provide enough information to make good decisions, or model is too weak  from sklearn.tree import DecisionTreeRegressor  tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels)  DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')  housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse  0.0  Better Evaluation with Cross Validation\n K-fold Cross Validation splits the training set into K folds, training the model K times by testing on one of the folds and composing the training set on the remaining K-1 Sklearns CV feature expects a utility function (greater is better) as opposed to a cost function (lower is better), so the scoring function is actually opposite of MSE, which is why the following code uses -scores in the square root  from sklearn.model_selection import cross_val_score  scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) tree_rmse_scores = np.sqrt(-scores)  def display_scores(scores): print(\u0026quot;Scores:\u0026quot;, scores) print(\u0026quot;Mean:\u0026quot;, scores.mean()) print(\u0026quot;Standard Deviation\u0026quot;, scores.std())  display_scores(tree_rmse_scores)  Scores: [67971.85204287 67101.92229431 69341.35567834 66956.22248918 69739.80377843 72468.59865874 66736.84144169 67241.24548885 72220.02744352 70187.38922156] Mean: 68996.52585375118 Standard Deviation 2037.77518366374   CV not only allows you to get an averaged estimate of model\u0026rsquo;s performance, but allows you to get an idea of how precise this estimate is through the standard deviation  lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores)  Scores: [67500.31361237 68404.48325957 68239.95757613 74813.56736728 68419.88576794 71632.92651865 65216.31837467 68702.06708289 71793.11060978 68131.30099374] Mean: 69285.3931163006 Standard Deviation 2576.7108344336184  from sklearn.ensemble import RandomForestRegressor  forest_reg = RandomForestRegressor() forest_reg.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. \u0026quot;10 in version 0.20 to 100 in 0.22.\u0026quot;, FutureWarning) RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  forest_mse = mean_squared_error(housing_labels, forest_reg.predict(housing_prepared)) forest_rmse = np.sqrt(forest_mse) print(forest_rmse)  21923.44698175341  forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores)  display_scores(forest_rmse_scores)  Scores: [50985.57169248 49600.72682606 50759.78939086 54557.52337524 51089.83837069 55891.91762608 51010.98084994 50261.62522741 55934.1764328 52825.16027951] Mean: 52291.73100710661 Standard Deviation 2239.368467655192   Forest looks more promising (lower RMSE), but the sizeable difference between CV performance and training set performance indicates overfitting on the training set Could simplify it, regularize it, or get more training data Ideally you want to shortlist 2-5 models promising models total can use joblib library to save promising models for experimentation/later iterations  Fine-Tune Your Model  fine tune the shortlist of models  Grid Search\n grid search will automate cross-validation on all combinations of hyperparameters specified  from sklearn.model_selection import GridSearchCV  param_grid = [ {'n_estimators': [3,10,30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]}, ]  forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) GridSearchCV(cv='warn', error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid='warn', n_jobs=None, param_grid=[{'max_features': [2, 4, 6, 8], 'n_estimators': [3, 10, 30]}, {'bootstrap': [False], 'max_features': [2, 3, 4], 'n_estimators': [3, 10]}], pre_dispatch='2*n_jobs', refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=0)   try powers of 10 if you have no clue what a hyperparameter should have different dictionaries in param_grid specify separate grid searches  grid_search.best_params_  {'max_features': 8, 'n_estimators': 30}  grid_search.best_estimator_  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)   refit=True argument for GridSearchCV will retrain on the entire training set after finding best estimator hyperparams  cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[\u0026quot;mean_test_score\u0026quot;], cvres[\u0026quot;params\u0026quot;]): print(np.sqrt(-mean_score), params)  65307.87885616995 {'max_features': 2, 'n_estimators': 3} 56745.41295641033 {'max_features': 2, 'n_estimators': 10} 52896.1050644663 {'max_features': 2, 'n_estimators': 30} 59681.45476015563 {'max_features': 4, 'n_estimators': 3} 52735.85674503226 {'max_features': 4, 'n_estimators': 10} 50892.22744742235 {'max_features': 4, 'n_estimators': 30} 59799.42942971745 {'max_features': 6, 'n_estimators': 3} 52650.77632649698 {'max_features': 6, 'n_estimators': 10} 50570.40999405222 {'max_features': 6, 'n_estimators': 30} 59094.68896450388 {'max_features': 8, 'n_estimators': 3} 52492.707418330094 {'max_features': 8, 'n_estimators': 10} 50432.33406200777 {'max_features': 8, 'n_estimators': 30} 62530.835324885855 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 55183.05939812397 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 60507.19412492283 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 53097.84240758412 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 59391.108929740585 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 52293.331430755774 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}   can include data preparation steps as hyperparameters in grid search  Randomized Search\n when hyperparameter search space is large, randomized usually gets comparable results faster explores $n$ different values for each hyperparameter for $n$ total iterations gives you more control over computing budget  Ensemble Methods\n Can combine models that perform best Will often perform better than the best individual model, especially if the models make different types of errors  Analyze the Best Models and their Errors\n Can gain insights by inspecting best models Random Forest can give feature importances:  feature_importances = grid_search.best_estimator_.feature_importances_  extra_attribs = [\u0026quot;rooms_per_hhold\u0026quot;, \u0026quot;pop_per_hhold\u0026quot;, \u0026quot;bedrooms_per_room\u0026quot;] cat_encoder = full_pipeline.named_transformers_[\u0026quot;cat\u0026quot;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True)  [(0.3938202057578966, 'median_income'), (0.15133729393112902, 'INLAND'), (0.1029603450675752, 'bedrooms_per_room'), (0.0754751387108862, 'pop_per_hhold'), (0.06620720663844283, 'longitude'), (0.06338183338235212, 'latitude'), (0.04206999831411881, 'housing_median_age'), (0.03242004947783398, 'rooms_per_hhold'), (0.015482413139961633, 'total_bedrooms'), (0.014766900750197388, 'population'), (0.014573461291699393, 'total_rooms'), (0.014411203134276336, 'households'), (0.006778298384461372, '\u0026lt;1H OCEAN'), (0.0032380720741344592, 'NEAR OCEAN'), (0.002983526044293852, 'NEAR BAY'), (9.40539007408315e-05, 'ISLAND')]   could drop less important features and try retraining could also analyze the specific errors the system makes and try to understand why they\u0026rsquo;re occuring and what could fix (extra features, cleaning outliers, removing uninformative features)  Evaluate Your Sytem on the Test Set  just apply pipeline to the test set (run transform, not fit_transform - we do not want to fit it to the training set!)  final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(\u0026quot;median_house_value\u0026quot;, axis=1) y_test = strat_test_set[\u0026quot;median_house_value\u0026quot;].copy()  X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared)  final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse)  print(final_rmse)  48026.91757298586   in some situations a point estimate of the generalization error will not be enough evidence to launch you can get an idea of precision by computing a 95% confidence interval for the generalization error:  from scipy import stats  confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))  array([46060.77003181, 49915.67977301])   performance will likely be a bit worse than what was measured in cross-validation resist temptation to tweak numbers on the test set present solution, highlighting assumptions, methodology, system limitations create presentations with clear visuals and keep points summarized  Launch, Monitor, and Maintain Your System   launch in prod, polishing code, writing tests and documentation, etc\n  can deploy model by loading it in production environment or wrapping it in a REST api (i.e. cloud functions, microservices)\n  this offers easier updating of system without disrupting main application, and can handle load balancing\n  can deploy in cloud (GCP)\n  need to write monitoring code after deployment to make sure performance doesn\u0026rsquo;t rot over time\n  models decay as the world changes and old data becomes less relevant, need to feed with new data\n  model performance can be inferred from downstream metrics (e.g. how many sales your recommender system is generating)\n  can use human testers to verfiy model outputs (experts, or crowdsourced from platforms like Amazon Mechanical Turk)\n  putting models in prod and maintaining them can be more work than actually creating them\n  automate as much of the process as possible:\n  collecting fresh data regularly and labelling (e.g. using human raters)\n  writing scripts to train the model and fine-tune hyperparameters automatically\n  writing scripts to evaluate new model vs. old model on updated test sets, and deploying the new model if performance has not decreased\n  monitor data quality as well, poor quality data leaking in can be hard to detect\n  create backups of every model and have infrastructure to rollback previous models quickly, in case new ones start failing badly for some reason\n  backups help for investigation/comparison\n  keep backups of datasets as well\n  Exercises  Try a Support Vector Machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\u0026quot;linear\u0026rdquo; (with various values for the C hyperparameter) or kernel=\u0026quot;rbf\u0026rdquo; (with various values for the C and gamma hyperparameters). Don’t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?  from sklearn.svm import SVR  svr = SVR() param_grid = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1.0, 10.0, 100.0]}, {'kernel': ['rbf'], 'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'gamma': ['scale', 'auto']} ] grid_search = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) GridSearchCV(cv='warn', error_score='raise-deprecating', estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False), iid='warn', n_jobs=None, param_grid=[{'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'kernel': ['linear']}, {'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'gamma': ['scale', 'auto'], 'kernel': ['rbf']}], pre_dispatch='2*n_jobs', refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=0)  best_svr = grid_search.best_estimator_  best_svr  SVR(C=100.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated', kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)  np.sqrt(-grid_search.best_score_)  72427.44454616884  Try replacing GridSearchCV with RandomizedSearchCV.  from sklearn.model_selection import RandomizedSearchCV from scipy.stats import uniform from scipy.stats import norm from pprint import pprint  random_forest = RandomForestRegressor()  print(\u0026quot;Parameters currently in use:\\n\u0026quot;) pprint(random_forest.get_params())  Parameters currently in use: {'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}  random_forest = RandomForestRegressor() n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)] max_features = ['auto', 'sqrt'] max_depth = [int(x) for x in np.linspace(10,110, num=11)] max_depth.append(None) min_samples_split = [2, 5, 10] min_samples_leaf = [1,2,4] bootstrap = [True, False] random_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap} random_search = RandomizedSearchCV(estimator=random_forest, param_distributions=random_grid, scoring='neg_mean_squared_error', verbose=1, n_iter=10, cv=3, n_jobs=-1, return_train_score=True)  random_search.fit(housing_prepared, housing_labels)  Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 31.4min finished RandomizedSearchCV(cv=3, error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_sta... param_distributions={'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=1)  best_forest = random_search.best_estimator_  best_forest  RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=110, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  np.sqrt(-random_search.best_score_)  49359.61219717077  Try adding a transformer in the preparation pipeline to select only the most important attributes.  class FeatureImportancesFilter(BaseEstimator, TransformerMixin): def __init__(self, feature_importances, importance_cutoff = 0.9): # no *args or **kargs self.feature_importances = feature_importances feature_indices = [i for i in range(len(self.feature_importances))] self.sorted_feature_importances = sorted(zip(self.feature_importances, feature_indices), reverse=True) self.importance_cutoff = importance_cutoff def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): importance_sum = 0 included_features = list() for importance, ix in self.sorted_feature_importances: if not importance_sum \u0026gt;= self.importance_cutoff: importance_sum += importance included_features.append(ix) else: break return X[:, included_features]   note: might need to fit to actual random forest regressor, or breaks if input shape changes  housing_prepared.shape  (16512, 16)  importance_filter = FeatureImportancesFilter(feature_importances, importance_cutoff=0.9)  importance_filter.transform(housing_prepared).shape  (16512, 8)  Try creating a single pipeline that does the full data preparation plus the final prediction.  e2e_pipeline = Pipeline([ ('data_prep', full_pipeline), ('filter', FeatureImportancesFilter(feature_importances)), ('clf', best_forest) ])  Automatically explore some preparation options using GridSearchCV.  e2e_grid_params = {'filter__importance_cutoff': (0.5, 0.6, 0.65)}  e2e_grid_search = GridSearchCV(e2e_pipeline, e2e_grid_params, scoring='neg_mean_squared_error', verbose=1, cv=3)  e2e_grid_search.fit(housing, housing_labels)  Fitting 3 folds for each of 3 candidates, totalling 9 fits [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 9 out of 9 | elapsed: 1.4min finished GridSearchCV(cv=3, error_score='raise-deprecating', estimator=Pipeline(memory=None, steps=[('data_prep', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3, transformer_weights=None, transformers=[('num', Pipeline(memory=None, steps=[('imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy='median'... min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False), iid='warn', n_jobs=None, param_grid={'filter__importance_cutoff': (0.5, 0.6, 0.65)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring='neg_mean_squared_error', verbose=1)  np.sqrt(-e2e_grid_search.best_score_)  66661.60437351998  e2e_grid_search.best_params_  {'filter__importance_cutoff': 0.65} ","date":"2020-08-17","permalink":"https://billwarker.com/posts/handson-ml-ch2/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 2 Notes"},{"content":"Notes Types of ML\n  ML: algorithms learning from data and improving performance on a task\n  advantage over rule based systems is that machine can update parameters/logic with new/more data (refreshing the model)\n  applying ML to discover/understand patterns in the data: data mining\n  Supervised learning: training a model with labelled examples. can be used for classification tasks (predict a discrete category) or regression (predict a continuous number)\n  Unsupervised learning algos: clustering unlabelled data, visualizing and dimensionality reduction, association rule learning\n  hierarchical clustering: sub-dividing clusters into smaller groups\n  dimensionality reduction: simplify data without losing too much information. i.e. merging correlated features. can help performance, takes up less disk and memory space\n  anomoly detection\n  association rule learning: relations between attributes (similar to data mining)\n  Semi-supervised learning. i.e. Google Photos, looks at unlabelled pictures to find common faces. once given a label for a person it can name everyone in the photo. mixed hierarchical models\n  Reinforcement learning: agent learns its environment and selects a policy (actions, strategy) to optimize some reward\n  Batch vs. Online Learning\n  Batch: training a model on all available data. train the system and then put it in prod. retrain new versions of the model with new data\n  requires a lot of computing resources, can be expensive. not suitable for autonomous systems with limited space for data (i.e. Mars rover)\n  Online: train system incrementally on mini-batches of data\n  great for continuous flows of data or limited storage resources\n  can be used to train on datasets that won\u0026rsquo;t fit in memory (out-of-core learning)\n  learning rate: how fast model adapts to new data\n  too high, forgets old data rapidly, too low, system has inertia (also less sensitive to noisey data)\n  need to monitor online systems if garbage data starts coming in\n  Instance-Based vs. Model-Based Learning\n  how a system generalizes (i.e. answers examples its never seen before)\n  Instance-based: generalizes to new examples by comparing similarity to training examples\n  i.e. KNN\n  Model-based: builds a model to predict new data\n  select a model (i.e. linear model) to represent the data\u0026rsquo;s pattern\n  tune model on a utility or cost function\n  if a model doesn\u0026rsquo;t generalize well, you can try again with better quality training data, more features, or a stronger model (e.g. polynomial vs. basic linear)\n  adding more data tends to get better results on all kinds of algos, to the point where performance can be identical with enough data\n  data needs to be representitive of the problem space trying to model, more data can eliminate noise but procedure needs to be solid or it risks sampling bias\n  Feature Engineering\n feature selection, picking the most useful features feature extraction, combining existing features to produce more useful ones + dimensionality reduction creating new features with the intro of new data  Performance\n  overfitting: performing well on the training data but not generalizing well\n  machine learning can pick up on noise in the data and sometimes even irrelevant features/useless metadata (i.e. data ids/index), detecting false patterns\n  overfitting happens when the model is too complex relative to the amount and noiseness of the data\n  regularization is constraining a model to make it simpler can help overfitting\n  controlled by model hyperparameters (knobs to tweak on the model itself, such as learning rate)\n  underfitting is opposite problem, model is too simple\n  fix it with a more complex model, more/better features, reducing regularization constraints\n  test models to see if they generalize well\n  split data into training and test sets to get error rate on new cases (out of sample/generalization error)\n  if training error is now but oos error is high, overfitting\n  use a third validation set to compare models/hyperparameters, then select the best one and use it one the test\n  cross validation splits the training set into subsets which are used for validation\n  need to make assumptions about the data to pick models reasonably\n  Questions How would you define ML?\n Algorithms that allow a computer to learn from data to improve on a task and generalize to new examples well  Four types of problems where it shines?\n Problems that traditionally require too many rules or hand-tuning Complex problems with no easy logic based model Problems where underlying strategy/solutions change over time, so new model can help adapt Data mining, learning underlying patterns in data  What is a labelled training set?\n Data that has the variable of interest classified (independent variable), and information about it in other attribures (dependent variables)  Two most common supervised tasks?\n Regression: predicting a continuous number for the target variable Classification: predicting a discrete category of target variable  Four kinds of unsupervised tasks?\n Clustering: creating groups in unstructured data Dimesionality Reduction: reducing the number of attributes in the training set while keeping most of the variance(underlying signal) Anomoly Dection: finding outliers Visualization Association Rule learning: data mining, learning patterns in the data  What type of ML algo would you use to allow a robot to walk in various unknown terrains?\n Reinforcement Learning  What type of algo would you use to segment your customers into multiple groups?\n Clustering, Unsupervised Learning  Would you frame the problem of spam detection as a supervised learning problem?\n Supervised; we can use examples of spam and ham (labelled training data) to create a model that identifies which is which  What is an online learning system?\n A system that can update with new data as it comes in, ingesting as it comes in through mini-batches  What is an out-of-core learning system?\n Using online learning to train the model on a dataset that wouldn\u0026rsquo;t fit inside the computers memory if you tried to train it in one giant batch.  What type of learning algo relies on similarity measures to make predictions?\n Instanced-based models, i.e. KNN  Difference between a model parameter and a hyperparameter?\n A model parameter is the coefficient determined by the algorithm to apply to a attribute/feature in the data when making predictions, a hyperparameter is an aspect of the model that you can adjust to change how it is trained  What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n These algos look to fit the model to the data (i.e. represent the problem with some simplified version of it). The strategy is to improve performance in respect to some cost/utility function. They make new predictions by applying the policy/parameters determined through training on the new data\u0026rsquo;s features  Four main challenges in ML?\n Not enough data Non-representative data Poor quality data Overfitting/Underfitting  If a model performs well on the training data but generalizes poorly to new examples, what is happening? 3 possible solutions?\n Model is overfitting to the training data and can\u0026rsquo;t generalize well to the new examples You can regularize the model (i.e. constrain it) to be less representative of the training data Train the model on more data/more representative data Tune the model on a validation set  What is a test set and why would you want to use it?\n Test the performance of the model on new examples to understand how well it generalizes to new data (i.e. the whole point)  What can go wrong if you tune hyperparameters on the test set?\n You fit the model to work well on the testing set, overfitting it and reducing the chance of generalizing well  What is cross validation, why is it better than a validation set?\n Cross validation takes different chunks of the training data and uses them iteratively to train and validate the model, creating a more robust model (training on different samples) and is a more economic use of data. ","date":"2020-08-02","permalink":"https://billwarker.com/posts/handson-ml-ch1/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 1 Notes"},{"content":"The Standard Error of the Mean The Standard Error of the Mean ($SE$) is the standard deviation of the sample distribution of the sample mean. To understand what this means, let\u0026rsquo;s break that sentence down in reverse order (i.e. chronologically):\nSample Mean: we have some probability density function $P$ for a population. We take a sample of $N$ instances from it and calculate our statistic of interest - in this case the mean, $\\bar{x}$. We have to take samples because it is hard/impossible to look at every instance in an entire population to calculate the true mean $\\mu$ hidden to us in nature (i.e. say we were interested in the average weight of all monkeys on the planet).\nSample Distribution: we take many samples (the number of which denoted by $M$) from the population\u0026rsquo;s probability density function $P$ and calculate the sample mean $\\bar{x}$ for each one. All of these sample means can be lumped together into a distribution $S$, which approximates a normal distribution the higher $M$ is due to the Central Limit Theorum. We want to create a sampling distribution because it allows us to reason about the likelihood of different values the sample mean can be. By doing so, we can look at the mean of this distribution and conclude that its the most likely value for $\\mu$ given the data we\u0026rsquo;ve used.\nStandard Deviation: The standard deviation is a metric that describes the dispersion of a dataset/distribution in relation to its mean. It speaks to how close/far the density of the distribution is spread in relation to its mean. The standard deviation of a sample distribution - our standard error $SE$ - is an indication of how representative the distribution is of our true mean $\\mu$. The smaller it is, the more representative it can be said to be; the larger it is, the harder it is to trust.\nTLDR: The Standard Error matters because it allows us to better understand how representative our sampling distribution is (i.e. our model of the true mean).\n$SE$ can be calculated with this formula:\n$$SE = \\frac{\\sigma}{\\sqrt{N}}$$\nwhere ${\\sigma}$ is the standard deviation of the population\u0026rsquo;s probability density function $P$ and $N$ is the number of instances in a sample.\nUnderstanding the Standard Error through Simulation import numpy as np import scipy as sp import pandas as pd import matplotlib.pyplot as plt  # create a random probability distribution function to model our population # in this case a Maxwell continuous random variable (picked randomly) x = np.linspace(sp.stats.maxwell.ppf(0.01), sp.stats.maxwell.ppf(0.99), 100) plt.plot(x, sp.stats.maxwell.pdf(x), 'b-', lw=5, alpha=0.6, label='maxwell pdf') plt.show() mean, var, skew, kurt = sp.stats.maxwell.stats(moments='mvsk') print(f'The true mean of the population is {mean} and its standard deviation is {var**(1/2)}')  The true mean of the population is 1.5957691216057308 and its standard deviation is 0.6734396116428514  # to create our sampling distribution S we take M samples of N instances each # we calculate the mean of each sample and add it to a list, which we can make a histogram with m_samples = 5000 n_instances = 250 sample_means = [] for m in range(m_samples): sample = sp.stats.maxwell.rvs(size=n_instances) # draw a sample from the population sample_means.append(sample.mean()) # add it to our sampling distribution sample_dist = pd.Series(sample_means) # our sample distribution as a pandas series  sample_dist.hist(bins=30) # visualizing our sample distribution of the sample mean  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1341e8fd0\u0026gt;  print(f\u0026quot;The mean of our sample distribution is {sample_dist.mean()}\u0026quot;) print(f\u0026quot;Its standard deviation (The Standard Error) is {sample_dist.std()}\u0026quot;)  The mean of our sample distribution is 1.5960355829136528 Its standard deviation (The Standard Error) is 0.04222284486122923  # calculating the same SE from the formula above se = np.sqrt(var)/np.sqrt(n_instances) print(f\u0026quot;The standard error calculated with the formula is: {se}\u0026quot;)  The standard error calculated with the formula is: 0.042592060787413163 ","date":"2020-07-12","permalink":"https://billwarker.com/posts/standard-error-of-the-mean/","tags":["stats"],"title":"The Standard Error of the Mean"},{"content":"Permutations of a Set Permutations of a set are particular orderings of its elements. To calculate the number of permutations (i.e. the possible orderings) of a subset $k$ distinct elements from a set of size $n$, the formula is:\n$$N(n, k) = \\frac{n!}{(n-k)!}$$\n$n!$ represents all orderings for every element in the set. $(n-k)!$ represents the orderings of the elements we\u0026rsquo;re not including in the subset of $k$ elements - dividing by this number allows us to factor them out of $n!$ in the numerator and give us the number of permutations.\nimport itertools import math S = ['A', 'B', 'C', 'D'] permutations = itertools.permutations(S, 2) # P(4,2) for p in permutations: print(p)  ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'A') ('B', 'C') ('B', 'D') ('C', 'A') ('C', 'B') ('C', 'D') ('D', 'A') ('D', 'B') ('D', 'C')  def permutations(n, k): return int(math.factorial(n)/math.factorial(n-k)) permutations(4, 2)  12  Combinations of a Set Combinations don\u0026rsquo;t care about order - they are just the different subsets of elements in the set. To calculate the number of combinations (i.e. subsets) of $k$ distinct elements from a set of size $n$, the formula is:\n$$C(n, k) = \\frac{n!}{k!(n-k)!}$$\nAn easy way to understand combinations is in relation to permutations - basically, we are calculating the number of permutations of a subset created by $P(n, k)$ and then just dividing that number by the possible ways to order its elements (since combinations don\u0026rsquo;t care about that).\n$$C(n, k) = \\frac{P(n, k)}{k!} = \\frac{n!}{(n-k)!}*\\frac{1}{k!}$$\nS = ['A', 'B', 'C', 'D'] combinations = itertools.combinations(S, 2) # P(4,2) for c in combinations: print(c)  ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'C') ('B', 'D') ('C', 'D')  def combinations(n, k): return int(permutations(n, k)/math.factorial(k)) combinations(4, 2)  6 ","date":"2020-07-07","permalink":"https://billwarker.com/posts/permutations-and-combinations/","tags":["stats"],"title":"Permutations and Combinations"}]