<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Data Science Notes</title>
    <link>https://billwarker.com/posts/</link>
    <description>Recent content in Posts on Data Science Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 20 Dec 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://billwarker.com/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ANOVA</title>
      <link>https://billwarker.com/posts/anova/</link>
      <pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/anova/</guid>
      <description>&lt;h3 id=&#34;use&#34;&gt;Use&lt;/h3&gt;
&lt;p&gt;ANOVA (Analysis of Variance) is a significance test that tests whether the population means from $n$ different groups are the same using the F distribution (therefore only works on numerical response data). It can test between two or more populations, therefore generalizing the t-test beyond just two groups. Examples of when you might want to do an ANOVA test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Testing whether employee stress levels are the same or different before, during, and after layoffs.&lt;/li&gt;
&lt;li&gt;Students from different colleges take the same exam. You want to see if one college outperforms the other.&lt;/li&gt;
&lt;li&gt;A factory is testing three different methods for producing breadsticks and wants to know which yields a superior crunchiness.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Article Summary - Leading With Decision-Driven Data Analytics</title>
      <link>https://billwarker.com/posts/article-summary-leading-with-decision-driven-data-analytics/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/article-summary-leading-with-decision-driven-data-analytics/</guid>
      <description>&lt;p&gt;Article Link: &lt;a href=&#34;https://sloanreview.mit.edu/article/leading-with-decision-driven-data-analytics&#34;&gt;https://sloanreview.mit.edu/article/leading-with-decision-driven-data-analytics&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Companies have more data than ever, but many fail to produce actionable insights or good results&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Being &amp;ldquo;data-driven&amp;rdquo; doesn&amp;rsquo;t stop analysts and leadership from making poor decisions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Failing to ask and answer the right/meaningful questions can be a trap to fall into, data can be used to reinforce existing beliefs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decision-driven data analytics: instead of finding a purpose for existing data, find data for existing purpose&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Calculating Sample Size to Ensure High Power</title>
      <link>https://billwarker.com/posts/calculating-sample-size-to-ensure-high-power/</link>
      <pubDate>Sun, 29 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/calculating-sample-size-to-ensure-high-power/</guid>
      <description>&lt;p&gt;Part 3 of 3 on a series of notes covering margin of error, power, and sample size calculations.&lt;/p&gt;
&lt;p&gt;Notes, with questions and examples, taken from the following reading: &lt;a href=&#34;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&#34;&gt;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding Statistical Power</title>
      <link>https://billwarker.com/posts/understanding-statistical-power/</link>
      <pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/understanding-statistical-power/</guid>
      <description>&lt;p&gt;Part 2 of 3 on a series of notes covering margin of error, power, and sample size calculations.&lt;/p&gt;
&lt;p&gt;Notes, with questions and examples, taken from the following reading: &lt;a href=&#34;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&#34;&gt;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding Margin of Error and Sample Size</title>
      <link>https://billwarker.com/posts/understanding-margin-of-error-and-sample-size/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/understanding-margin-of-error-and-sample-size/</guid>
      <description>&lt;p&gt;Part 1 of 3 on a series of notes covering margin of error, power, and sample size calculations.&lt;/p&gt;
&lt;p&gt;Notes, with questions and examples, taken from the following reading: &lt;a href=&#34;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&#34;&gt;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>What is a Hash?</title>
      <link>https://billwarker.com/posts/what-is-a-hash/</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/what-is-a-hash/</guid>
      <description>&lt;p&gt;Well, for starters, a hash is an important part of a very useful and fast data structure called a hash table. Let&amp;rsquo;s reframe the question and ask what a hash table is instead.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Hands on Machine Learning - Chapter 3 Notes</title>
      <link>https://billwarker.com/posts/handson-ml-ch3/</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/handson-ml-ch3/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hands on Machine Learning - Chapter 2 Notes</title>
      <link>https://billwarker.com/posts/handson-ml-ch2/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/handson-ml-ch2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hands on Machine Learning - Chapter 1 Notes</title>
      <link>https://billwarker.com/posts/handson-ml-ch1/</link>
      <pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/handson-ml-ch1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Standard Error of the Mean</title>
      <link>https://billwarker.com/posts/standard-error-of-the-mean/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/standard-error-of-the-mean/</guid>
      <description>&lt;h2 id=&#34;the-standard-error-of-the-mean&#34;&gt;The Standard Error of the Mean&lt;/h2&gt;
&lt;p&gt;The Standard Error of the Mean ($SE$) is the &lt;strong&gt;standard deviation&lt;/strong&gt; of the &lt;strong&gt;sample distribution&lt;/strong&gt; of the &lt;strong&gt;sample mean&lt;/strong&gt;. To understand what this means, let&amp;rsquo;s break that sentence down in reverse order (i.e. chronologically):&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample Mean&lt;/strong&gt;: we have some probability density function $P$ for a population. We take a sample of $N$ instances from it and calculate our statistic of interest - in this case the mean, $\bar{x}$. We have to take samples because it is hard/impossible to look at every instance in an entire population to calculate the true mean $\mu$ hidden to us in nature (i.e. say we were interested in the average weight of all monkeys on the planet).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample Distribution&lt;/strong&gt;: we take many samples (the number of which denoted by $M$) from the population&amp;rsquo;s probability density function $P$ and calculate the sample mean $\bar{x}$ for each one. All of these sample means can be lumped together into a distribution $S$, which approximates a normal distribution the higher $M$ is due to the &lt;strong&gt;Central Limit Theorum&lt;/strong&gt;. We want to create a sampling distribution because it allows us to reason about the likelihood of different values the sample mean can be. By doing so, we can look at the mean of this distribution and conclude that its the most likely value for $\mu$ given the data we&amp;rsquo;ve used.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Standard Deviation&lt;/strong&gt;: The standard deviation is a metric that describes the dispersion of a dataset/distribution in relation to its mean. It speaks to how close/far the density of the distribution is spread in relation to its mean. The standard deviation of a sample distribution - our standard error $SE$ - is an indication of how representative the distribution is of our true mean $\mu$. The smaller it is, the more representative it can be said to be; the larger it is, the harder it is to trust.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TLDR: The Standard Error matters because it allows us to better understand how representative our sampling distribution is (i.e. our model of the true mean).&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Permutations and Combinations</title>
      <link>https://billwarker.com/posts/permutations-and-combinations/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/permutations-and-combinations/</guid>
      <description>&lt;h1 id=&#34;permutations-of-a-set&#34;&gt;Permutations of a Set&lt;/h1&gt;
&lt;p&gt;Permutations of a set are particular orderings of its elements. To calculate the number of permutations (i.e. the possible orderings) of a subset $k$ distinct elements from a set of size $n$, the formula is:&lt;/p&gt;
&lt;p&gt;$$N(n, k) = \frac{n!}{(n-k)!}$$&lt;/p&gt;
&lt;p&gt;$n!$ represents all orderings for every element in the set. $(n-k)!$ represents the orderings of the elements we&amp;rsquo;re not including in the subset of $k$ elements - dividing by this number allows us to factor them out of $n!$ in the numerator and give us the number of permutations.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>