[{"content":"Notes from Causal Inference for the Brave and True\nhttps://matheusfacure.github.io/python-causality-handbook/03-Stats-Review-The-Most-Dangerous-Equation.html\n Stats Review: The Most Dangerous Equation The Standard Error of Mean is a dangerous equation to not know:\n$SE = \\frac{\\sigma}{\\sqrt{n}}$\nWith $\\sigma$ as the standard deviation and $n$ as the sample size.\nLooking at a dataset of ENEM scores (Brazillian standardized high school scores similar to SAT) from different schools over three years:\nimport warnings warnings.filterwarnings('ignore') import pandas as pd import numpy as np from scipy import stats import seaborn as sns from matplotlib import pyplot as plt from matplotlib import style style.use(\u0026quot;fivethirtyeight\u0026quot;)  df = pd.read_csv(\u0026quot;data/enem_scores.csv\u0026quot;) df.sort_values(by=\u0026quot;avg_score\u0026quot;, ascending=False).head(10)   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  year school_id number_of_students avg_score     16670 2007 33062633 68 82.97   16796 2007 33065403 172 82.04   16668 2005 33062633 59 81.89   16794 2005 33065403 177 81.66   10043 2007 29342880 43 80.32   18121 2007 33152314 14 79.82   16781 2007 33065250 80 79.67   3026 2007 22025740 144 79.52   14636 2007 31311723 222 79.41   17318 2007 33087679 210 79.38     Initial observation is that top performing schools seem to have a low number of students\n Taking a look at the top 1%:  plot_data = (df .assign(top_school = df[\u0026quot;avg_score\u0026quot;] \u0026gt;= np.quantile(df[\u0026quot;avg_score\u0026quot;], .99)) [[\u0026quot;top_school\u0026quot;, \u0026quot;number_of_students\u0026quot;]] .query(f\u0026quot;number_of_students\u0026lt;{np.quantile(df['number_of_students'], .98)}\u0026quot;)) plt.figure(figsize=(6,6)) sns.boxplot(x=\u0026quot;top_school\u0026quot;, y=\u0026quot;number_of_students\u0026quot;, data=plot_data) plt.title(\u0026quot;Number of Students in 1% top schools (right)\u0026quot;)  Text(0.5, 1.0, 'Number of Students in 1% top schools (right)')  The data does suggest that top schools do have less students, which makes sense intuitively.\nThe trap appears when we just take this at face value and make decisions on it:\n What if we looked at bottom 1% of schools too?  q_99 = np.quantile(df['avg_score'], .99) q_01 = np.quantile(df['avg_score'], .01)  plot_data = (df .sample(10000) .assign(group = lambda d: np.select([d['avg_score'] \u0026gt; q_99, d['avg_score'] \u0026lt; q_01], ['Top', 'Bottom'], 'Middle')))  plt.figure(figsize=(10,5)) sns.scatterplot(y=\u0026quot;avg_score\u0026quot;, x=\u0026quot;number_of_students\u0026quot;, hue=\u0026quot;group\u0026quot;, data=plot_data) plt.title(\u0026quot;ENEM Score by Number of Students in the School\u0026quot;)  Text(0.5, 1.0, 'ENEM Score by Number of Students in the School')  plot_data.groupby('group')['number_of_students'].median()  group Bottom 95.5 Middle 104.0 Top 77.0 Name: number_of_students, dtype: float64   Bottom 1% of schools also have less students as well  As the number of units in a sample grows, the variance of the sample decreases and averages get more precise.\n Smaller samples can have a lot of variance in their expected outcome variables due to chance  Speaks to a fundamental fact about the reality of information: it is always imprecise.\n The question becomes: can we quantify how imprecise it is? Probabilty is an acceptance of the lack of certainty in our knowledge and the development of methods for dealing with our ignorance  The Standard Error of our Estimates We can test and see if the $ATE$ from the last chapter (GPA scores for students in traditional classrooms vs. online) was significant. First step is calculating the standard error $SE$:\ndata = pd.read_csv(\u0026quot;data/online_classroom.csv\u0026quot;) online = data.query(\u0026quot;format_ol == 1\u0026quot;)[\u0026quot;falsexam\u0026quot;] face_to_face = data.query(\u0026quot;format_ol == 0 \u0026amp; format_blended == 0\u0026quot;)[\u0026quot;falsexam\u0026quot;]  def se(y: pd.Series): return y.std() / np.sqrt(len(y))  print(f\u0026quot;SE for Online: {se(online)}\u0026quot;) print(f\u0026quot;SE for Face to Face: {se(face_to_face)}\u0026quot;)  SE for Online: 1.5371593973041635 SE for Face to Face: 0.8723511456319106  Confidence Intervals The Standard Error $SE$ of an estimate is a measure of confidence.\nHas a different interpretation depending on the different views of statistics (Frequentist and Bayesian).\nFrequentist view:\n The data is a manifestation of a true data generating process If we could run multiple experiments and collect multiple datasets, all would resemble the underlying process  For the sake of an example lets say that the true distribution of a student\u0026rsquo;s test score is normal distribution $N(\\mu, \\sigma^{2})$ with $\\mu = 74$ and $\\sigma = 2$.\nRun 10,000 experiments, collecting 500 units per sample:\ntrue_std = 2 true_mean = 74 n = 500 def run_experiment(): return np.random.normal(true_mean, true_std, 500) np.random.seed(42) plt.figure(figsize=(8,5)) freq, bins, img = plt.hist([run_experiment().mean() for _ in range(10000)], bins=40, label=\u0026quot;Experiment's Mean\u0026quot;) plt.vlines(true_mean, ymin=0, ymax=freq.max(), linestyles=\u0026quot;dashed\u0026quot;, label=\u0026quot;True Mean\u0026quot;) plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7fbb7ab33e90\u0026gt;   This is the distribution of sample means; the sample distribution The standard error is the standard deviation of this distribution With the standard error we can create an interval that will contain the true mean 95% of the time (95% CI) We take the desired $Z$ score for the normal distribution, in this case Z = 1.96 for 95% CDF and $\\pm$ that multipled by $SE$ to get the confidence interval around a point estimate $SE$ serves as our estimate for the means' distribution of our experiments  from scipy.stats import norm  z = norm.ppf(0.975)  z  1.959963984540054  np.random.seed(321) exp_data = run_experiment() exp_se = exp_data.std() / np.sqrt(len(exp_data)) exp_mu = exp_data.mean() ci = (exp_mu - z * exp_se, exp_mu + z * exp_se)  print(ci) # 95% of the time the data's true mean will fall within this interval  (73.83064660084463, 74.16994997421483)  We can construct a confidence interval for $ATE$ on GPA scores in our classroom example:\ndef ci(y: pd.Series, confidence = 0.975): return (y.mean() - norm.ppf(confidence) * se(y), y.mean() + norm.ppf(confidence) * se(y))  print(f\u0026quot;95% CI for Online: {ci(online)}\u0026quot;) print(f\u0026quot;95% CI for Face to Face: {ci(face_to_face)}\u0026quot;)  95% CI for Online: (70.62248602789292, 76.64804014231983) 95% CI for Face to Face: (76.8377077560225, 80.2572614106441)   We can see that there\u0026rsquo;s no overlap between the two groups' CIs: this is evidence that the results were not by chance Very likely that there is a significant causal decrease in academic performance once you switch from face to face to online classes.  There is a nuance to confidence intervals:\n The population mean is constant; you shouldn\u0026rsquo;t really say that the confidence interval contains the population mean with 95% chance. Since it is constant, it is either in the interval or not. The 95% refers to the frequency that such confidence intervals, computed in many other studies, contain the true mean. 95% confidence in the algorithm used to compute the 95% CI, not on a particular interval itself Bayesian statistics and the use of probability intervals are able to say that an interval contains the distribution mean 95% of the time.  Hypothesis Testing Is the difference in two means significant, or statistically different from zero/another value?\n Recall that the sum or difference of normal distributions is also a normal distribution The resulting mean will be the sum or difference of the two distributions' means, while the variance will always be the sum of variance  $$N(\\mu_1, \\sigma_{1}^{2}) - N(\\mu_2, \\sigma_{2}^{2}) = N(\\mu_1 - \\mu_2, \\sigma_{1}^{2} + \\sigma_{2}^{2})$$ $$N(\\mu_1, \\sigma_{1}^{2}) + N(\\mu_2, \\sigma_{2}^{2}) = N(\\mu_1 + \\mu_2, \\sigma_{1}^{2} + \\sigma_{2}^{2})$$\nnp.random.seed(123) n1 = np.random.normal(4, 3, 30000) n2 = np.random.normal(1, 4, 30000) n_diff = n1 - n2 sns.distplot(n1, hist=False, label=\u0026quot;N(4,3)\u0026quot;) sns.distplot(n2, hist=False, label=\u0026quot;N(1,4)\u0026quot;) sns.distplot(n_diff, hist=False, label=\u0026quot;N(4,3) - N(1,4) = N(3,5)\u0026quot;)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbb7ec4a550\u0026gt;   If we take the distribution of the means of our two groups and subtract one from the other, we get a third distribution equaling the difference in the means and the standard deviation of the distribution will be the square root of the sum of the standard deviations:  $$ \\mu_{diff} = \\mu_1 - \\mu_2 $$ $$ SE_{diff} = \\sqrt{SE_1 + SE_2} = \\sqrt{\\frac{\\sigma_{1}^{2}}{n_1} + \\frac{\\sigma_{2}^{2}}{n_2}} $$\nConstructing the distribution of the difference with the classroom example:\ndiff_mu = online.mean() - face_to_face.mean() diff_se = np.sqrt(online.var()/len(online) + face_to_face.var()/len(face_to_face)) ci = (diff_mu - 1.96 * diff_se, diff_mu + 1.96 * diff_se)  ci  (-8.376410208363357, -1.4480327880904964)  Plot the confidence interval with the distribution of differences between online and face-to-face groups:\ndiff_dist = stats.norm(diff_mu, diff_se) x = np.linspace(diff_mu - 4 * diff_se, diff_mu + 4 * diff_se, 100) y = diff_dist.pdf(x) plt.plot(x,y) plt.vlines(ci[0], ymin=0, ymax = diff_dist.pdf(ci[0])) plt.vlines(ci[1], ymin=0, ymax = diff_dist.pdf(ci[1]), label = \u0026quot;95% CI\u0026quot;) plt.legend() plt.show()   We can say that we\u0026rsquo;re 95% that the true difference between groups falls within this interval of (-8.37, -1.44)  We can create a z statistic by dividing the difference in means by the standard error of the differences:\n$$ z = \\frac{\\mu_{diff} - H_0}{SE_{diff}} = \\frac{(u_1 - u_2) - H_0}{\\sqrt{\\frac{\\sigma_{1}^{2}}{n_1} + \\frac{\\sigma_{2}^{2}}{n_2}}} $$\n Where $H_0$ is the value which we want to test our difference against The z statistic is a measure of how extreme the observed difference is With the null hypothesis we ask: \u0026ldquo;how likely is it that we would observe this difference if the true/population difference was actually zero/[insert whatever your null hypothesis is]?\u0026rdquo; The z statistic is computed through the data to be standardized to the standard normal distribution; i.e. if the difference were indeed zero we would see z be within two standard deviations of the mean 95% of the time.  z = diff_mu / diff_se print(z)  -2.7792810791031064  x = np.linspace(-4, 4, 100) y = stats.norm.pdf(x, 0, 1) plt.plot(x, y, label=\u0026quot;Standard Normal\u0026quot;) plt.vlines(z, ymin=0, ymax= .05, label=\u0026quot;Z statistic\u0026quot;, color=\u0026quot;C1\u0026quot;) plt.legend() plt.show()   We can see that the Z statistic is a pretty extreme value (more than 2 standard devations away from the mean) Interesting point about hypothesis tests: they\u0026rsquo;re less conservative than checking if the 95% CIs from the two groups overlap; i.e. they can overlap but still be a result that\u0026rsquo;s statistically significant If we pretend that the face-to-face group has an average score of 74 with a standard error of 7 and the online group has an average score of 71 with a standard error of 1:  ctrl_mu, ctrl_se = (71, 1) test_mu, test_se = (74, 7) diff_mu = test_mu - ctrl_mu diff_se = np.sqrt(ctrl_se + test_se) groups = zip(['Control', 'Test', 'Diff'], [[ctrl_mu, ctrl_se], [test_mu, test_se], [diff_mu, diff_se]]) for name, stats in groups: print(f\u0026quot;{name} 95% CI:\u0026quot;, (stats[0] - 1.96 * stats[1], stats[0] + 1.96 * stats[1],))  Control 95% CI: (69.04, 72.96) Test 95% CI: (60.28, 87.72) Diff 95% CI: (-2.5437171645025325, 8.543717164502532)   The CI for the difference between groups contains 0, so maybe the example provided by the author above isn\u0026rsquo;t a great one\u0026hellip; moving on\u0026hellip;  P-Values   From Wikipedia: “the p-value is the probability of obtaining test results at least as extreme as the results actually observed during the test, assuming that the null hypothesis is correct”\n  i.e. the probablity of seeing the results given that the null hypothesis is true\n  It is not equal to the probability of the null hypothesis being true!\n  Not $P(H_{0} \\vert data)$, but rather $P(data \\vert H_{0})$\n  To obtain, just compute the area under the standard normal distribution before or after the z statistic\n  Simply plug the z statistic into the CDF of the standard normal distribution:\n  print(f'P-value: {norm.cdf(z)}')  P-value: 0.0027239680835564706   Means that there\u0026rsquo;s a 0.2% chance of observing this z statistic given the null hypothesis is true; this falls within the accepted significance level to reject the null hypothesis The p-value avoids us having to specify a confidence level We can know exactly at which confidence our test will pass or fail though, given the p-value; with a P-value of 0.0027, we will have significance up to the 0.2% level A 95% CI and a 99% CI for the difference won\u0026rsquo;t contain zero, but a 99.9% CI will:  diff_mu = online.mean() - face_to_face.mean() diff_se = np.sqrt(online.var()/len(online) + face_to_face.var()/len(face_to_face)) print(\u0026quot;95% CI:\u0026quot;, (diff_mu - norm.ppf(.975)*diff_se, diff_mu + norm.ppf(.975)*diff_se)) print(\u0026quot;99% CI:\u0026quot;, (diff_mu - norm.ppf(.995)*diff_se, diff_mu + norm.ppf(.995)*diff_se)) print(\u0026quot;99.9% CI:\u0026quot;, (diff_mu - norm.ppf(.9995)*diff_se, diff_mu + norm.ppf(.9995)*diff_se))  95% CI: (-8.37634655308288, -1.4480964433709733) 99% CI: (-9.464853535264012, -0.3595894611898425) 99.9% CI: (-10.72804065824553, 0.9035976617916743)  Key Ideas  The standard error enables us to put degrees of certainty around our estimates by enabling us to calculate confidence intervals around our point estimates as well as the statistical significance of a result given hypothesis testing Wrapping everything up, we can create an A/B testing function to automate all the work done above:  def AB_test(test: pd.Series, control: pd.Series, confidence=0.95, h0=0): mu1, mu2 = test.mean(), control.mean() se1, se2 = test.std()/np.sqrt(len(test)), control.std()/np.sqrt(len(control)) diff = mu1 - mu2 se_diff = np.sqrt(test.var()/len(test) + control.var()/len(control)) z_stats = (diff - h0)/se_diff p_value = norm.cdf(z_stats) def critical(se): return -se*norm.ppf((1 - confidence)/2) print(f\u0026quot;Test {confidence*100}% CI: {mu1} +- {critical(se1)}\u0026quot;) print(f\u0026quot;Control {confidence*100}% CI: {mu2} +- {critical(se2)}\u0026quot;) print(f\u0026quot;Test - Control {confidence*100}% CI: {diff} +- {critical(se_diff)}\u0026quot;) print(f\u0026quot;Z statistic: {z_stats}\u0026quot;) print(f\u0026quot;P-Value: {p_value}\u0026quot;)  AB_test(online, face_to_face)  Test 95.0% CI: 73.63526308510637 +- 3.0127770572134565 Control 95.0% CI: 78.5474845833333 +- 1.7097768273108005 Test - Control 95.0% CI: -4.912221498226927 +- 3.4641250548559537 Z statistic: -2.7792810791031064 P-Value: 0.0027239680835564706  ","date":"2021-06-05","permalink":"https://billwarker.com/posts/stats-review-the-most-dangerous-equation/","tags":["stats","causal-inference","experimentation"],"title":"Stats Review: The Most Dangerous Equation"},{"content":"Notes from Causal Inference for the Brave and True\nhttps://matheusfacure.github.io/python-causality-handbook/02-Randomised-Experiments.html\n Randomized Experiments The Golden Standard Association becomes causation when there is no bias between treatment \u0026amp; control groups.\n There\u0026rsquo;s no difference between them except for the treatment itself The outcome of the untreated group is the same as the counterfactual of the treated group $E[Y_{0} \\vert T = 0] = E[Y_{0} \\vert T = 1]$  Randomized Experiments, otherwise known as Randomized Controlled Trials (RCTs), can make bias vanish:\n Randomly assigning individuals in a population to either a treatment or control group Doesn\u0026rsquo;t need to be 50/50 split, as long as the sample size is large enough to be representative Randomization annihilates bias by making the potential outcomes independent of the treatment  $(Y_{0}, Y_{1}) \\perp!!!\\perp T$\n Where $\\perp!!!\\perp$ is the symbol for conditional independence This means that the potential outcomes are independent of the treatment Emphasis on potential outcomes $Y_{0}$ or $Y_{1}$ In randomized trials we don\u0026rsquo;t want the outcome $Y$ to be independent of the treatment, because we think the treatment causes the outcome But saying the potential outcomes $Y_{0}$ or $Y_{1}$ are independent of the treatment is to say that in expectation, they are the same between the control and treatment groups (i.e. the groups are comparable)  $(Y_{0}, Y_{1}) \\perp T$\n Where $\\perp$ essentially means dependence This means that the treatment is the only thing generating a difference between the outcome in the treated and control groups Which, if this is the case, implies $E[Y_{0} \\vert T = 0] = E[Y_{0} \\vert T = 1] = E[Y_{0}]$ Which gives us $E[Y \\vert T = 1] - E[Y \\vert T = 0] = E[Y_{1} - Y_{0}] = ATE$ Meaning the randomization allows us to just use the simple difference in means between treatment and control as the treatment effect  In a School Far, Far Away  Let\u0026rsquo;s say we wanted to know if remote learning has a positive or negative impact on student performance If we were to compare students in schools that give mostly online classes to those that just use traditional classrooms, we would run the risk of mistaking association for causation (bias exists) $T = 1$ for online schools and $T = 0$ for traditional schools  Potential biases:\n Online schools attract more studious, disciplined students $\\rightarrow E[Y_{0} \\vert T = 1] \u0026gt; E[Y_{0} \\vert T = 0]$ (positive bias) Online schools consist of poorer students who cannot afford traditional schooling $\\rightarrow E[Y_{0} \\vert T = 1] \u0026lt; E[Y_{0} \\vert T = 0]$ (negative bias) We could still speak to correlation, but can\u0026rsquo;t make any convincing claims about causality  Randomly assigning the online and traditional classes to students solves this\n On average the treatment is the only difference between the two groups  import pandas as pd import numpy as np  data = pd.read_stata(\u0026quot;113462-V1/data-file-and-program/ReplicationData2.dta\u0026quot;)  data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  gpa cum_total_crds gender asian black hawaiian hispanic unknown white ethnic_dummy format_ol format_blended sat_math_NEW sat_verbal_NEW enroll_count format_f2f_v_ol format_f2f_v_blended format_combined_v_f2f falsexam experiment1     0 2.014 63.0 1 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 540.0 540.0 NaN 0.0 0.0 1.0 0.0 0.0   1 3.720 33.0 1 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 590.0 630.0 NaN 0.0 0.0 1.0 0.0 0.0   2 NaN 4.0 0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 650.0 570.0 NaN 0.0 0.0 1.0 0.0 0.0   3 NaN 10.0 1 NaN NaN NaN NaN NaN NaN NaN 0.0 0.0 690.0 690.0 NaN 0.0 0.0 1.0 0.0 0.0   4 NaN 0.0 0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 480.0 420.0 NaN 0.0 0.0 1.0 0.0 0.0     data_groups = data.assign(class_format = np.select([data[\u0026quot;format_ol\u0026quot;].astype(bool), data[\u0026quot;format_blended\u0026quot;].astype(bool)], [\u0026quot;online\u0026quot;, \u0026quot;blended\u0026quot;], default=\u0026quot;face_to_face\u0026quot;)).groupby([\u0026quot;class_format\u0026quot;]).mean()  data_groups   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  gpa cum_total_crds gender asian black hawaiian hispanic unknown white ethnic_dummy format_ol format_blended sat_math_NEW sat_verbal_NEW enroll_count format_f2f_v_ol format_f2f_v_blended format_combined_v_f2f falsexam experiment1   class_format                         blended 3.127560 34.509934 0.561404 0.230088 0.115044 0.017699 0.008850 0.008850 0.619469 0.416667 0.0 1.0 620.413793 579.554795 2.421053 NaN 1.0 0.0 50.018696 1.000000   face_to_face 3.100013 31.566413 0.573620 0.146154 0.084615 0.003846 0.026923 0.000000 0.738462 0.261538 0.0 0.0 625.521173 590.618893 2.525714 0.0 0.0 1.0 29.138031 0.493289   online 3.061357 36.774096 0.526012 0.220472 0.055118 0.007874 0.023622 0.023622 0.669291 0.330709 1.0 0.0 615.911950 570.251572 2.375723 1.0 1.0 0.0 40.963379 1.000000     We can use the difference in means between groups as the $ATE$:\n Looking at GPA, we can conclude that $ATE = E[Y_{1} - Y_{0}] = 3.06 - 3.10 = -0.04$ (comparing online and face_to_face groups) Online classes caused students to perform slightly worse Is this difference significant though, maybe another question  Either way the randomized experiment wiped out any bias between the groups:\n We can see that gender is fairly evenly distributed between groups white students/variable slightly overindexed in face_to_face however; these slight imbalances are due to small dataset size  The Ideal Experiment RCTs are the most reliable way to uncover causal effects - a well designed RCT is a scientist\u0026rsquo;s dream.\nSometimes however, we can\u0026rsquo;t control the assignment mechanism due to cost/ethical reasons:\n e.g. If we wanted to understand the effects of smoking during preganancy, we couldn\u0026rsquo;t just force a random portion of moms to smoke while they were pregnant e.g. A big bank couldn\u0026rsquo;t just give random lines of credit to customers to measure the impact on churn Conditional randomization can help lower the cost sometimes Nothing can be done for unethical/unfeasible experiments though Always worth it to ask what the ideal experiment would be, this can shed some light on how to uncover the causal effect without perfect conditions  The Assignment Mechanism Causal inference techniques try to identify the assignment mechanism of the treatments.\n  In RCTs the assignment mechanism is pure randomness\n  Understanding the assignment mechanism can make inference more certain\n  Assignment mechanisms can\u0026rsquo;t just be found looking at associations in the data through EDA\n  In causal questions you can usually argue both ways: X causes Y, Y causes X, or Z causes X and Y, and the X/Y correlation is just spurious\n  Understanding the assignment mechanism leads to more convincing answers and makes causal inference exciting\n  Key Ideas  RCTs make the treatment and control groups comparable; this is the equivalent to being able to see the counterfactuals for both groups When the potential outcome for the untreated $Y_{0}$ is the same for both the test and control groups, this allows us to call their difference in means for the outcome variable $Y$ as the average treatment effect $ATE$  Breaking this down from the original association equation:\n$ATE = E[Y \\vert T = 1] - E[Y \\vert T = 0] = E[Y_{1} - Y_{0} \\vert T = 1] + E[Y_{0} \\vert T = 1] - E[Y_{0} \\vert T = 0]$\nWhere the average treatment effect/difference in means/association between the groups is equal to the average treatment effect of the treated plus a bias term ($ATET + BIAS$)\nRCTs make $Y_{0}$ the same between both groups so $E[Y_{0} \\vert T = 1] - E[Y_{0} \\vert T = 0] = x - x = 0$, eliminating the bias term\nThis reduces the average treatment effect to be equal to the average treatment effect of the treated:\n$ATE = ATET \\rightarrow E[Y \\vert T = 1] - E[Y \\vert T = 0] = E[Y_{1} - Y_{0} \\vert T = 1]$\nLong story short we can take the average treatment effect/difference in means between treatment to be equivalent to the causal effect of the treatment on the treatment group. Association becomes causation.\nRCTs are great, but unfortunately they can\u0026rsquo;t always be the solution due to ethical/cost/feasibility reasons.\n","date":"2021-03-20","permalink":"https://billwarker.com/posts/randomized-experiments/","tags":["stats","causal-inference","experimentation"],"title":"Randomized Experiments/RCTs"},{"content":"Notes from Causal Inference for the Brave and True\nhttps://matheusfacure.github.io/python-causality-handbook/01-Introduction-To-Causality.html\n Introduction to Causality Data Science is kind of like a cup of beer, with a little bit of foam on the top:\n The beer is statistical foundations, scientific curiousity, passion for difficult problems The foam is the hype and unrealistic expectations that will disappear eventually  Focus on what makes your work valuable.\nAnswering a Different Kind of Question  ML doesn\u0026rsquo;t bring intelligence, it brings predictions Must frame problems as prediction ones, and it\u0026rsquo;s not so good at explaining causation  Causal questions are everywhere:\n Does X cause an increase in sales? Does higher education lead to higher earnings? Does immigration cause unemployment to go up?  ML and correlation-type predictions don\u0026rsquo;t work for these questions.\nWe always hear that correlation isn\u0026rsquo;t causation:\n Explaining why takes some understanding This book explains how to figure out when correlation is causation  When Association is Causation We can intuitively understand why correlation doesn\u0026rsquo;t necessarily mean causation:\n If someone says that schools that give their students tablets to work with perform better, we can quickly point out that these schools are probably better funded, richer families, etc. We can\u0026rsquo;t say that tablets make students perform better, but they\u0026rsquo;re associated/correlated with better performance (due to underlying factors)  import pandas as pd import numpy as np from scipy.special import expit import seaborn as sns import matplotlib.pyplot as plt from matplotlib import style  style.use(\u0026quot;fivethirtyeight\u0026quot;) np.random.seed(123) n = 100 tuition = np.random.normal(1000, 300, n).round() tablet = np.random.binomial(1, expit((tuition - tuition.mean()) / tuition.std())).astype(bool) enem_score = np.random.normal(200 - 50 * tablet + 0.7 * tuition, 200) enem_score = (enem_score - enem_score.min()) / enem_score.max() enem_score *= 1000 df = pd.DataFrame(dict(enem_score=enem_score, Tuition=tuition, Tablet=tablet))  df   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  enem_score Tuition Tablet     0 227.622953 674.0 False   1 219.079925 1299.0 True   2 400.889622 1085.0 False   3 122.761509 548.0 False   4 315.064276 826.0 False   ... ... ... ...   95 451.019929 1309.0 True   96 113.288467 675.0 True   97 116.042782 591.0 False   98 266.238616 1114.0 True   99 297.431514 886.0 True    100 rows × 3 columns\n plt.figure(figsize=(6,8)) sns.boxplot(y=\u0026quot;enem_score\u0026quot;, x=\u0026quot;Tablet\u0026quot;, data=df).set_title(\u0026quot;ENEM score by Tablet in Class\u0026quot;) plt.show()  $T_i$ is the treatment intake for unit $i$:\n 1 if unit $i$ received the treatment 0 otherwise  $Y_i$ is the observed outcome variable of interest for unit $i$\nThe fundamental problem of causal inference is we can never observe the same unit with/without treatment:\n Like two diverging roads in life\u0026hellip; you always wonder what could have been \u0026lt;/3 Potential outcomes are talked about a lot, denoting what would/could have happened if some treatment was taken Sometimes call the outcome that happened \u0026ldquo;factual\u0026rdquo;, and the one that didn\u0026rsquo;t as \u0026ldquo;counterfactual\u0026rdquo;  $Y_{0i}$ is the potential outcome for unit $i$ without the treatment\n$Y_{1i}$ is the potential outcome for the same unit $i$ with the treatment\nIn our example:\n $Y_{1i}$ is the academic performance of student $i$ if they are in a classroom with tablets $Y_{0i}$ otherwise If the student gets the tablet, we can observe $Y_{1i}$, if not we can observe $Y_{0i}$ Each counterfactual is still defined, we just can\u0026rsquo;t see it - a potential outcome  With potential outcomes we can define the treatment effect:\n$ Y_{1i} - Y_{0i} $\n Of course we can never know the treatment effect directly because we can only observe one of the potential outcomes  Focus on easier things to estimate/measure:\nAverage treatment effect:\n$ ATE = E[Y_1 - Y_{0}] $\n Where $E[\u0026hellip;]$ is the expected value  Average treatment effect on the treated:\n$ ATET = E[Y_1 - Y_{0} \\vert T = 1] $\nPretending we could see both potential outcomes (a gift from the causal inference gods):\n Collected data on 4 schools We know if they gave tablets to students and their score on a test $T = 1$ is treatment (getting the tablets) $Y$ is test score  data = pd.DataFrame(dict(i = [1,2,3,4], y0 = [500,600,700,800], y1 = [450,600,600,750], t = [0,0,1,1], y = [500,600,600,750], te = [-50,0,-200,50],)) # TE is treatment effect  data   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  i y0 y1 t y te     0 1 500 450 0 500 -50   1 2 600 600 0 600 0   2 3 700 600 1 600 -200   3 4 800 750 1 750 50     $ATE$ would be the mean of $TE$:\ndata.te.mean()  -50.0   Tablets reduced the academic performance of students, on average, by 50 pts  $ATET$:\ndata[data.t == 1].te.mean()  -75.0   For schools that were treated, tablets reduced academic performance by 75 pts on average  In reality (where we can\u0026rsquo;t observe counterfactuals) the data would look like:\nreality_data = pd.DataFrame(dict(i = [1,2,3,4], y0 = [500,600,np.nan,np.nan], y1 = [np.nan,np.nan,600,750], t = [0,0,1,1], y = [500,600,600,750], te = [np.nan,np.nan,np.nan,np.nan],))  reality_data   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  i y0 y1 t y te     0 1 500.0 NaN 0 500 NaN   1 2 600.0 NaN 0 600 NaN   2 3 NaN 600.0 1 600 NaN   3 4 NaN 750.0 1 750 NaN     You can\u0026rsquo;t just take the mean of the treated and compare it with the mean of the untreated to try and answer the question of causality:\n That\u0026rsquo;s committing a grave sin: mistaking association for causation  Bias The main enemy of causal inference.\nSchools with tablets are likely richer than those without; i.e. the treated schools (with tablets) are not the same as untreated schools (without tablets, likely poorer). The $Y_0$ of the treated is different from the $Y_0$ of the untreated.\nLeverage your understanding of how the world works:\n The $Y_0$ of the treated schools is likely larger than untreated schools for other reasons Schools that can afford tablets can also afford other factors that contribute to better the scores  Association is measured by $E[Y \\vert T = 1] - E[Y \\vert T = 0]$\n e.g. the average test score for schools with tablets minus the average test score of those without them  Causation is measured by $E[Y_{1} - Y_{0}]$\nTo see how they relate:\nFirst, take the association measurement and replace observed outcomes with potential outcomes\n$E[Y \\vert T = 1] - E[Y \\vert T = 0] = E[Y_{1} \\vert T = 1] - E[Y_{0} \\vert T = 0]$\nNow lets add and subtract $E[Y_{0} \\vert T = 1]$, the counterfactual outcome. What would have been the outcome of the treated group, had they not received treatment.\n$E[Y \\vert T = 1] - E[Y \\vert T = 0] = E[Y_{1} \\vert T = 1] - E[Y_{0} \\vert T = 0] + E[Y_{0} \\vert T = 1] - E[Y_{0} \\vert T = 1]$\nThrough reordering the terms and merging some expectations we get:\n$E[Y \\vert T = 1] - E[Y \\vert T = 0] = E[Y_{1} - Y_{0} \\vert T = 1] + E[Y_{0} \\vert T = 1] - E[Y_{0} \\vert T = 0]$\nWhere\n $E[Y_{1} - Y_{0} \\vert T = 1]$ is $ATET$ $E[Y_{0} \\vert T = 1] - E[Y_{0} \\vert T = 0]$ is our $BIAS$ term  Association is equal to the treatment effect on the treated plus a bias term:\n The bias is given by how the treated and control group differ before the treatment; in the case neither of them has received the treatment In this example, we think that $E[Y_0 \\vert T = 0] \u0026lt; E[Y_0 \\vert T = 1]$; that schools who can afford to give tablets are better than those that can\u0026rsquo;t, regardless of the tablets treatment  Bias arises from many things we can\u0026rsquo;t control changing together with the experiment (confounding variables).\n e.g. treated and untreated schools don\u0026rsquo;t just differ on tablets, but on tuition cost, location, teachers, etc. To claim that tablets improve performance, we would need schools with and without them to be, on average, similar to each other  plt.figure(figsize=(10,6)) sns.scatterplot(x=\u0026quot;Tuition\u0026quot;, y=\u0026quot;enem_score\u0026quot;, hue=\u0026quot;Tablet\u0026quot;, data=df, s=70).set_title(\u0026quot;ENEM score by Tuition Cost\u0026quot;)  Text(0.5, 1.0, 'ENEM score by Tuition Cost')  We know the problem, and here\u0026rsquo;s the solution:\nIf $E[Y_{0} \\vert T = 0] = E[Y_{0} \\vert T = 1]$, then association is causation!\n This is saying that the treatment and control group are comparable before the treatment If we could observe $Y_{0}$ for the treated group, then its outcome would be the same as the untreated  This makes the bias term vanish in association, leaving only $ATET$:\n$E[Y \\vert T = 1] - E[Y \\vert T = 0] = E[Y_{1} - Y_{0} \\vert T = 1] + 0$\nIf $E[Y_{0} \\vert T = 0] = E[Y_{0} \\vert T = 1]$, the causal impact on the treated is the same as in the untreated (because they are similar).\n$E[Y_{1} - Y_{0} \\vert T = 1] = E[Y_{1} \\vert T = 1] - E[Y_{0} \\vert T = 1]$\n$\\hspace{3.35cm} = E[Y_{1} \\vert T = 1] - E[Y_{0} \\vert T = 0]$\n$\\hspace{3.35cm} = E[Y \\vert T = 1] - E[Y \\vert T = 0]$\nHence the difference in means becomes the causal effect:\n$E[Y \\vert T = 1] - E[Y \\vert T = 0] = ATE = ATET$\nCausal inference is all about finding clever ways to remove bias through experimentation, making the treatment and control groups comparable so that all the difference we can see between them is only the average treatment effect.\nKey Ideas Association is not causation, but it can be (if there are no other differences between the groups being tested, AKA bias).\nPotential outcome notation and the idea of counterfactuals - two potential realities, but only one of them can be measured (the funadamental problem of causal inference).\n \u0026ldquo;What happens in a man\u0026rsquo;s life is already written. A man must move through life as his destiny wills.\u0026quot;\n\u0026ldquo;Yes, but each man is free to live as he chooses. Though they seem opposite, both are true\u0026rdquo;.\n","date":"2021-03-15","permalink":"https://billwarker.com/posts/introduction-to-causality/","tags":["stats","causal-inference"],"title":"Introduction to Causality"},{"content":"Use ANOVA (Analysis of Variance) is a significance test that tests whether the population means from $n$ different groups are the same using the F distribution (therefore only works on numerical response data). It can test between two or more populations, therefore generalizing the t-test beyond just two groups. Examples of when you might want to do an ANOVA test:\n Testing whether employee stress levels are the same or different before, during, and after layoffs. Students from different colleges take the same exam. You want to see if one college outperforms the other. A factory is testing three different methods for producing breadsticks and wants to know which yields a superior crunchiness.  One-Way vs. Two-Way ANOVA Refers to the number of independent variables in the test.\nOne-Way\n Testing to see if there is a difference in the single independent variable between $n$ groups If $n=2$, it is equivalent to a t-test and you should just do that instead $(F = t^2)$  Two-Way\n Tests the impact of two independent variables that can have multiple levels on a single dependent variable Can be done with replication, which means you\u0026rsquo;re duplicating your test(s) with multiple groups, meaning they are doing more than one thing  Levels refers to the different groups within an independent variable. E.g. one independent variable is brand of cereal: Cheerios, Corn Flakes, Shreddies (3 levels). Another other independent variable is sugar content: sweetened or unsweetened (2 levels).\nAssumptions (focusing on One-Way ANOVA)  Data from each group is generated froma a normal (or approximately normal) distribution. Responses in a given group are independent and identically distributed (I.I.D) The variance of each group is the same (but can be unknown at the outset of the test)  TLDR: the groups have the same distribution shapes, but testing to see if they\u0026rsquo;re centered around different means.\nThis is an assumption under ideal circumstances, but doens\u0026rsquo;t need to be absolute - ANOVA is robusy to heterogeneity (i.e. difference) of variance so long as the largest variance isn\u0026rsquo;t more than 4 times the smallest variance. The general affect that heterogeneity is that it makes ANOVA less efficient and the test has less power.\nNull and Alternate Hypotheses $H_0$: The group means come from the same population: $\\mu_1 = \\mu_2 = \\mu_n $\n$H_A$: The group means don\u0026rsquo;t come from the same population, the variable of interest is different due to the groups.\nANOVA is always a one-sided hypothesis test.\nTest Statistic Test Statistic $W$ is defined as:\n$$ W = \\frac{MS_B}{MS_W} $$\nWhere $MS_B$ is the Between Group Variance, defined as:\n$$ MS_B = m \\times \\text{sample variance of group means} $$\n$$ MS_B = \\frac{m}{n - 1} \\sum_{i=1}^n \\left(\\bar{x}_{i} - \\bar{x}\\right)^{2} $$\nAnd $MS_W$ is the Average Within Group Variance, defined as:\n$$ MS_W = \\text{sample mean of } s_{i}^2, \\ldots, s_{n}^2 $$\n$$ MS_W = \\frac{s_{i}^2, \\ldots , s_{n}^2}{n} $$\nExample Conducting a One-Way ANOVA test to see whether employee stress levels are the same or different before, during, and after layoffs. Employees have their stress levels surveyed at these three different times and the average stress level of the group is calculated.\n$H_0$: Layoffs don\u0026rsquo;t have an impact on stress levels, so the mean stress level before, during, and after the layoffs should be the same.\n$H_A$: Layoffs do impact stress levels, so means should be different before, during, and after.\nThis is a one-sided test that we will conduct with significance $\\alpha = 0.05$\nimport numpy as np import matplotlib.pyplot as plt from scipy.stats import f, f_oneway  normal_stress = np.array([2, 3, 7, 2, 6]) announced_stress = np.array([10, 8, 7, 5, 10]) during_stress = np.array([10, 13, 14, 13, 15]) groups = [normal_stress, announced_stress, during_stress] total_data = np.concatenate(groups, axis = 0) m = len(normal_stress) n = len(groups)  for group in groups: print(group.mean())  4.0 8.0 13.0  m, n  (5, 3)  MSw = np.array([x.var(ddof=1) for x in groups]).mean() # taking sample variance so ddof=1  MSw  4.5  MSb = np.sum(np.square(np.array([x.mean() - total_data.mean() for x in groups]))) * (m/(n - 1))  MSb  101.66666666666666  W = MSb / MSw  W  22.59259259259259  Calculating the same test statistic with SciPy\u0026rsquo;s f_oneway method:\nstatistic, p_value = f_oneway(normal_stress, announced_stress, during_stress) statistic, p_value  (22.59259259259259, 8.538592454274676e-05)  W == statistic  True  Test Statistic Intuition If population means $\\mu_i$ are equal then ratio that equals $W$ should be near 1, with the variance between the groups being close to the variance within the groups (since they\u0026rsquo;re all from the same underlying population). If they\u0026rsquo;re not equal then the between group variance $MS_B$ should be larger while the average within group variance $MS_W$ stays the same.\nNull Distribution $f\\left(W \\vert H_0\\right)$ is the PDF of $W \\text{\\textasciitilde} F\\left(n - 1, n (m - 1)\\right)$, where $F$ is the F-Distribution with $(n - 1)$ and $n(m - 1)$ degrees of freedom.\nfrom scipy.stats import f  dfn = n - 1 dfd = n * (m - 1) f_dist = f(dfn, dfd)  f_dist.ppf(0.95)  3.8852938346523933  dfn, dfd  (2, 12)  f_range = np.linspace(f_dist.ppf(0.0001), f_dist.ppf(0.9999), num=1000)  plt.figure(figsize=(8,5)) plt.plot(f_range, f_dist.pdf(f_range)) plt.axvline(f_dist.ppf(0.95), c='r', label='p=0.05') plt.scatter(W, f_dist.pdf(W), c='g', label='test statistic') plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7f8f128913d0\u0026gt;  Given the results of the test we can reject the null hypothesis $H_0$ in favour of $H_A$ - employee stress levels do seem to be impacted by layoffs (duhhhh).\nAppendix - Refresh on Population and Sample Variance/Standard Deviation Calculations Population Variance and Standard Deviation:\n$$ \\sigma^2 = \\frac{\\Sigma (x_i - \\mu)^2}{N} $$\n$$ \\sigma = \\sqrt{\\frac{\\Sigma (x_i - \\mu)^2}{N}} $$\nSample Variance and Standard Deviation:\n$$ S^2 = \\frac{\\Sigma (x_i - \\bar{x})^2}{n - 1} $$\n$$ s = \\sqrt{\\frac{\\Sigma (x_i - \\bar{x})^2}{n - 1}} $$\nIn the population calculations, we\u0026rsquo;re dividing by the total size of the population, while in the sample calculations we\u0026rsquo;re dividing by the sample\u0026rsquo;s total degrees of freedom.\n","date":"2020-12-20","permalink":"https://billwarker.com/posts/anova/","tags":["stats"],"title":"ANOVA"},{"content":"Article Link: https://sloanreview.mit.edu/article/leading-with-decision-driven-data-analytics\n  Companies have more data than ever, but many fail to produce actionable insights or good results\n  Being \u0026ldquo;data-driven\u0026rdquo; doesn\u0026rsquo;t stop analysts and leadership from making poor decisions\n  Failing to ask and answer the right/meaningful questions can be a trap to fall into, data can be used to reinforce existing beliefs\n  Decision-driven data analytics: instead of finding a purpose for existing data, find data for existing purpose\n  Asking the Right Question   E.g. company wants to reduce churn, develops a sophisticated model to predict which customers are likely to churn, and then sends them a gift basket\n  A churn model is valuable information, but the decision made off of it (send a gift basket) is untested\n  How does sending a gift basket affect a customer\u0026rsquo;s likelihood to churn? This can\u0026rsquo;t be answered by existing data (need for experiment)\n  Too much emphasis on relying on pre-existing data can lead one into the trap of only thinking of questions or context that can be answered/explained by it\n  If data-driven decision making anchors on just available data, it can lead decision makers to focus on the wrong questions\n  Decision-driven data analytics starts with a proper definition of the decision that needs to be made, and then looks for data to make an educated action on it\n  Avoid the Trap of Reinforcing Existing Beliefs   Twitter offers a three-step attribution process for businesses to evaluate the effectiveness of advertising on it\n  Get customer data \u0026gt; look for customers in twitter history \u0026gt; if customers are found, add information on their engagement with the business\n  Analysts will then compare these with customers who didn’t engage on Twitter, see a stark difference (customers on Twitter are much more engaged), and conclude that Twitter plays a major impact on sales\n  Twitter would love for people to make this conclusion\n  Comparing customers who do and don’t use Twitter is apples to oranges\n  Twitter engagement doesn’t necessarily cause sales, sales are from customers who like the product, and therefore engage on twitter\n  Again, correlation $\\neq$ causation\n  Decision-Driven Data Analytics 3 steps:\n#1 Define the decision that needs to be made and consider the options available\n What needs to be done and why? Is the solution already known/obvious? Consider all possible options for achieving the desired result, and narrow the list down through judgement/data  #2 Determine what data would be needed to narrow down the list\n Develop criteria to rank the options Goal of data analytics is to turn unknowns into knowns Starting with the decision to be made forces us to consider what we don’t know Focus on the high-impact data that would determine the outcome of a decision Once this is thought out, gather the data (i.e. experiment)  #3 Use the insights gained to execute on the best course of action\n It all comes down to asking important questions and thinking beyond the scope of what you already know ","date":"2020-12-10","permalink":"https://billwarker.com/posts/article-summary-leading-with-decision-driven-data-analytics/","tags":["analytics","strategy"],"title":"Article Summary - Leading With Decision-Driven Data Analytics"},{"content":"Part 3 of 3 on a series of notes covering margin of error, power, and sample size calculations.\nNotes, with questions and examples, taken from the following reading: https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html\nEnsuring a Test has High Power Power values of 80-90% are commonly accepted as norms when running hypothesis tests. Certain power levels can guaranteed in studies by including enough samples to control the variability in the parameter of interest.\nThe input for the sample size formulas include desired power, the level of significance, and the effect size. Effect size is selected to represent a meaningful or practically important difference in the parameter of interest.\nThe formulas below produce minimum sample sizes to ensure that their associated flavours of hypothesis tests will have a specified probability of rejecting the null hypothesis when it is false (i.e. power). Additionally, for certain studies one may need to factor in the likelihood of attrition or lose in the samples afterwards.\nSample Size for One Sample, Continuous Outcome In a hypothesis test comparing the mean of a continuous normal variable in a single population to a known mean, the hypotheses are:\n$$ H_0: \\mu = \\mu_0 $$ $$ H_A: \\mu \\neq \\mu_0 $$\nWhere $\\mu_0$ is the known mean (e.g. historical control).\nThe formula for determining sample size to ensure the test has a specified power is:\n$$ n = \\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\n $\\alpha$ is the selected level of significance and $Z_{1-\\alpha/2}$ is the value from the standard normal distribution holding $1-\\alpha/2$ below it $1-\\beta$ is the selected power, and $Z_{1-\\beta}$ is the value from the standard normal distribution holding $1-\\beta$ below it  For 80% power, this associated Z value is $Z_{0.80} = 0.84$. For 90% power, it is $Z_{0.90} = 1.282$.\nfrom scipy.stats import norm import numpy as np  norm.ppf(0.8)  0.8416212335729143  norm.ppf(0.9)  1.2815515655446004  $ES$ is the effect size, defined as follows:\n$$ ES = \\frac{\\lvert\\mu_1 - \\mu_0\\rvert}{\\sigma} $$\n $\\mu_0$ is the mean under $H_0$ $\\mu_1$ is the mean under $H_1$ $\\sigma$ is the standard deviation of the outcome of interest  The numerator of the effect size is the absolute difference in means, $\\lvert\\mu_1 - \\mu_0\\rvert$, representing what is considered a meaningful or important difference in the population.\nIt can be difficult to underestimate $\\sigma$ at the outset of a test - in sample size calculations it is common to use a value from a previous study or a study performed on a comparable population. Regardless, $\\sigma$ should always be conservative (i.e. reasonably large), so that the resultant sample size isn\u0026rsquo;t too small.\nExample  An investigator hypothesizes that in people free of diabetes, fasting blood glucose, a risk factor for coronary heart disease, is higher in those who drink at least 2 cups of coffee per day. A cross-sectional study is planned to assess the mean fasting blood glucose levels in people who drink at least two cups of coffee per day. The mean fasting blood glucose level in people free of diabetes is reported as 95.0 mg/dL with a standard deviation of 9.8 mg/dL.7 If the mean blood glucose level in people who drink at least 2 cups of coffee per day is 100 mg/dL, this would be important clinically. How many patients should be enrolled in the study to ensure that the power of the test is 80% to detect this difference? A two sided test will be used with a 5% level of significance.  two_cups_glucose = 100.0 mean_glucose = 95.0 std_glucose = 9.8 effect_size = (two_cups_glucose - mean_glucose)/std_glucose effect_size  0.5102040816326531  The effect size represents a meaningful standardized difference in the population mean - 95 mg/dL vs. 100 mg/dL, or 0.51 standard deviation units different.\nZ_significance = norm.ppf(1 - (0.05/2)) beta = 0.2 Z_power = norm.ppf(1 - beta) n_patients = ((Z_significance + Z_power)/effect_size)**2 n_patients  30.152256387475454  Therefore a sample size of n=31 (rounding up) will ensure that a two-sided test with $\\alpha = 0.05$ has 80% power to detect a 5 mg/dL difference in mean fasting blood glucose levels.\n In the planned study, participants will be asked to fast overnight and to provide a blood sample for analysis of glucose levels. Based on prior experience, the investigators hypothesize that 10% of the participants will fail to fast or will refuse to follow the study protocol.  Factoring in 10% attritition to hit the needed 31 participants:\n31 / (1 - 0.1)  34.44444444444444  Factoring in an attrition rate of 10%, 35 participants should be enrolled in the study.\nSample Size for One Sample, Bernoulli Outcome In studies where the plan is to perform a hypothesis test comparing the proportion of successes in a bernoulli variable in a single population to a known proportion, the hypotheses become:\n$$ H_0: p = p_0 $$ $$ H_A: p \\neq p_0 $$\nThe formula for calculating sample size remains the same as the one for one sample, continuous outcome. This is because a bernoulli random variable approximates to a normal distribution across many trials due to CLT:\n$$ n = \\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\nThe effect size $ES$ is calculated as:\n$$ ES = \\frac{\\lvert p_A - p_0\\rvert}{\\sqrt{p_0(1-p_0)}} $$\n where $p_0$ is the proportion under $H_0$ and $p_A$ is the proportion under $H_A$ the numerator is again a meaningful difference in proportions  We use $p_0$ for the standard deviation calculation in the denominator because we want to measure the effect size of $p_A$, the proportion in our alternate hypothesis, in relation to what we already know/have observed about the population.\nExample  A medical device manufacturer produces implantable stents. During the manufacturing process, approximately 10% of the stents are deemed to be defective. The manufacturer wants to test whether the proportion of defective stents is more than 10%. If the process produces more than 15% defective stents, then corrective action must be taken. Therefore, the manufacturer wants the test to have 90% power to detect a difference in proportions of this magnitude. How many stents must be evaluated? For you computations, use a two-sided test with a 5% level of significance.  p0_stents = 0.1 pA_stents = 0.15 effect_size = np.abs(pA_stents - p0_stents)/\\ np.sqrt(p0_stents * (1 - p0_stents)) effect_size  0.1666666666666666  We could round this effect size up to 0.17 - doing so would simplify our understanding of what the standardized difference we\u0026rsquo;re looking for is, but it would lower the number of samples in our test by way of increasing the denominator.\nalpha = 0.05 beta = 0.1 Z_significance = norm.ppf(1 - (alpha/2)) Z_power = norm.ppf(1 - beta) n_stents = np.square((Z_significance + Z_power)/effect_size) n_stents  378.26723021186257  Therefore, 379 stents should be evaluated to ensure that a two-sided test with $\\alpha = 0.05$ and 90% power would detect a 5% difference (the delta between a 10% and 15% defective rate) in the proportion of defective stents produced.\nSample Sizes for Two Independent Samples, Continuous Outcomes When the plan is to perform a hypothesis test on the mean difference of a continuous random variable (CRV) in two independent populations, the hypotheses of interest are:\n$$ H_0: \\mu_1 = \\mu_2 $$ $$ H_A: \\mu_1 \\neq \\mu_2 $$\nwhere $\\mu_1$ and $\\mu_2$ are the means in the two comparison populations.\nThe formulas for determining sample size and effect size:\n$$ n_i = 2\\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\n$$ ES = \\frac{\\lvert\\mu_1 - \\mu_0\\rvert}{\\sigma} $$\nWhere $n_i$ is the sample size required in each group (i=1,2). When doing a hypothesis test on two independent groups, the pooled estimate of standard deviation $S_p$ is used:\n$$ S_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{(n_1 + n_2 - 2)}} $$\nIf the variability of each of the two populations is known at the outset of the test, then we can use algebra to get the sample sizes by reversing the equation (and setting $n_1 = n_2$, generating samples of equal size). However, it is usually the case that data on the variability of the outcome will only be available for a single population that you\u0026rsquo;re testing an alternative against. This can be used as a substitute for the standard deviation in the effect size to plan the test.\nExample  An investigator is planning a study to assess the association between alcohol consumption and grade point average among college seniors. The plan is to categorize students as heavy drinkers or not using 5 or more drinks on a typical drinking day as the criterion for heavy drinking. Mean grade point averages will be compared between students classified as heavy drinkers versus not using a two independent samples test of means. The standard deviation in grade point averages is assumed to be 0.42 and a meaningful difference in grade point averages (relative to drinking status) is 0.25 units. How many college seniors should be enrolled in the study to ensure that the power of the test is 80% to detect a 0.25 unit difference in mean grade point averages? Use a two-sided test with a 5% level of significance.  gpa_std = 0.42 gpa_delta = 0.25 alpha = 0.05 beta = 0.2  Since variability is only known for average GPA (not for the two populaitons, heavy drinkers vs. non heavy drinkers), we\u0026rsquo;ll use it to plan the study.\neffect_size = gpa_delta/gpa_std effect_size  0.5952380952380952  Pretty large effect size (i.e. we\u0026rsquo;re testing for a pretty obvious difference between the two populations), so we won\u0026rsquo;t need as many samples to achieve our desired power.\nZ_significance = norm.ppf(1 - (alpha/2)) Z_power = norm.ppf(1 - beta) n_students = 2 * np.square((Z_significance + Z_power)/effect_size) n_students  44.30535632445374  Therefore in each group we would need 44 students - 44 heavy drinkers, 44 who aren\u0026rsquo;t heavy drinkers, 88 students total.\nSample Size for Matched Samples, Continuous Outcome  in studies where the plan is to perform a hypothesis test on the mean difference in a continuous outcome variable based on matched data:  $$ H_0: \\mu_d = 0 $$ $$ H_A: \\mu_d \\neq 0 $$\nWhere $\\mu_d$ is the mean difference in the population.\nThe formula for sample size is again:\n$$ n_i = \\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\nWhile effect size is calculated as:\n$$ ES = \\frac{\\mu_d}{\\sigma_d} $$\nWhere $\\sigma_d$ is the standard deviation of the difference in the outcome (i.e. difference based on measurements over time/between matched pairs).\nExample  An investigator wants to evaluate the efficacy of an acupuncture treatment for reducing pain in patients with chronic migraine headaches. The plan is to enroll patients who suffer from migraine headaches. Each will be asked to rate the severity of the pain they experience with their next migraine before any treatment is administered. Pain will be recorded on a scale of 1-100 with higher scores indicative of more severe pain. Each patient will then undergo the acupuncture treatment. On their next migraine (post-treatment), each patient will again be asked to rate the severity of the pain. The difference in pain will be computed for each patient. A two sided test of hypothesis will be conducted, at α =0.05, to assess whether there is a statistically significant difference in pain scores before and after treatment. How many patients should be involved in the study to ensure that the test has 80% power to detect a difference of 10 units on the pain scale? Assume that the standard deviation in the difference scores is approximately 20 units.  pain_std = 20 pain_delta = 10 alpha = 0.05 beta = 0.2 effect_size = pain_delta/pain_std effect_size  0.5  Z_significance = norm.ppf(1 - (alpha/2)) Z_power = norm.ppf(1 - beta) n_samples = np.square((Z_significance + Z_power) / effect_size) n_samples  31.395518937396353  Therefore a sample size n=32 patients with migraines will ensure that a two sided test with $\\alpha=0.05$ has 80% power to detect a mean difference of 10% pain before and after the treatment, assuming all patients complete the treatment.\nSample Sizes for Two Independent Samples, Dichotomous Outcomes In studies where the plan is to perform a hypothesis test comparing the proportions of successes in two independent populations, the hypotheses of interest are:\n$$ H_0: p_1 = p_2 $$ $$ H_A: p_1 \\neq p_2 $$\nWhere $p_1$ and $p_2$ are the proportions in the two comparison populations.\nThe formulas for determining sample size and effect size are:\n$$ n_i = 2\\left(\\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{ES}\\right)^2 $$\n$$ ES = \\frac{\\lvert{p_1 - p_2}\\rvert}{\\sqrt{p(1-p)}} $$\n $n_i$ is the sample size required for each group (i=1,2). $\\lvert{p_1 - p_2}\\rvert$ is the absolute value of the difference in proportions between the two groups expected under the alternate hypothesis $H_A$. $p$ is the overall proportion, based on pooling the data from the two comparison groups (can be computed by taking the mean of the proportions in the two groups, assuming that the groups will be of approximately equal size.  Example   Clostridium difficile (also referred to as \u0026ldquo;C. difficile\u0026rdquo; or \u0026ldquo;C. diff.\u0026quot;) is a bacterial species that can be found in the colon of humans, although its numbers are kept in check by other normal flora in the colon. Antibiotic therapy sometimes diminishes the normal flora in the colon to the point that C. difficile flourishes and causes infection with symptoms ranging from diarrhea to life-threatening inflammation of the colon. Illness from C. difficile most commonly affects older adults in hospitals or in long term care facilities and typically occurs after use of antibiotic medications.\n  In recent years, C. difficile infections have become more frequent, more severe and more difficult to treat. Ironically, C. difficile is first treated by discontinuing antibiotics, if they are still being prescribed. If that is unsuccessful, the infection has been treated by switching to another antibiotic. However, treatment with another antibiotic frequently does not cure the C. difficile infection. There have been sporadic reports of successful treatment by infusing feces from healthy donors into the duodenum of patients suffering from C. difficile. (Yuk!) This re-establishes the normal microbiota in the colon, and counteracts the overgrowth of C. diff.\n  The efficacy of this approach was tested in a randomized clinical trial reported in the New England Journal of Medicine (Jan. 2013). The investigators planned to randomly assign patients with recurrent C. difficile infection to either antibiotic therapy or to duodenal infusion of donor feces. In order to estimate the sample size that would be needed, the investigators assumed that the feces infusion would be successful 90% of the time, and antibiotic therapy would be successful in 60% of cases. How many subjects will be needed in each group to ensure that the power of the study is 80% with a level of significance α = 0.05?\n  p_feces = 0.9 p_anti = 0.6 p = np.mean([p_feces, p_anti]) effect_size = np.abs(p_feces - p_anti)/np.sqrt(p * (1 - p)) effect_size  0.692820323027551  Again, pretty sizeable effect size.\nalpha = 0.05 beta = 0.2 Z_significance = norm.ppf(1 - alpha/2) Z_power = norm.ppf(1 - beta) n_subjects = 2 * np.square((Z_significance + Z_power) / effect_size) n_subjects  32.70366555978786  Each group would need about 33 subjects, so (66 subjects total) to detect a 30% difference in the two methods with $\\alpha = 0.05$ and 80% power.\nConcluding Notes Determining the appropriate design of a study is more important than the analysis; you can always re-analzye the data, you can\u0026rsquo;t always just redo studies. We need a sample size large enough to answer the research question, byachieving acceptable margins of error and powers for the results.\n","date":"2020-11-29","permalink":"https://billwarker.com/posts/calculating-sample-size-to-ensure-high-power/","tags":["stats"],"title":"Calculating Sample Size to Ensure High Power"},{"content":"Part 2 of 3 on a series of notes covering margin of error, power, and sample size calculations.\nNotes, with questions and examples, taken from the following reading: https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html\nType I and Type II Errors, and their relationship to Power In hypothesis testing there are two kinds of errors that can be made when deciding whether to reject the null hypothesis or not:\nType I Error A type I error is falsely rejecting $h_0$ (the null hypothesis) when it is actually true, i.e. a false positive. Imagine a doctor looking at a man and telling him he\u0026rsquo;s pregnant. The level of significance in a hypothesis test, $\\alpha$ (alpha), is the probability of a type I error occuring:\n$\\alpha$ = P(Type I Error) = P(Rejct $H_0$ | $H_0$ is True)\nWe can use alpha as a control for the probability of making a type I error.\nType II Error A type II error is not rejecting $h_0$ when it it\u0026rsquo;s actually false, i.e. a false negative. This time, the doctor looks at a pregnant lady and tells her she\u0026rsquo;s got to start exercising to get rid of that giant bump in her belly. The probability of making a type II error is denoted as $\\beta$ (beta):\n$\\beta$ = P(Type II Error) = P(Do not reject $H_0$ | $H_0$ is False)\nStatistical Power The power of a hypothesis test is the probability that $H_0$ will be correctly rejected when it is false. In other words, its the probability of detecting an effect if it actually exists. This is the probability of not making a type II error:\nPower = 1 - $\\beta$ = 1 - P(Do not reject $H_0$ | $H_0$ is False)\nA good hypothesis tests has a low significance threshold (small $\\alpha$) and high power (small $\\beta$). Power is a single piece in a puzzle of four interconnected parts:\n The chosen significance level of the hyptothesis test, $\\alpha$ The desired power of the test, (1 - $\\beta$) The sample size, which determines the variability of the parameter of interest The effect size, the difference observed in the parameter of interest that denotes a meaningful difference (determined through domain knowledge)  Power analysis involves estimating one of these variables given we know the values of the other three.\nExample Say we have some parameter of interest in a population that we want to make an inference on. We want to test the following hypothesis about this parameter:\nThe Null Hypothesis $H_0$: the population mean $\\mu$ for the parameter is 90. $\\mu$ = 90\nThe Alternative Hypothesis $H_A$: the population mean $\\mu$ for the parameter is not 90. $\\mu$ $\\neq$ 90\nHere are the conditions for the test:\n We want to test a hypothesis with a significance level of $\\alpha$ = 0.05, i.e. the probability of a false positive is only 5%. The test is two sided, meaning we are testing to see if the parameter of interest is lower or higher than our null hypothesis From previous tests on the population we can safely estimate that the standard deviation $\\sigma$ of the parameter is 20. $\\sigma$ = 20 To conduct this test we select a sample of n = 100  To conduct the test we compute the parameter\u0026rsquo;s sample mean $\\bar{X}$ and then decide whether it provides enough evidence to support the alternative hypothesis. To do this we compute a test statistic and compare it to the appropriate critical value; since we know the variability of the parameter we can use a Z test.\nIf the null hypothesis is true ($\\mu$ = 90) then we are likely to select a sample whose mean is close to 90. However it\u0026rsquo;s possible to have a sample mean that is much larger or smaller than 90.\nWe can use the Central Limit Theorum here: when n is sufficiently large (in this case n=100 is enough), the distribution of sample means is approximately normal with a mean of:\n$$ \\mu_X = \\mu $$\nThe standard error of our sample can be calculated as:\n$$ SE = \\frac{\\sigma}{\\sqrt{n}} = \\frac{20}{\\sqrt{100}} = 2.0 $$\nIf the null hypothesis is true, then it is possible to observe any sample mean from the sampling distribution below:\nimport numpy as np from scipy.stats import norm import matplotlib.pyplot as plt  h0_true_mean = 90 standard_deviation = 20 sample_size = 100 standard_error = standard_deviation / np.sqrt(sample_size) h0_sample_dist = norm(h0_true_mean, standard_error) x_range = h0_sample_dist.ppf(np.linspace(0.0001, 0.9999, num=1000)) plt.figure(figsize=(12,6)) plt.plot(x_range, h0_sample_dist.pdf(x_range)) plt.title(\u0026quot;Sampling Distribution for $\\\\bar{X}$ given $H_0$: $\\mu$ = 90 is True\u0026quot;, fontsize=16, pad=10) plt.xlabel(\u0026quot;Values for $\\\\bar{X}$\u0026quot;, fontsize=12) plt.ylabel(\u0026quot;$\\\\bar{X}$ Probability (PDF)\u0026quot;, fontsize=12) plt.show()  Given this sampling distribution, we determine critical lower and upper values at which we reject $H_0$ based on our chosen significance ($\\alpha = 0.05$) and the decision that this will be a two-sided test:\nupper_rejection_cutoff = h0_sample_dist.ppf(0.975) # 2.5% probability of occuring at or after this threshold lower_rejection_cutoff = h0_sample_dist.ppf(0.025) # 2.5% probability of occuring at or before this threshold print(f\u0026quot;Upper rejection cutoff: {upper_rejection_cutoff}\u0026quot;) print(f\u0026quot;Lower rejection cutoff: {lower_rejection_cutoff}\u0026quot;)  Upper rejection cutoff: 93.9199279690801 Lower rejection cutoff: 86.0800720309199  Speaking in terms of the Z test, we would take the calculated sample mean $\\bar{X}$ and convert it into a Z score. We\u0026rsquo;d then find this Z score\u0026rsquo;s probability on a standard normal distribution and if it was less than 5% (i.e. outside of our lower and upper rejection cutoffs), we would reject $H_0$.\nIn this example the critical values for a two-sided test with $\\alpha$ = 0.05 are 86.06 and 93.92 (-1.96 and 1.96 on the Z scale), so the decision rule becomes reject $H_0$ if $\\bar{X}$ $\\leq$ 86.06 or if $\\bar{X}$ $\\geq$ 93.92.\nplt.figure(figsize=(12,6)) plt.plot(x_range, h0_sample_dist.pdf(x_range)) plt.axvline(lower_rejection_cutoff, color='r', linestyle='--', label=f'{lower_rejection_cutoff}') plt.axvline(upper_rejection_cutoff, color='r', linestyle='--', label=f'{upper_rejection_cutoff}') lower_rejection_range = np.linspace(h0_sample_dist.ppf(0.0001), lower_rejection_cutoff, num=1000) upper_rejection_range = np.linspace(upper_rejection_cutoff, h0_sample_dist.ppf(0.9999), num=1000) non_rejection_range = np.linspace(lower_rejection_cutoff, upper_rejection_cutoff, num=1000) plt.fill_between(non_rejection_range, 0, h0_sample_dist.pdf(non_rejection_range), color='green', alpha=0.5) plt.fill_between(lower_rejection_range, 0, h0_sample_dist.pdf(lower_rejection_range), color='red', alpha=0.5) plt.fill_between(upper_rejection_range, 0, h0_sample_dist.pdf(upper_rejection_range), color='red', alpha=0.5) plt.title(\u0026quot;Rejection Region for Test $H_0$: $\\\\mu$ = 90 vs. $H_A$: $\\\\neq$ 90 at $\\\\alpha$ = 0.05\u0026quot;, fontsize=16, pad=10) plt.xlabel(\u0026quot;Values for $\\\\bar{X}$\u0026quot;, fontsize=12) plt.ylabel(\u0026quot;$\\\\bar{X}$ Probability (PDF)\u0026quot;, fontsize=12) plt.legend() plt.show()  The red areas that aren\u0026rsquo;t between the two rejection lines represent the probability of a Type I error, which are the values for sample mean $\\bar{X}$ whose probabilities sum to $\\alpha$ = 0.05. If $\\bar{X}$ is in these regions, we reject $H_0$ with a 5% probability of making a type I error. The green area represents the chosen range where $\\bar{X}$ supports the null hypothesis, so we do not reject $h_0$.\nIf we suppose the alternative hypothesis, $H_A$ is true ($\\mu$ $\\neq$ 90) and that the true mean is actually 94, this is what the distributions of the sample mean look like for the null and alternate hypotheses:\ntrue_mean = 94 x_range = np.linspace(80,100, num=1000) hA_sample_dist = norm(true_mean, standard_error) plt.figure(figsize=(12,6)) plt.axvline(true_mean, linestyle='--', label='$\\mu$ = 94') plt.axvline(lower_rejection_cutoff, color='r', linestyle='--', label=f'{lower_rejection_cutoff}') plt.axvline(upper_rejection_cutoff, color='r', linestyle='--', label=f'{upper_rejection_cutoff}') plt.plot(x_range, h0_sample_dist.pdf(x_range), color='b', label='$H_0$') plt.plot(x_range, hA_sample_dist.pdf(x_range), color='r', label='$H_A$') false_neg_range = np.linspace(lower_rejection_cutoff, upper_rejection_cutoff, num=1000) plt.fill_between(false_neg_range, 0, hA_sample_dist.pdf(false_neg_range), color='red', alpha=0.5) power_range = np.linspace(upper_rejection_cutoff, 100, num=1000) plt.fill_between(power_range, 0, hA_sample_dist.pdf(power_range), color='green', alpha=0.5) plt.title(\u0026quot;Distribution of $\\\\bar{X}$ under $H_0$: $\\\\mu$ = 90 and under $H_A$: $\\\\mu$ = 94 \u0026quot;) plt.xlabel(\u0026quot;Values for $\\\\bar{X}$\u0026quot;, fontsize=12) plt.ylabel(\u0026quot;$\\\\bar{X}$ Probability (PDF)\u0026quot;, fontsize=12) plt.legend() plt.show()  If the true mean is 94, then the alternative hypothesis $H_A$ is true. The probability of a type II error, $\\beta$, is the red shaded area: this is the overlap between the alternate hypothesis' distribution has with the \u0026ldquo;do not reject region\u0026rdquo; of the null hypothesis.\nThe test\u0026rsquo;s power, i.e. the probability of a true positive, rejecting $h_0$ when it is truly false, is the green shaded area to the right of the null hypothesis' upper rejection cutoff (as set by $\\alpha$). It can be calculated as the probability of $\\bar{X}$ being a value greater than $H_0$\u0026rsquo;s upper rejection cutoff of 93.91, given $H_A$ is true (1 - probability of beta).\nTo do this we can put the upper rejection cutoff $\\bar{X}$ = 93.91 in terms of its associated Z statistic:\n$$ Power = 1 - \\beta = P(\\bar{X} \u0026gt; 93.91 | H_A) $$\n$$ Power = P\\left(Z \u0026gt; \\frac{93.92 - 94}{\\frac{20}{\\sqrt{100}}}\\right) $$\n$$ Power = P\\left(Z \u0026gt; -0.04\\right) $$\nWe can convert the Z score of -0.04 using the cumulative density function, which will represent the probablility of drawing values less than or equal to -0.04 on a standard normal distribution\nbeta_calc = (upper_rejection_cutoff - true_mean)/standard_error  norm.cdf(beta_calc)  0.4840322065576678  This gives us a beta $\\beta$ of 0.484 (the probability of $\\bar{X}$ being less or equal to 93.91, giving us a false negative). From there we can just subtract this value from 1 to get the probability of that not happening (true positive):\n$$ Power = 1 - \\beta = P(\\bar{X} \u0026gt; 93.91 | H_A) = 1 - 0.484 = 0.516 $$\nTherefore, the given power of this test between $H_0$ and $H_A$ is 51.6% (not great). $\\beta$ can also be calculated from our $H_A$ distribution object, by obtaining the CDF at $H_0$\u0026rsquo;s upper rejection region:\nbeta_from_dist = hA_sample_dist.cdf(upper_rejection_cutoff) power = round(1 - beta_from_dist, 4) * 100 print(f\u0026quot;The power of this hypothesis test is {power}%\u0026quot;)  The power of this hypothesis test is 51.6%  $\\beta$ and power are related to $\\alpha$, the variance of the outcome and the effect size (i.e. the difference in the parameter of interest between $H_0$ and $H_A$). If we increased $\\alpha$ from 0.05 to 0.10, the upper rejection limit of $H_0$ would shift to the left and be larger, increasing the test\u0026rsquo;s power. While this would give the test higher power, it would also reduce the confidence we could have in the test.\nThe effect size and variance of the outcome affect power in clear ways:\n Increase the desired effect size between $H_0$ and $H_A$ to move their respective distributions further away from each other, reducing their overlap Gathering more samples and reducing the variance of $H_0$ and $H_A$\u0026rsquo;s distributions will also reduce their overlap  Using the exact same components as the plot above, here is what the test\u0026rsquo;s power becomes when $H_0$: $\\mu$ = 90 and $H_A$: $\\mu$ = 98, an effect size of 8 units:\nhA_mean = 98 x_range = np.linspace(80,110, num=1000) hA_sample_dist = norm(hA_mean, standard_error) plt.figure(figsize=(12,6)) plt.axvline(lower_rejection_cutoff, color='r', linestyle='--', label=f'{lower_rejection_cutoff}') plt.axvline(upper_rejection_cutoff, color='r', linestyle='--', label=f'{upper_rejection_cutoff}') plt.plot(x_range, h0_sample_dist.pdf(x_range), color='b', label='$H_0$') plt.plot(x_range, hA_sample_dist.pdf(x_range), color='r', label='$H_A$') false_neg_range = np.linspace(lower_rejection_cutoff, upper_rejection_cutoff, num=1000) plt.fill_between(false_neg_range, 0, hA_sample_dist.pdf(false_neg_range), color='blue', alpha=0.5) power_range = np.linspace(upper_rejection_cutoff, 110, num=1000) plt.fill_between(power_range, 0, hA_sample_dist.pdf(power_range), color='green', alpha=0.5) plt.title(\u0026quot;Distribution of $\\\\bar{X}$ under $H_0$: $\\\\mu$ = 90 and under $H_A$: $\\\\mu$ = 94 \u0026quot;) plt.xlabel(\u0026quot;Values for $\\\\bar{X}$\u0026quot;, fontsize=12) plt.ylabel(\u0026quot;$\\\\bar{X}$ Probability (PDF)\u0026quot;, fontsize=12) plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7ff6971a7390\u0026gt;  Calculating the Power for this test by obtaining $\\beta$ from $H_A$\u0026rsquo;s distribution variable:\nbeta_from_dist = hA_sample_dist.cdf(upper_rejection_cutoff) power = round(1 - beta_from_dist, 4) * 100 print(f\u0026quot;The power of this hypothesis test is {power}%\u0026quot;)  The power of this hypothesis test is 97.92999999999999%  Note to be continued in Part 3: Ensuring a Test has High Power","date":"2020-11-22","permalink":"https://billwarker.com/posts/understanding-statistical-power/","tags":["stats"],"title":"Understanding Statistical Power"},{"content":"Part 1 of 3 on a series of notes covering margin of error, power, and sample size calculations.\nNotes, with questions and examples, taken from the following reading: https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html\nConfidence Interals, Margins of Error, and Sample Sizes Confidence intervals take the following general form: Point Estimate $\\pm$ Margin of Error\nFor confidence intervals based on normal data, this looks like:\n$$ \\bar{X} \\pm E $$\n $\\bar{X}$ is the sample mean generated through the experiment (our point estimate) $E$ is the margin of error, calculated as: $$ E=Z\\frac{\\sigma}{\\sqrt{n}} $$\n $Z$ is the Z statistic of a standard normal distribution for a desired confidence level (Z = 1.96 for 95% confidence) $\\sigma$ is the standard deviation of the population $\\mu$ (as best as we know/can estimate it) $\\sqrt{n}$ is the square root of the sample size  In planning experiments we need to determine the sample size required to achieve a sufficiently small margin of error. If the margin of error is too wide then the test is fairly uninformative. To determine the sample size needed first we need to define the desired margin of error, and then we can use algebra to solve:\n$$ E = Z\\frac{\\sigma}{\\sqrt{n}} $$\n$$ \\sqrt{n}E = Z\\sigma $$\n$$ \\sqrt{n} = \\frac{Z\\sigma}{E} $$\n$$ n = \\left(\\frac{Z\\sigma}{E}\\right)^2 $$\n$\\sigma$ can be difficult to estimate at the outset of a experiment, so it can be appropriate to use a value for the standard deviation from a previous study done to a comparable population. However it\u0026rsquo;s determined, $\\sigma$ should be a conservative estimate (i.e. as large as is reasonable) so that the resulting sample size isn\u0026rsquo;t too small.\nThe following examples demonstrate these sample size calculations for different scenarios and random variables.\nSample Size for One Sample, Continuous Outcome Example 1:\n An investigator wants to estimate the mean systolic blood pressure in children with congenital heart disease who are between the ages of 3 and 5. How many children should be enrolled in the study? The investigator plans on using a 95% confidence interval (so Z=1.96) and wants a margin of error of 5 units. The standard deviation of systolic blood pressure is unknown, but the investigators conduct a literature search and find that the standard deviation of systolic blood pressures in children with other cardiac defects is between 15 and 20.  from scipy.stats import norm  Z = norm.ppf(0.975) std = 20 E = 5  n = ((Z*std)/E)**2  n  61.46334113110599  In order to ensure a 95% confidence interval the study will need 62 participants (rounding up). Selecting a smaller sample size could potentially produce a confidence interval with a larger margin of error.\nQuestion 1:\n An investigator wants to estimate the mean birth weight of infants born full term (approximately 40 weeks gestation) to mothers who are 19 years of age and under. The mean birth weight of infants born full-term to mothers 20 years of age and older is 3,510 grams with a standard deviation of 385 grams. How many women 19 years of age and under must be enrolled in the study to ensure that a 95% confidence interval estimate of the mean birth weight of their infants has a margin of error not exceeding 100 grams? If 5% of women are expected to deliver prematurely, how many participants should there be to account for this possibility?  Z = norm.ppf(0.975) std = 385 E = 100  n = ((Z*std)/E)**2  n  56.94002336973868  In order to ensure a 95% confidence interval the study will need 57 participants. If 5% of women are expected to deliver prematurely then we would need $\\frac{n}{0.95} = 60$ participants\nexpected_premature = 0.05 n = (n/(1 - expected_premature))  n  59.93686670498809  Sample Size for One Sample, Binary Outcome (Bernoulli) In experiments to estimate the proportion of successes in a variable with a binary outcome (yes/no, AKA a bernoulli random variable), the formula becomes:\n$$ n = p(1-p)\\left(\\frac{Z}{E}\\right)^2 $$\n $n$ is equal to the variance of a bernoulli trial multiplied by the square of the desired confidence Z score over the margin of error  Working backwards to get the margin of error:\n$$ n = p(1-p)\\left(\\frac{Z}{E}\\right)^2 = \\sigma^2\\left(\\frac{Z}{E}\\right)^2 $$\n$$ \\frac{n}{\\sigma^2} = \\left(\\frac{Z}{E}\\right)^2 $$\n$$ \\sqrt{\\frac{n}{\\sigma^2}} = \\frac{Z}{E} $$\n$$ \\frac{\\sqrt{n}}{\\sigma} = \\frac{Z}{E} $$\n$$ E\\frac{\\sqrt{n}}{\\sigma} = Z $$\n$$ \\frac{E}{\\sigma} = \\frac{Z}{\\sqrt{n}} $$\n$$ E = Z\\frac{\\sigma}{\\sqrt{n}} $$\nIn planning an experiment, $p$ is our estimate of the propensity for the binary outcome to be a success and $1-p$ is the propensity for it to be failure. If no knowledge is known for an estimate of $p$, using 0.5 (50/50 chance) will maximize the variance and the sample size.\nExample 2:\n An investigator wants to estimate the proportion of freshmen at his University who currently smoke cigarettes (i.e., the prevalence of smoking). How many freshmen should be involved in the study to ensure that a 95% confidence interval estimate of the proportion of freshmen who smoke is within 5% of the true proportion?  Since we have no information of the proportion of freshmen who smoke, we use 0.5 to estimate the sample size as follows:\np = 0.5 Z = norm.ppf(0.975) E = 0.05  n = p*(1-p)*(Z/E)**2  n  384.14588206941244  To ensure a 95% confidence interval estimate of the proportion of freshmen who smoke is within 5% of the true population, a sample size of 385 is needed.\nQuestion 2:\n Suppose that a similar study was conducted 2 years ago and found that the prevalence of smoking was 27% among freshmen. If the investigator believes that this is a reasonable estimate of prevalence 2 years later, it can be used to plan the next study. Using this estimate of p, what sample size is needed (assuming that again a 95% confidence interval will be used and we want the same level of precision)?  p = 0.27 Z = norm.ppf(0.975) E = 0.05  n = p*(1-p)*(Z/E)**2  n  302.86061342352474  To ensure a 95% confidence interval estimate of the proportion of freshmen who smoke is within 5% of the true population, a sample size of 303 is needed.\nExample 3:\n An investigator wants to estimate the prevalence of breast cancer among women who are between 40 and 45 years of age living in Boston. How many women must be involved in the study to ensure that the estimate is precise? National data suggest that 1 in 235 women are diagnosed with breast cancer by age 40. This translates to a proportion of 0.0043 (0.43%) or a prevalence of 43 per 10,000 women. Suppose the investigator wants the estimate to be within 10 per 10,000 women with 95% confidence.  p = 43/10000 Z = norm.ppf(0.975) E = 10/10000  n = p*(1-p)*(Z/E)**2  n  16447.244355390107  A sample size of n=16447 will ensure a 95% confidence interval estimate of the prevelance of breast cancer is within 0.10 (10 women per 10,000).\n Suppose this sample size isn\u0026rsquo;t feasible, and the investigators thought a sample size of 5,000 would be practical How precisely can we estimate the prevalence with a sample size of n=5,000?  The confidence interval formula to estimate prevalence is:\n$$ \\hat{p}\\pm Z\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}} $$\nThis is just the sample mean plus/minus the Z score multiplied the standard error of the mean. If we assume the prevalence of breast cancer in the sample will be close to that based on national data, we can expect the margin of error to be approximately:\n$$ Z\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}} = 1.96\\sqrt{\\frac{0.0043(1-0.00.43)}{5000}} = 0.0018 $$\nsample_size = 5000  E = Z*((p*(1-p))/sample_size)**(1/2)  E  0.0018136837847535663  With n=5,000 women in the sample, a 95% confidence interval would be expected to have a margin of error of 0.0018 (18 per 10,000). The investigators would need to decide if this is precise enough to answer the question. This comes with the assumption that the propensity for one to get breast cancer in Boston is similar to the propensity to get it nationally, which might be a stretch.\nSample Sizes for Two Independent Samples, Continuous Outcome For studies where the plan is to estimate the difference in means between two independent populations, the formula for determining sample sizes becomes:\n$$ n_i = 2\\left(\\frac{Z\\sigma}{ES_p}\\right)^2 $$\n $n_i$ is the sample size required in each group $Z$ is again the Z score from the standard normal distribution for the confidence level used $E$ is the desired margin of error $\\sigma$ is the standard deviation of the outcome variable $S_p$ is the pooled estimate of the common standard deviation between the two populations, calculated as:  $$ S_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{(n_1 + n_2 - 2)}} $$\nIf data is available on variability of the outcome in each population, then $S_p$ can be computed and used in the sample size formula. Usually though there\u0026rsquo;s only data on the variance in one group, usually the control.\nWhen planning an investigation data is often available from other trials that involved a placebo or control group, and a standard deviation from these trials can be used for the experimental (non-control) group in this trial. When this is the case we forget about $S_p$ and just use the following:\n$$ n_i = 2\\left(\\frac{Z\\sigma}{E}\\right)^2 $$\nNote: sample size formula generates estimates for samples of equal size, alternative formulas can be used for samples of different sizes\nSkipping Example 4 and going straight to Example 5:\n An investigator wants to compare two diet programs in children who are obese. One diet is a low fat diet, and the other is a low carbohydrate diet. The plan is to enroll children and weigh them at the start of the study. Each child will then be randomly assigned to either the low fat or the low carbohydrate diet. Each child will follow the assigned diet for 8 weeks, at which time they will again be weighed. The number of pounds lost will be computed for each child. Based on data reported from diet trials in adults, the investigator expects that 20% of all children will not complete the study. A 95% confidence interval will be estimated to quantify the difference in weight lost between the two diets and the investigator would like the margin of error to be no more than 3 pounds. How many children should be recruited into the study? Again, the issue is determining the variability in the outcome of interest (σ), here the standard deviation in pounds lost over 8 weeks. To plan this study, investigators use data from a published study in adults. Suppose one such study compared the same diets in adults and involved 100 participants in each diet group. The study reported a standard deviation in weight lost over 8 weeks on a low fat diet of 8.4 pounds and a standard deviation in weight lost over 8 weeks on a low carbohydrate diet of 7.7 pounds.  Can use the information in the second bullet to compute the pooled estimate of the standard deviation between low fat and low carbohydrate groups:\nn_fat = 100 n_carb = 100 std_fat = 8.4 std_carb = 7.7 std_pooled = (((n_fat - 1)*std_fat**2 \\ + (n_carb - 1)*std_carb**2) \\ / (n_fat + n_carb - 2))**(1/2)  std_pooled  8.057605103254938  $S_p = 8.1 $, rounding up. We will use this as $\\sigma$ in our experiment, and do not need to multiply the margin of error by the pooled variance (drop $ES_p$ in the denominator and just use $E$):\nZ = norm.ppf(0.975) E = 3 n = 2*((Z*std_pooled)/E)**2  n  55.423714207459135  $n = 56$, rounding up. This means that $2 \\times n_i = 2 \\times 56 = 112$ children should be recruited for the study (not counting attrition). If we factor in an attrition rate of 20%:\nattrition_rate = 0.2 n_total = (n * 2) / (1 - attrition_rate)  n_total  138.55928551864784  Factoring in attrition, about 140 children should participate in the study.\nSample Size for Matched Samples, Continuous Outcomes In studies where the plan is to estimate the mean difference of a continuous outcome based on matched (i.e. paired) data:\n$$ n = \\left(\\frac{Z\\sigma_d}{E}\\right)^2 $$\nIn this case, $\\sigma_d$ is the standard deviation of the difference scores. The standard deviation between the paired data points must be used here, you can\u0026rsquo;t estimate the difference using past trials.\nSample Sizes for Two Independent Samples, Binary Outcome In studies where the plan is to estimate the difference in proportions between two independent populations, the formula for determining the sample sizes required in each comparison group is:\n$$ n_i = {p_1(1-p_1) + p_2(1-p_2)}\\left(\\frac{Z}{E}\\right)^2 $$\n $n_i$ is the sample size required in each group ${p_1(1-p_1) + p_2(1-p_2)}$ is their pooled variance $Z$ is again the Z score from the standard normal distribution for the confidence level used $E$ is the desired margin of error $p_1$ and $p_2$ are the propensities for success in each group  To estimate the sample size we need to approximate $p_1$ and $p_2$, or if we have no prior intuitions and just want to generate the most conservative and largest sample sizes we can again use just 0.5.\nIf we\u0026rsquo;re comparing an unknown group with a group that we know already have data on (e.g. the control group), we can use its proportion for both $p_1$ and $p_2$. Alternative formulas can be used with groups with different sample sizes\nExample 6\n  An investigator wants to estimate the impact of smoking during pregnancy on premature delivery. Normal pregnancies last approximately 40 weeks and premature deliveries are those that occur before 37 weeks. The 2005 National Vital Statistics report indicates that approximately 12% of infants are born prematurely in the United States.5 The investigator plans to collect data through medical record review and to generate a 95% confidence interval for the difference in proportions of infants born prematurely to women who smoked during pregnancy as compared to those who did not. How many women should be enrolled in the study to ensure that the 95% confidence interval for the difference in proportions has a margin of error of no more than 4%?\n  The sample sizes (i.e., numbers of women who smoked and did not smoke during pregnancy) can be computed using the formula shown above. National data suggest that 12% of infants are born prematurely.\n  # using the proportion of p=0.12 for both groups (smoking and non-smoking) Z = norm.ppf(0.975) E = 0.04 p = 0.12 n = (p*(1-p) + p*(1-p))*(Z/E)**2  n  507.07256433162445  A sample size of $n_1=508$ women who smoked during pregnancy and $n_2=508$ who did not during pregnancy will ensure that the 95% confidence interval for the difference in proportions who deliver prematurely will have a margin of error of no more than 4%.\nAttrition could be a factor in this trial as confounding factors could happen to either group (someone stops/starts smoking or decides to drop out for whatever reason)\nNote to be continued in Part 2: Understanding Statistical Power","date":"2020-11-21","permalink":"https://billwarker.com/posts/understanding-margin-of-error-and-sample-size/","tags":["stats"],"title":"Understanding Margin of Error and Sample Size"},{"content":"Well, for starters, a hash is an important part of a very useful and fast data structure called a hash table. Let\u0026rsquo;s reframe the question and ask what a hash table is instead.\nThe idea behind a Hash Table A hash table is a data structure that allows for the fast retrieval of data within it, regardless of how much is being stored. It\u0026rsquo;s used to do many things, from efficient caching to database indexing and error checking. The central idea of a hash table can be explained with an example. Let\u0026rsquo;s say we have a big array filled with random elements, one of which we want to retrieve:\nfrom numpy.random import randint def create_array_with_hidden_string(array_size, hidden_string): random_numbers = randint(0, array_size**2, size=(array_size,)) array = list(random_numbers) element_ix = randint(0, array_size) array.insert(element_ix, hidden_string) return array, element_ix hidden_string = 'yeet' array, hidden_string_ix = create_array_with_hidden_string(10**3, hidden_string)  array[:10] # first 10 elements of the array  [438536, 563467, 983737, 888887, 142932, 801221, 364091, 488761, 87409, 695441]  In the code above we\u0026rsquo;ve created an array with 1001 strings, one of which we want to find. Our string of interest, yeet, has been inserted at random index in our array. Let\u0026rsquo;s find it. One way we could do so is through a linear search - simply iterating through the elements and stopping once we find yeet. Let\u0026rsquo;s create a function that implements a linear search and reports how long it takes:\nimport time def linear_search_and_time(array, hidden_string): t_start = time.time() for i in range(len(array)): if array[i] == hidden_string: t_finish = time.time() t_delta = t_finish - t_start print(f\u0026quot;\u0026quot;\u0026quot;found {array[i]} in array of size {len(array)} at index {i} in {round(t_delta,4)} seconds\u0026quot;\u0026quot;\u0026quot;)  linear_search_and_time(array, hidden_string)  found yeet in array of size 1001 at index 81 in 0.0004 seconds  Linear search certainly does the trick, but what if our array was of a much bigger size? A consideration of linear search is that the time it takes to complete scales, well, linearly with the size of the array being searched. In Big O Notation, the time complexity of a linear search is O(N):\narray_sizes = [10**4, 10**5, 10**6, 10**7] for size in array_sizes: array, hidden_string_ix = create_array_with_hidden_string(size, hidden_string) linear_search_and_time(array, hidden_string)  found yeet in array of size 10001 at index 2145 in 0.0085 seconds found yeet in array of size 100001 at index 28500 in 0.1093 seconds found yeet in array of size 1000001 at index 543310 in 2.1833 seconds found yeet in array of size 10000001 at index 9412368 in 36.1402 seconds  As we can see, the linear search gets slower as the size of the array grows.\nSearch is basically instantaneous if you know the position/index of the element you want to find in the array - no matter its size, an array lookup always takes a fast, constant time. In Big O terms, this is O(1):\ndef index_lookup_and_time(array, hidden_string_ix): t_start = time.time() hidden_string = array[hidden_string_ix] t_finish = time.time() t_delta = t_finish - t_start print(f\u0026quot;\u0026quot;\u0026quot;found {hidden_string} in array of size {len(array)} at index {hidden_string_ix} in {round(t_delta,4)} seconds\u0026quot;\u0026quot;\u0026quot;)  for size in array_sizes: array, hidden_string_ix = create_array_with_hidden_string(size, hidden_string) index_lookup_and_time(array, hidden_string_ix)  found yeet in array of size 10001 at index 4934 in 0.0 seconds found yeet in array of size 100001 at index 72609 in 0.0 seconds found yeet in array of size 1000001 at index 827164 in 0.0 seconds found yeet in array of size 10000001 at index 2401275 in 0.0 seconds  This idea of using the index of an element to access it is at the core of hash tables. Independent of the size of the hash table or the position of any of its elements, actions such as adding, accessing, or deleting have a constant O(1) time complexity.\nHashing Algorithms To achieve this, a unique index is created for each value in the table using a hashing algorithm. When a value is \u0026ldquo;hashed\u0026rdquo;, it means that it is being passed through an algorithm that transforms it into a memory address (i.e. an index position). This address should never change over the lifetime of the element and is calculated using aspects of both the element itself and the hash table it is being placed into. In Python\u0026rsquo;s implementation of object hashing, this requirement means that only immutable (i.e. unchangeable) data types like strings, ints, and tuples can be hashed. If mutable objects were able to be hashed, then the hashing algorithm would generate different indexes for them as they changed and internal consistency would be lost.\nWhen an element is added, deleted, or retrieved from a hash table it is first passed into a hashing algorithm to find its location. Whatever action needs to happen then takes place after that.\nThere are different ways to implement hashing algorithms, but in principle they should all be easy to calculate as to keep things fast. Algorithms can also differ based on the intended inputs' data types. For numeric keys, a simple hashing function might divide the key by the number of available addresses within an array (i.e. its size) and take the remainder:\n# first, let's create an empty array of a certain size def make_empty_hash_table(size: int): return [None] * size  array = make_empty_hash_table(5) print(array)  [None, None, None, None, None]  # create our number hashing algorithm def simple_numeric_hash(num: int, array_size: int): return num % array_size  # check indices generated by elements print(simple_numeric_hash(30, len(array))) print(simple_numeric_hash(21, len(array))) print(simple_numeric_hash(9, len(array)))  0 1 4  # define function to insert the elements at the indices provided by the hashing algorithm def insert_number_into_hash_table(num, array): ix = simple_numeric_hash(num, len(array)) array[ix] = num return array  array = insert_number_into_hash_table(30, array) array = insert_number_into_hash_table(21, array) array = insert_number_into_hash_table(9, array) print(array)  [30, 21, None, None, 9]  For a string key, it might sum the ASCII codes for each character in the string and do the same division by the array size, taking the remainder:\n# create another empty array array = make_empty_hash_table(6)  # create our string hashing algorithm def simple_string_hash(string: str, array_size: int): ascii_sum = 0 for char in string: ascii_sum += ord(char) return ascii_sum % array_size  # check indices generated by pet hamster names print(simple_string_hash('Squeeky', len(array))) print(simple_string_hash('Squishy', len(array))) print(simple_string_hash('Squirrelly', len(array)))  5 2 4  # define function to insert the elements at the indices provided by the hashing algorithm def insert_string_into_hash_table(string, array): ix = simple_string_hash(string, len(array)) array[ix] = string return array  # store some pet hamsters in a hash table array = insert_string_into_hash_table('Squeeky', array) array = insert_string_into_hash_table('Squishy', array) array = insert_string_into_hash_table('Squirrelly', array) print(array)  [None, None, 'Squishy', None, 'Squirrelly', 'Squeeky']  Managing Collisions A hashing algorithm is an elegant way to sort data and have it be quickly accessible. There is one problem that needs to be solved with this approach, however - what if two elements generate the same hash?\nprint(simple_string_hash('Squishy', len(array))) print(simple_string_hash('Smokey', len(array)))  2 2  This is what\u0026rsquo;s known as a collision - when multiple elements yield the same hash. Positions in a hash table can\u0026rsquo;t belong (at least directly) to more than a single element, so available positions are used up as elements are added in. Fortunately, there are a variety of solutions that hash tables can use to manage collisions. These solutions fall into two categories: open addressing and closed addressing.\nOpen addressing solutions will place a collided element in another open position within the hash table. One implementation of this is linear probing, which will do a linear search from the collided index for the next available spot and place the element there. When the element needs to be accessed, the hash table will compute its hash, find the index position, see that position is occupied by something else, and then do a linear search from that spot to find it. Other open addressing algorithms behave similarly, just switching up how the search is performed: plus 3 rehashing checks every third available position, quadratic probing squares the number of failed placement attempts and checks that many positions away, and double hashing applies a second hash function after the first yields a collision. With all of these approaches the goal is find an available position further away from the collision - uniformly distributing across the table will reduce the likelihood of further collisions happening.\nHere\u0026rsquo;s a simple implementation of open addressing with linear probing:\ndef linear_probing_insert(element, ix, array): if array[ix] is None: array[ix] = element else: print(f'collision at index {ix}') lin_search_positions = list(range(ix + 1, len(array))) + list(range(0, ix)) for pos in lin_search_positions: if array[pos] is None: array[pos] = element print(f'added {element} at index {pos} instead') return array print(f'nowhere to put {element}') return array def insert_with_open_addressing(string, array): ix = simple_string_hash(string, len(array)) return linear_probing_insert(string, ix, array)  open_example = array  insert_with_open_addressing('Smokey', open_example)  collision at index 2 added Smokey at index 3 instead [None, None, 'Squishy', 'Smokey', 'Squirrelly', 'Squeeky']  Closed addressing solutions create linked lists at occupied positions in the hash table and add elements sequentially within them. Whereas open addressing solutions aim to place elements across the entire hash table\u0026rsquo;s available positions, closed addressing solutions opt to nest multiple elements within a sub-level list at the positions themselves (think going deep instead of going wide). Either way, both approaches end up having to do some sort of search after hashing an element and getting a collided position.\nHere\u0026rsquo;s a simple implementation of closed addressing with linked lists:\ndef closed_addressing_insert(element, ix, array): if array[ix] is None: array[ix] = [element] else: print(f'collision at index {ix}') array[ix].append(element) print(f'added {element} in a nested list at index {ix}') def insert_with_closed_addressing(string, array): ix = simple_string_hash(string, len(array)) return closed_addressing_insert(string, ix, array)  closed_example = make_empty_hash_table(6)  insert_with_closed_addressing('Squishy', closed_example) insert_with_closed_addressing('Squirrelly', closed_example) insert_with_closed_addressing('Squeeky', closed_example) insert_with_closed_addressing('Smokey', closed_example)  collision at index 2 added Smokey in a nested list at index 2  closed_example  [None, None, ['Squishy', 'Smokey'], None, ['Squirrelly'], ['Squeeky']]  One can aim to avoid collisions entirely by using a larger hash table with more available positions than values to occupy them. The ratio between stored elements and available positions in a hash table is called its load factor, and the lower this is the lower the likelihood of collisions (and subsequent linear searches). If a hash table contains too many collisions, its performance will worsen as more searches with increased time complexity are needed. No collisions means that access time will always be a speedy O(1) time complexity. A good hashing algorithm should aim to minimize collisions, resolve them quickly if they do happen, and generate a uniform distribution of hash values across the table.\nThe Difference between a Hash Table and a Hash Map A map is an association between a key and a value - for key K remember value V.\nMapping between keys and values can be done in many different ways, and one way of doing so is with a hash table. A hash table is just a structure for storing data, but when it stores data that is in the form of a distinct key-value pairing, it\u0026rsquo;s called a hash map.\nIn a hash table that simply stores values like the one above, there are no key-value pairs; each value is its own key with no association with another object.\nIn essence, a hash table is a data structure for quickly storing and accessing a data. A hash map is a particular kind of hash table that stores key-value pairs.\nHash Map Implementation with Lists There are three components to this Hash Map:\n Hash Table: a simple array Hashing Algorithm: a simple hash function that sums the ASCII values in a string, divides it by the size of the array, and returns the remainder Collision Handling: A closed addressing solution for adding elements into a list at each position, and then performing a linear search at the time of accessing  class HashMap: def __init__(self, size): self.size = size self.map = [None] * self.size def _get_hash(self, key): hash_ = 0 for char in str(key): hash_ += ord(char) return hash_ % self.size def add(self, key, value): key_hash = self._get_hash(key) key_value = [key, value] if self.map[key_hash] is None: self.map[key_hash] = list([key_value]) return True else: for pair in self.map[key_hash]: if pair[0] == key: pair[1] = value return True self.map[key_hash].append(key_value) return True def get(self, key): key_hash = self._get_hash(key) if self.map[key_hash] is not None: for pair in self.map[key_hash]: if pair[0] == key: return pair[1] return None def delete(self, key): key_hash = self._get_hash(key) if self.map[key_hash] is None: return False for i in range(0, len(self.map[key_hash])): if self.map[key_hash][i][0] == key: self.map[key_hash].pop(i) return True def display(self): for item in self.map: if item is not None: print(str(item))  # lists of hamsters and their favourite foods - the key value pairs to be inserted into the hash map hamsters = ['Squeeky', 'Squishy', 'Speedy', 'Smokey', 'Squirrelly', 'Snowy', 'Sleepy', 'Smoochy', 'Smarty', 'Snoozy', 'Smoothy', 'Shiny'] fav_foods = ['Pizza', 'Corn', 'Baguettes', 'Beef Jerky', 'Pho', 'Pad Thai', 'Shawarma', 'Chocolate', 'Carrots', 'Wine', 'Smoothies', 'Apples']  To demonstrate the idea of a hash table\u0026rsquo;s load factor, we\u0026rsquo;ll create two hash maps - one with a size equalling the number of elements, and one with a size that is the square of the number of elements.\nhamster_hash_map = HashMap(len(hamsters)) for hamster, food in zip(hamsters, fav_foods): hamster_hash_map.add(hamster, food)  hamster_hash_map.display()  [['Squishy', 'Corn'], ['Sleepy', 'Shawarma']] [['Squirrelly', 'Pho'], ['Snowy', 'Pad Thai'], ['Smarty', 'Carrots']] [['Speedy', 'Baguettes'], ['Smoochy', 'Chocolate']] [['Shiny', 'Apples']] [['Smokey', 'Beef Jerky']] [['Snoozy', 'Wine']] [['Squeeky', 'Pizza'], ['Smoothy', 'Smoothies']]  When the hash table is small relative to the number of elements being stored, collisions can occur. Not a big deal with only a few elements, but at much larger scales this would begin to hurt performance.\nbig_hamster_hash_map = HashMap(len(hamsters)**2) for hamster, food in zip(hamsters, fav_foods): big_hamster_hash_map.add(hamster, food)  big_hamster_hash_map.display()  [['Smoochy', 'Chocolate']] [['Squeeky', 'Pizza']] [['Smoothy', 'Smoothies']] [['Squishy', 'Corn']] [['Speedy', 'Baguettes']] [['Sleepy', 'Shawarma']] [['Smokey', 'Beef Jerky']] [['Smarty', 'Carrots']] [['Squirrelly', 'Pho']] [['Snoozy', 'Wine']] [['Shiny', 'Apples']] [['Snowy', 'Pad Thai']]  When the size of the hash table is much larger than the number of elements, there\u0026rsquo;s a reduced chance of collisions occuring. This is an ideal scenario where the table is highly performant and has an increased likelihood of O(1) access times.\n","date":"2020-09-28","permalink":"https://billwarker.com/posts/what-is-a-hash/","tags":["comp-sci"],"title":"What is a Hash?"},{"content":"Notes Classification: predicting classes/categories\nIntroducing MNIST  handwritten digits  from sklearn.datasets import fetch_openml  mnist = fetch_openml('mnist_784', version=1)  mnist.keys()  dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])  X, y = mnist['data'], mnist['target']  X.shape  (70000, 784)   70K samples, 784 features (28x28 pixels)  y.shape  (70000,)  import matplotlib as mpl import matplotlib.pyplot as plt  some_digit = X[0] some_digit_image = some_digit.reshape(28,28) plt.imshow(some_digit_image, cmap=\u0026quot;binary\u0026quot;) plt.axis(\u0026quot;off\u0026quot;) plt.show()  y[0]  '5'   cast target variable to integers  import numpy as np  y = y.astype(np.uint8)  X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:],   dataset is already shuffled, some CV folds will be similar (no missing digits)  Training a binary classifier  simplify the problem to classifying a single digit Stochastic Gradient Descent (SGD) classifier: good for large datasets, deals with training instances independently, one at a time relies on randomness  y_train_5 = (y_train == 5) y_test_5 = (y_test == 5)  from sklearn.linear_model import SGDClassifier  sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5)  SGDClassifier(random_state=42)  sgd_clf.predict([some_digit])  array([ True])  Performance Measures  Can use cross validation Custom implementation of CV:  from sklearn.model_selection import StratifiedKFold from sklearn.base import clone  skfolds = StratifiedKFold(n_splits=3, random_state=42) for train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_5[train_index] X_test_fold = X_train[test_index] y_test_fold = y_train_5[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct/len(y_pred))  /opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True. FutureWarning 0.95035 0.96035 0.9604   performs stratified sampling to get a representative ratio of each class  from sklearn.model_selection import cross_val_score  cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.95035, 0.96035, 0.9604 ])   accuracy doesn\u0026rsquo;t mean much here, since about 10% of the samples are 5s. If you guess non-5 for every sample you\u0026rsquo;ll get around 90% accuracy  from sklearn.base import BaseEstimator  class Never5Classifier(BaseEstimator): def fit(self, X, y=None): return self def predict(self, X): return np.zeros((len(X), 1), dtype=bool)  never_5_clf = Never5Classifier() cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')  array([0.91125, 0.90855, 0.90915])   accuracy is generally not the preferred performance measure, especially with skewed datasets  Confusion Matrix\n count the number of times instances of class A are classfied as class B  from sklearn.model_selection import cross_val_predict   cross_val_predict returns the predictions for each test fold  y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)  y_train_pred.shape  (60000,)  from sklearn.metrics import confusion_matrix  confusion_matrix(y_train_5, y_train_pred)  array([[53892, 687], [ 1891, 3530]])    each row represents an actual class\n  each column represents a predicted class\n  first row of this matrix is non-5s, second row is 5s\n  row 1 col 1: true negatives\n  row 1 col 2: false positives\n  row 2 col 1: false negatives\n  row 2 col 2: true positives\n  a perfect classifier would only have true positives and true negatives\n  # pretending we reached perfection y_train_perfect_predictions = y_train_5 confusion_matrix(y_train_5, y_train_perfect_predictions)  array([[54579, 0], [ 0, 5421]])  Precision and Recall\nPrecision: accuracy of positive predictions $$ precision = \\frac{TP}{TP+FP} $$\nRecall (AKA sensitivity or True Positive Rate): $$ recall = \\frac{TP}{TP+FN} $$\nfrom sklearn.metrics import precision_score, recall_score  precision_score(y_train_5, y_train_pred)  0.8370879772350012  recall_score(y_train_5, y_train_pred)  0.6511713705958311   higher precision than recall: when the classifier predicted a 5, it was likely to be correct. however, this came at the cost of incorrectly predicting non-5 on samples that it was less sure about its convenient to combine precision and recall into a single metric, the F1 score:  $$ F_1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} = 2\\times\\frac{precision\\times{recall}}{precision + recall} = \\frac{TP}{TP + \\frac{FN+TP}{2}}$$\n this is the harmonic mean; while regular mean treats all values equally, harmonic mean gives much more weight to low values F1 is only high if both precision and recall are  from sklearn.metrics import f1_score  f1_score(y_train_5, y_train_pred)  0.7325171197343846   while F1 is a metric that favours equally good precision and recall, there are instances when prioritizing one of the two is more valuable e.g. a classifier that detects whether videos are safe for kids: you want high precision because the cost of being wrong is very high, and its better to reject potentially safe videos if it ensures that no unsafe videos are recommended e.g. a classifier for catching shoplifting can favour recall; maximizing the potential for catching all instances of shoplifting is well worth the potential for a few false positives here and there there is a tradeoff between precision and recall, can\u0026rsquo;t have it both ways  Precision/Recall Trade-Off\n controlled by the threshold at which a sample is classified as true can control this threshold by calling decision_function() instead of predict() for sklearn estimators and setting a threshold yourself:  y_scores = sgd_clf.decision_function([some_digit]) y_scores  array([2164.22030239])  threshold = 0 y_some_digit_pred = (y_scores \u0026gt; threshold) y_some_digit_pred  array([ True])  threshold = 8000 y_some_digit_pred = (y_scores \u0026gt; threshold) y_some_digit_pred  array([False])   raising the threshold increases precision and lowers recall lowering the threshold increases recall and lowers precision  y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function')  from sklearn.metrics import precision_recall_curve  precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)  import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline sns.set_style('whitegrid')  def plot_precision_recall_vs_threshold(precisions, recalls, thresholds, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(thresholds, precisions[:-1], 'b--', label='Precision') plt.plot(thresholds, recalls[:-1], 'g--', label='Recall') plt.legend()  plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.show()   precision can go down when the threshold increases e.g. getting 4/5 (80%) correct, then raising threshold and getting 3/4 correct (75%) can also plot precision vs. recall directly:  def plot_precision_vs_recall(precisions, recalls, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(recalls, precisions, 'b--') plt.xlabel('Recall') plt.ylabel('Precision')  plot_precision_vs_recall(precisions, recalls) plt.show()   precision really starts to dip around 70% recall might make sense to set the threshold before that drop, but depends on the context of your project to set the threshold at a specific precision: np.argmax() gives the first index of the maximum value; in this case, the first instance where precision \u0026gt; 90 is true  threshold_90_precision = thresholds[np.argmax(precisions \u0026gt;= 0.9)] threshold_90_precision  3370.0194991439557  y_train_pred_90 = (y_scores \u0026gt;= threshold_90_precision)  precision_score(y_train_5, y_train_pred_90)  0.9000345901072293  recall_score(y_train_5, y_train_pred_90)  0.4799852425751706  The ROC Curve\n Receiver Operating Characteristic plots true positive rate (TPR) vs false positive rate (FPR) FPR is ratio of false positives, i.e. 1 - True Negative Rate (TNR) TNR is also known as specificity ROC is plotting sensitivity (recall) vs 1 - specificity  from sklearn.metrics import roc_curve  fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)  def plot_roc_curve(fpr, tpr, label=None, figsize=(10,6)): plt.figure(figsize=figsize) plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0,1], [0,1], 'k--') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate (Recall)') plt.legend()  plot_roc_curve(fpr, tpr) plt.show()  No handles with labels found to put in legend.   dotted line represents purely random classifier good classifiers have the highest true positive rate with the lowest false positive rate (top left corner) can compare ROC scores of different classifiers by measuring area under the curve AUC a purely random classifier will have an AUC of 0.5 (think the integral of the purely random classifier, the linear line) false positive rate is so high here because there aren\u0026rsquo;t that many 5s in the data (only about 10%), so don\u0026rsquo;t get misled by the great looking ROC curve  from sklearn.metrics import roc_auc_score  roc_auc_score(y_train_5, y_scores)  0.9604938554008616   use the precision/recall curve when the positive class is rare or you care more about the false positives than the false negatives otherwise use the ROC curve for skewed datasets where the positive class is rare, you can see that precision really suffers when recall increases (a result of having less training data on the positive class, the classifier is trying to catch every positive class despite not having a lot to go off of). paints a different picture than ROC Try calculating ROC, AUC, Precision and Recall for Random Forest estimator:  from sklearn.ensemble import RandomForestClassifier  forest_clf = RandomForestClassifier(random_state=42)  y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba')  y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class  fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)  plot_roc_curve(fpr_forest, tpr_forest, label=\u0026quot;Random Forest\u0026quot;) plt.plot(fpr, tpr, \u0026quot;b:\u0026quot;, label=\u0026quot;SGD\u0026quot;) plt.legend() plt.show()   Closer to top left than SGD, more AUC, better performance Explaining the ROC curve: to achieve an almost 99% recall/TPR rate (we correctly predict positive for 99% of all the real positive samples in the dataset), it seems like we will have to accept the tradeoff of getting 15% of the negative samples incorrectly predicted as positive (the false positive rate)  roc_auc_score(y_train_5, y_scores_forest)  0.9983436731328145  y_preds_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)  precision_score(y_train_5, y_preds_forest)  0.9905083315756169  recall_score(y_train_5, y_preds_forest)  0.8662608374838591  Multiclass Classification  AKA multinomial Logistic Regression, Random Forest, naive Bayes classifiers are examples of algorithms that can make multiclass classifications natively Can get around this with algos that only work as binary; in the example of MNIST train 10 binary classifiers, pick the class with the highest decision score. Known as one-versus-rest/one-versus-all strategy (OvR). You could also train a binary classifier for every pair of digits. i.e. 1 vs 2, 1 vs 3, 2 vs 5, etc. This is one-versus-one (OvO) strategy, and you pick the classifier that wins the most duels. Advantage of this strategy is that you only train the classifiers on subsets of the entire target variable space Some algos scale poorly with the size of the training set, so OvO strategy is preferred In general however, OvR is preferred, as its more straightforward Sklearn classifiers will automatically detecty multiclass classifications from the target variable and will use a strategy based on the algorithm used  from sklearn.svm import SVC  # svm_clf = SVC() # svm_clf.fit(X_train, y_train) # all digits in target variable # svm_clf.predict([some_digit])    SVC uses OvO strategy; it actually trained 45 binary classifiers\n  NOTE: This takes forever to run. Sklearn can\u0026rsquo;t use GPUs to speed up training; GPUs are only useful for training deep learning models with architectures like Tensorflow or PyTorch\n  SVMs have a quadratic time complexity, calculating the distance between each point in the dataset: $$ O({n_{features}}\\times{n_{observations}^2}) $$\n  Caches common points but this kills memory regardless\n  This doesn\u0026rsquo;t scale well over a couple 10k features\n  decision_function() returns 10 scores per instance, and classifier picked the highest one:\n  # some_digit_scores = svm_clf.decision_function([some_digit]) # some_digit_scores  # np.argmax(some_digit_scores)  # svm_clf.classes[np.argmax(some_digit_scores)]   index just happened to match the class itself, but this is just luck Sklearn can be forced to use either OvO or OvR using OneVsOneClassifier or OneVsRestClassifier classes; just create an instance with the classifier you want passed as constructor  from sklearn.multiclass import OneVsRestClassifier  # ovr_clf = OneVsRestClassifier(SVC()) # ovr_clf.fit(X_train, y_train) # ovr_clf.predict([some_digit])  # len(ovr_clf.estimators_)   SGD Classifiers can directly classify instances into multiple classes Decision function returns a score per class  sgd_clf.fit(X_train, y_train) sgd_clf.predict([some_digit])  array([3], dtype=uint8)  sgd_clf.decision_function([some_digit])  array([[-31893.03095419, -34419.69069632, -9530.63950739, 1823.73154031, -22320.14822878, -1385.80478895, -26188.91070951, -16147.51323997, -4604.35491274, -12050.767298 ]])  cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.87365, 0.85835, 0.8689 ])   84% on all testing folds is decent - if you were to use a random classifier you\u0026rsquo;d get 10% accuracy simply scaling inputs can increase score:  from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))  cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\u0026quot;accuracy\u0026quot;)  array([0.8983, 0.891 , 0.9018])  Error Analysis  can improve shortlisted models by analyzing the errors they make make predictions using the cross_val_predict() function, then call confusion_matrix():  y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) conf_mx = confusion_matrix(y_train, y_train_pred)  conf_mx  array([[5577, 0, 22, 5, 8, 43, 36, 6, 225, 1], [ 0, 6400, 37, 24, 4, 44, 4, 7, 212, 10], [ 27, 27, 5220, 92, 73, 27, 67, 36, 378, 11], [ 22, 17, 117, 5227, 2, 203, 27, 40, 403, 73], [ 12, 14, 41, 9, 5182, 12, 34, 27, 347, 164], [ 27, 15, 30, 168, 53, 4444, 75, 14, 535, 60], [ 30, 15, 42, 3, 44, 97, 5552, 3, 131, 1], [ 21, 10, 51, 30, 49, 12, 3, 5684, 195, 210], [ 17, 63, 48, 86, 3, 126, 25, 10, 5429, 44], [ 25, 18, 30, 64, 118, 36, 1, 179, 371, 5107]])   As a heatmap:  plt.matshow(conf_mx, cmap=plt.cm.gray) plt.xlabel(\u0026quot;Predicted\u0026quot;) plt.ylabel(\u0026quot;Actual Values\u0026quot;) plt.show()    Main diagonal: predicted rate of actual digit\n  Rows are actual classes, Columns are predicted classes\n  Lower rate could mean that less of that class in the dataset or lower performance on it\n  Can divide absolute prediction sums by total number of instances for each class to get error rates:\n  row_sums = conf_mx.sum(axis=1, keepdims=True) norm_conf_mx = conf_mx / row_sums np.fill_diagonal(norm_conf_mx, 0) # fill in diagonals to look at errors only plt.matshow(norm_conf_mx, cmap=plt.cm.gray) plt.xlabel(\u0026quot;Predicted\u0026quot;) plt.ylabel(\u0026quot;Actual Values\u0026quot;) plt.show()   Many different numbers get misclassified for 8s, but 8s themselves seem to get classified properly Confusion matrix isn\u0026rsquo;t symmetrical necessarily General confusion around 5s and 3s too If we wanted to fix the model, focusing on improving scores on 8s would be beneficial: Could collect more data on numbers that look like 8s but aren\u0026rsquo;t 8s Could add extra features, like writing an algorithm to count closed loops, or preprocessing the image to make some patterns like closed loops stand out more Analyzing individual errors is good too but time consuming  def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = (len(instances) - 1) // images_per_row + 1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap = mpl.cm.binary, **options) plt.axis(\u0026quot;off\u0026quot;)  cl_a, cl_b = 3, 5 X_aa = X_train[(y_train == cl_a) \u0026amp; (y_train_pred == cl_a)] X_ab = X_train[(y_train == cl_a) \u0026amp; (y_train_pred == cl_b)] X_ba = X_train[(y_train == cl_b) \u0026amp; (y_train_pred == cl_a)] X_bb = X_train[(y_train == cl_b) \u0026amp; (y_train_pred == cl_b)] plt.figure(figsize=(8,8)) plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5) plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5) plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5) plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)    row 1 col 1: 3s that were classified as 3s\n  row 1 col 2: 3s that were classified as 5s\n  row 2 col 1: 5s that were classified as 3s\n  row 2 col 2: 5s that were classified as 5s\n  Hard to understand why SGD classifier made the errors it did; as a linear model, it just assigned a weight per pixel and when it sees a new image it sums up the weighted pixel intensities to get a score for each class\n  3s are different from 5s mainly with the vertical line that connects the top horizontal line and the bottom arc; this means the classifier would be quite sensitive to image shifting and rotation\n  Could preprocess images to make sure they\u0026rsquo;re centered/not too rotated\n  Multilabel Classification  Assigning multiple labels to one sample E.g. detecting multiple people\u0026rsquo;s faces in a photo, or whether a digit is even or odd Outputs multiple binary tags. E.g. y_pred = [1, 0, 1] (yes to classes 1 and 3, no to 2)  from sklearn.neighbors import KNeighborsClassifier   Side note: % (Modulus) yields the remainder when the first operand is divided by the second  5 % 3 # 3 goes into 5 once, with remainder 2  2  10 % 3 # 3 goes into 10 three times, with remainder 1  1  y_train_large = (y_train \u0026gt;= 7) y_train_odd = (y_train % 2 == 1) y_multilabel = np.c_[y_train_large, y_train_odd]  y_multilabel  array([[False, True], [False, False], [False, False], ..., [False, True], [False, False], [ True, False]])   label 1: digit is 7 or above label 2: digit is odd  knn_clf = KNeighborsClassifier() knn_clf.fit(X_train, y_multilabel)  KNeighborsClassifier()   KNeighbors supports multilabel classification, though not all classifiers do  knn_clf.predict([some_digit]) # some_digit = 5  array([[False, True]])   One approach to measure multilabel performance is measure F1 score for each individual label and compute the average score across them:  y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)  f1_score(y_multilabel, y_train_knn_pred, average=\u0026quot;macro\u0026quot;)  0.976410265560605   this assumes that all labels are equally important, which might not always be the case can give labels weight according to number of appeareances in training data by setting average=\u0026quot;weighted\u0026quot;  Multioutput Classification  Generalization of multilabel classification where each label can be multiclass (i.e. have more than two possible values) following example denoises images by predicting what the pixel intensitiy should be for each pixel in a sample (multiple classes for multiple labels) this somewhat blurs the line between classification and regression (predicting pixel intensity is more of a regression task)  noise = np.random.randint(0, 100, (len(X_train), 784)) X_train_mod = X_train + noise noise = np.random.randint(0, 100, (len(X_test), 784)) X_test_mod = X_test + noise y_train_mod = X_train y_test_mod = X_test  def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = mpl.cm.binary, interpolation=\u0026quot;nearest\u0026quot;) plt.axis(\u0026quot;off\u0026quot;)  knn_clf.fit(X_train_mod, y_train_mod)  KNeighborsClassifier()  some_index = np.random.randint(0, len(X_test_mod)) clean_digit = knn_clf.predict([X_test_mod[some_index]])  plt.figure(figsize=(8,4)) plt.subplot(121); plot_digit(X_test_mod[some_index]) plt.subplot(122); plot_digit(clean_digit)  Exercises in a Separate Notebook","date":"2020-09-26","permalink":"https://billwarker.com/posts/handson-ml-ch3/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 3 Notes"},{"content":"Notes Main Steps:\n Frame the problem Get data EDA (exploratory data analysis) Prepare the data for ML Model Selection Tune the model Present solution Launch, monitor, iterate  Look at the big picture  predict the median housing price for a district in CA what\u0026rsquo;s the business objective (not building a model for fun) ask: what is the current, non-ML solution? why can\u0026rsquo;t we use that start thinking about/designing the system  Pipelines\n sequence of data processing components typically runs asynchronously: When you execute something synchronously, you wait for it to finish before moving on to another task. When you execute something asynchronously, you can move on to another task before it finishes. components are self contained, downstream components can keep working for a while by just using the last output from the broken component (async)  Types of Regression\n multiple regression: multiple features to make a prediction univariate regression: only trying to predict a single value  Selecting a performance measure\n Root Mean Square Error (RMSE):  $$ RMSE(X,h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^{2}} $$\n gives higher weight to larger errors lowercase italic font for scalars, lowercase bold for vectors, uppercase bold for matrices if there are many outliers, then Mean Absolute Error (MAE) might be a better cost function:  $$ MAE(X,h) = \\frac{1}{m} \\sum_{i=1}^{m} \\lvert h(x^{(i)}) - y^{(i)}\\rvert$$\n different ways to measure difference between vectors RMSE is euclidian distance, $l_2$ norm computing sum of absolutes is $l_1$ norm, measures the distance between two points if you can only travel along orthogonal (perpendicular) lines $l_0$ just gives the number of non-zero elements in vector higher the norm index, the more it focuses on large values and neglects smaller ones. this is why RMSE is more sensitive to outliers than MAE  Verify Assumptions\n list and verify assumptions e.g. what does the output exactly need to be, what do we think is true about the problem/solution we\u0026rsquo;ve proposed  Get the Data  usually data is in DBs or spread across many files, so common first step is jumping through the hoops of access, getting used to schemas, legal precautions, etc. best to automate process of fetching data, future proof against changes  import os import tarfile from six.moves import urllib  DOWNLOAD_ROOT = \u0026quot;https://raw.githubusercontent.com/ageron/handson-ml/master/\u0026quot; HOUSING_PATH = os.path.join(\u0026quot;datasets\u0026quot;, \u0026quot;housing\u0026quot;) HOUSING_URL = DOWNLOAD_ROOT + \u0026quot;datasets/housing/housing.tgz\u0026quot; def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): if not os.path.isdir(housing_path): os.makedirs(housing_path) tgz_path = os.path.join(housing_path, \u0026quot;housing.tgz\u0026quot;) urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close()  fetch_housing_data()  import pandas as pd  def load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(HOUSING_PATH, \u0026quot;housing.csv\u0026quot;) return pd.read_csv(csv_path)  housing = load_housing_data() housing.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY     housing.info()  \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total_rooms 20640 non-null float64 total_bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB  housing[\u0026quot;ocean_proximity\u0026quot;].value_counts()  \u0026lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64  housing.describe()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value     count 20640.000000 20640.000000 20640.000000 20640.000000 20433.000000 20640.000000 20640.000000 20640.000000 20640.000000   mean -119.569704 35.631861 28.639486 2635.763081 537.870553 1425.476744 499.539680 3.870671 206855.816909   std 2.003532 2.135952 12.585558 2181.615252 421.385070 1132.462122 382.329753 1.899822 115395.615874   min -124.350000 32.540000 1.000000 2.000000 1.000000 3.000000 1.000000 0.499900 14999.000000   25% -121.800000 33.930000 18.000000 1447.750000 296.000000 787.000000 280.000000 2.563400 119600.000000   50% -118.490000 34.260000 29.000000 2127.000000 435.000000 1166.000000 409.000000 3.534800 179700.000000   75% -118.010000 37.710000 37.000000 3148.000000 647.000000 1725.000000 605.000000 4.743250 264725.000000   max -114.310000 41.950000 52.000000 39320.000000 6445.000000 35682.000000 6082.000000 15.000100 500001.000000      percentiles: what percentage of the data falls beneath this point. i.e. if 50th percentile is 100 for an attribute that means half of all the samples have a value less than 100 for that attribute if mean varies a lot from median that speaks to the presence of outliers pulling it up/down  %matplotlib inline import matplotlib.pyplot as plt  housing.hist(bins=50, figsize=(20,15)) plt.show()   Median income doesn\u0026rsquo;t seem to be expressed as USD Median house value and age seem to be capped features have different scales (needs feature scaling)  Create a test set\n before anything else, set aside test set this will avoid data snooping bias; i.e. fitting the model to better generalize on the test set  import numpy as np  def split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices]  train_set, test_set = split_train_test(housing, 0.2)  len(train_set)  16512  len(test_set)  4128   this function will regenerate different sets on each run you can seed the random number generator, but this breaks if you add new data to the dataset (regenerates new train/test) eventually data you\u0026rsquo;ve trained on before will make it into the test set on multiple reruns with this pipeline can use each instance\u0026rsquo;s indentifier (assuming unique and immutable) to decide whether or not it should go in the test set code below computes a hash of each instance\u0026rsquo;s indentifier and puts that instance in the test set if hash is lower or equal to 20% of the maximum hash value. ensures consistency across multiple runs, and the test set will always contain 20% of the new data, but never any instance that was previously in the training set  from zlib import crc32 def test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) \u0026amp; 0xffffffff \u0026lt; test_ratio * 2**32 def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set]  housing_with_id = housing.reset_index() train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026quot;index\u0026quot;)  train_set   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  index longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   3 3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY   6 6 -122.25 37.84 52.0 2535.0 489.0 1094.0 514.0 3.6591 299200.0 NEAR BAY   ... ... ... ... ... ... ... ... ... ... ... ...   20635 20635 -121.09 39.48 25.0 1665.0 374.0 845.0 330.0 1.5603 78100.0 INLAND   20636 20636 -121.21 39.49 18.0 697.0 150.0 356.0 114.0 2.5568 77100.0 INLAND   20637 20637 -121.22 39.43 17.0 2254.0 485.0 1007.0 433.0 1.7000 92300.0 INLAND   20638 20638 -121.32 39.43 18.0 1860.0 409.0 741.0 349.0 1.8672 84700.0 INLAND   20639 20639 -121.24 39.37 16.0 2785.0 616.0 1387.0 530.0 2.3886 89400.0 INLAND    16512 rows × 11 columns\n   note: this approach looks fugazi, indices aren\u0026rsquo;t shuffled\n  the problem still exists but would need a better implementation than what\u0026rsquo;s here\n  if you use row index as a unique identifier, you must make sure that new data always gets appended to the end of the dataset and a row is never deleted\n  instead, you can try to engineer a unique ID for each row by combining some of the (ideally most stable/constant) features\n  e.g. a district\u0026rsquo;s latitude/longitude is guaranteed to be stable for a few million years lol:\n  housing_with_id['id'] = housing[\u0026quot;longitude\u0026quot;] * 1000 + housing[\u0026quot;latitude\u0026quot;] train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \u0026quot;id\u0026quot;)  len(housing_with_id)  20640  len(housing_with_id[\u0026quot;id\u0026quot;].unique()) # also fugazi  12590   the approach above in the book obviously doesn\u0026rsquo;t work haha  from sklearn.model_selection import train_test_split  train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)  train_set   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     14196 -117.03 32.71 33.0 3126.0 627.0 2300.0 623.0 3.2596 103000.0 NEAR OCEAN   8267 -118.16 33.77 49.0 3382.0 787.0 1314.0 756.0 3.8125 382100.0 NEAR OCEAN   17445 -120.48 34.66 4.0 1897.0 331.0 915.0 336.0 4.1563 172600.0 NEAR OCEAN   14265 -117.11 32.69 36.0 1421.0 367.0 1418.0 355.0 1.9425 93400.0 NEAR OCEAN   2271 -119.80 36.78 43.0 2382.0 431.0 874.0 380.0 3.5542 96500.0 INLAND   ... ... ... ... ... ... ... ... ... ... ...   11284 -117.96 33.78 35.0 1330.0 201.0 658.0 217.0 6.3700 229200.0 \u0026lt;1H OCEAN   11964 -117.43 34.02 33.0 3084.0 570.0 1753.0 449.0 3.0500 97800.0 INLAND   5390 -118.38 34.03 36.0 2101.0 569.0 1756.0 527.0 2.9344 222100.0 \u0026lt;1H OCEAN   860 -121.96 37.58 15.0 3575.0 597.0 1777.0 559.0 5.7192 283500.0 \u0026lt;1H OCEAN   15795 -122.42 37.77 52.0 4226.0 1315.0 2619.0 1242.0 2.5755 325000.0 NEAR BAY    16512 rows × 10 columns\n  the above are random sampling methods which work well enough on a large dataset if the dataset is small then you should do stratified sampling (i.e. take steps to ensure that the sample is representative of the whole pop.) divide the data into different groups (strata) and randomly sample from those in a way which is representative e.g. stratify by median income:  # use pd.cut to bin the median income into categories housing['income_cat'] = pd.cut(housing['median_income'], bins=[0., 1.5, 3., 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])  housing['income_cat'].hist()  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc82c0cf90\u0026gt;  from sklearn.model_selection import StratifiedShuffleSplit  split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) for train_index, test_index in split.split(housing, housing['income_cat']): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index]  # checking to see if it worked as expected strat_test_set[\u0026quot;income_cat\u0026quot;].value_counts()/len(strat_test_set)  3 0.350533 2 0.318798 4 0.176357 5 0.114583 1 0.039729 Name: income_cat, dtype: float64   the stratified sampling matches the proportions seen in the histogram; the test set in this instance is representative of the income_cat distribution found in the whole dataset now remove income_cat attribute to restore data to original form  for set_ in (strat_train_set, strat_test_set): set_.drop(\u0026quot;income_cat\u0026quot;, axis=1, inplace=True)  Discover and Visualize the Data to Gain Insights (EDA)  only explore the training set, pust the test set aside if training set is large, you might want to just explore a sample to make manipulations faster create a copy of the training set so you can play around without harming it  Visualizing Geographical Data:\nhousing = strat_train_set.copy()  housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc822c8990\u0026gt;   Can visualize density in the geographical viz with lower alpha  housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=housing['population']/100, label='population', figsize=(10,7), c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True) plt.legend()  \u0026lt;matplotlib.legend.Legend at 0x7fcc80e608d0\u0026gt;   prices are higher along the coast (duh), with some hotspots in the bay area and around LA could use a clustering algo to find many clusters, and then create features that measure proximity to the cluster centers Ocean proxmity is useful feature  Look for correlations:\n standard correlation coefficient only measures linear relationships, misses any other kinds  corr_matrix = housing.corr()  corr_matrix['median_house_value'].sort_values(ascending=False)  median_house_value 1.000000 median_income 0.687160 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population -0.026920 longitude -0.047432 latitude -0.142724 Name: median_house_value, dtype: float64  from pandas.plotting import scatter_matrix  attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age'] scatter_matrix(housing[attributes], figsize=(12,8)) plt.show()   diagonal lines are histograms of attributes  # focus on median income housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1)  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc83a74110\u0026gt;   clear correlation can also see the cap at $500k quite clearly other less visible lines at \\$350K, $450K; might want to remove from data to stop model from picking up on quirks  Experimenting with Attribute Combinations\n EDA shows some interesting correlations so far some attributes are tail heavy, so could transform to normal distribution by taking their log can take combinations of features to get relationships; some attributes make less sense when considered independently e.g. compute avg. rooms per household as rooms/households  housing['rooms_per_household'] = housing['total_rooms']/housing['households'] housing['bedrooms_per_room'] = housing['total_bedrooms']/housing['total_rooms'] housing['population_per_household'] = housing['population']/housing['households']  # check corr matrix again corr_matrix = housing.corr() corr_matrix['median_house_value'].sort_values(ascending=False)  median_house_value 1.000000 median_income 0.687160 rooms_per_household 0.146285 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population_per_household -0.021985 population -0.026920 longitude -0.047432 latitude -0.142724 bedrooms_per_room -0.259984 Name: median_house_value, dtype: float64   bedrooms per room is much more correlated (negatively) to median house value than total rooms/bedrooms houses with a lower bedroom/room ratio seem to be more expensive EDA doesn\u0026rsquo;t need to be completely thorough, point is to get insights that will get you a reasonably decent prototype iterative process  Prepare the Data for Machine Learning Algorithms write functions to prepare data because:\n can reproduce on fresh data build a library of transformation functions can use in live system easier experimentation  housing = strat_train_set.drop(\u0026quot;median_house_value\u0026quot;, axis=1) housing_labels = strat_train_set[\u0026quot;median_house_value\u0026quot;].copy()  Handling Missing Values\n 3 options: get rid of of corresponding samples get rid of the entire attribute impute values to (mean, median, etc)  from sklearn.impute import SimpleImputer  imputer = SimpleImputer(strategy=\u0026quot;median\u0026quot;)  # create copy of numerical features housing_num = housing.drop(\u0026quot;ocean_proximity\u0026quot;, axis=1)  imputer.fit(housing_num) imputer.statistics_  array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])  X = imputer.transform(housing_num)  housing_tr = pd.DataFrame(X, columns=housing_num.columns)  Handling Text and Categorical Attributes\n encode to numerical values  housing_cat = housing[[\u0026quot;ocean_proximity\u0026quot;]]  from sklearn.preprocessing import OrdinalEncoder  ordinal_encoder = OrdinalEncoder() housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)  housing_cat_encoded[:5]  array([[0.], [0.], [4.], [1.], [0.]])  ordinal_encoder.categories_  [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]   ordinal encoding places ranking on the attributes, so some algos might interpret nearby values as similar (4\u0026amp;3 are more similar than 1\u0026amp;7, although these are all just arbitrary indices) for nominal categories its better to one-hot encode them; create a binary attribute per category also known as creating dummy variables  from sklearn.preprocessing import OneHotEncoder  cat_encoder = OneHotEncoder() housing_cat_1hot = cat_encoder.fit_transform(housing_cat)  housing_cat_1hot  \u0026lt;16512x5 sparse matrix of type '\u0026lt;class 'numpy.float64'\u0026gt;' with 16512 stored elements in Compressed Sparse Row format\u0026gt;   sparse matrices don\u0026rsquo;t use memory to store zero elements  cat_encoder.categories_  [array(['\u0026lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]   if a categorical attribute has a large number of categories, one hot encoding will create a large number of input features and may hurt performance. in this case embeddings are useful (denser representations)  Custom Transformers\n write to align with Scikit-Learn so you can use pipelines add hyperparameters to gate any data preparation steps you aren\u0026rsquo;t sure about  from sklearn.base import BaseEstimator, TransformerMixin  rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6  class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, population_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household]  attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False) housing_extra_attribs = attr_adder.transform(housing.values)  np.c_ stacks arrays on their last axis (turns column vectors into a matrix)\nnp.c_[np.array([1,2,3]), np.array([4,5,6])]  array([[1, 4], [2, 5], [3, 6]])  Feature Scaling:\n  ML algos don\u0026rsquo;t perform well when features have different scales\n  Two ways to feature scale:\n  min-max scaling: attributes are shifted and rescaled so they range from 0-1. $$x_{scaled} = \\frac{x_n - x_{min}}{x_{max} - x_{min}}$$\n  standardization: subtract the mean and divide by standard deviation\n  standardization is less affected by outliers in the data $$x_{scaled} = \\frac{x_n - {\\bar{x}}}{\\sigma}$$\n  fit all transformers to the training set, then use them on the test\n  Transformation Pipelines\nfrom sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler  num_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy=\u0026quot;median\u0026quot;)), ('attribs_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()) ])  housing_num_tr = num_pipeline.fit_transform(housing_num)   calls fit_transform() on all of the transformers sequentially  from sklearn.compose import ColumnTransformer  num_attribs = list(housing_num) cat_attribs = [\u0026quot;ocean_proximity\u0026quot;] full_pipeline = ColumnTransformer([ (\u0026quot;num\u0026quot;, num_pipeline, num_attribs), (\u0026quot;cat\u0026quot;, OneHotEncoder(), cat_attribs), ])  housing_prepared = full_pipeline.fit_transform(housing)  Select and Train a Model Training and Evaluating on the Training Set\n preprocessing steps make training models simple  from sklearn.linear_model import LinearRegression  lin_reg = LinearRegression() lin_reg.fit(housing_prepared, housing_labels)  LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)  some_data = housing.iloc[:5] some_labels = housing_labels.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(f\u0026quot;Predictions: {lin_reg.predict(some_data_prepared)}\u0026quot;) print(f\u0026quot;Labels: {list(some_labels)}\u0026quot;)  Predictions: [211944.80589799 321295.84907457 210851.33029021 62359.51850965 194954.19182968] Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]  from sklearn.metrics import mean_squared_error  housing_predictions = lin_reg.predict(housing_prepared) lin_mse = mean_squared_error(housing_labels, housing_predictions) lin_rmse = np.sqrt(lin_mse) print(lin_rmse)  68898.54780411992   error of \\$68K here isn\u0026rsquo;t very satisfying (housing prices range from \\$120K to \\$265K) if a model is underfitting the data it means that the features don\u0026rsquo;t provide enough information to make good decisions, or model is too weak  from sklearn.tree import DecisionTreeRegressor  tree_reg = DecisionTreeRegressor() tree_reg.fit(housing_prepared, housing_labels)  DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')  housing_predictions = tree_reg.predict(housing_prepared) tree_mse = mean_squared_error(housing_labels, housing_predictions) tree_rmse = np.sqrt(tree_mse) tree_rmse  0.0  Better Evaluation with Cross Validation\n K-fold Cross Validation splits the training set into K folds, training the model K times by testing on one of the folds and composing the training set on the remaining K-1 Sklearns CV feature expects a utility function (greater is better) as opposed to a cost function (lower is better), so the scoring function is actually opposite of MSE, which is why the following code uses -scores in the square root  from sklearn.model_selection import cross_val_score  scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) tree_rmse_scores = np.sqrt(-scores)  def display_scores(scores): print(\u0026quot;Scores:\u0026quot;, scores) print(\u0026quot;Mean:\u0026quot;, scores.mean()) print(\u0026quot;Standard Deviation\u0026quot;, scores.std())  display_scores(tree_rmse_scores)  Scores: [67971.85204287 67101.92229431 69341.35567834 66956.22248918 69739.80377843 72468.59865874 66736.84144169 67241.24548885 72220.02744352 70187.38922156] Mean: 68996.52585375118 Standard Deviation 2037.77518366374   CV not only allows you to get an averaged estimate of model\u0026rsquo;s performance, but allows you to get an idea of how precise this estimate is through the standard deviation  lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores)  Scores: [67500.31361237 68404.48325957 68239.95757613 74813.56736728 68419.88576794 71632.92651865 65216.31837467 68702.06708289 71793.11060978 68131.30099374] Mean: 69285.3931163006 Standard Deviation 2576.7108344336184  from sklearn.ensemble import RandomForestRegressor  forest_reg = RandomForestRegressor() forest_reg.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. \u0026quot;10 in version 0.20 to 100 in 0.22.\u0026quot;, FutureWarning) RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  forest_mse = mean_squared_error(housing_labels, forest_reg.predict(housing_prepared)) forest_rmse = np.sqrt(forest_mse) print(forest_rmse)  21923.44698175341  forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\u0026quot;neg_mean_squared_error\u0026quot;, cv=10) forest_rmse_scores = np.sqrt(-forest_scores)  display_scores(forest_rmse_scores)  Scores: [50985.57169248 49600.72682606 50759.78939086 54557.52337524 51089.83837069 55891.91762608 51010.98084994 50261.62522741 55934.1764328 52825.16027951] Mean: 52291.73100710661 Standard Deviation 2239.368467655192   Forest looks more promising (lower RMSE), but the sizeable difference between CV performance and training set performance indicates overfitting on the training set Could simplify it, regularize it, or get more training data Ideally you want to shortlist 2-5 models promising models total can use joblib library to save promising models for experimentation/later iterations  Fine-Tune Your Model  fine tune the shortlist of models  Grid Search\n grid search will automate cross-validation on all combinations of hyperparameters specified  from sklearn.model_selection import GridSearchCV  param_grid = [ {'n_estimators': [3,10,30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3,10], 'max_features': [2,3,4]}, ]  forest_reg = RandomForestRegressor() grid_search = GridSearchCV(forest_reg, param_grid, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) GridSearchCV(cv='warn', error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), iid='warn', n_jobs=None, param_grid=[{'max_features': [2, 4, 6, 8], 'n_estimators': [3, 10, 30]}, {'bootstrap': [False], 'max_features': [2, 3, 4], 'n_estimators': [3, 10]}], pre_dispatch='2*n_jobs', refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=0)   try powers of 10 if you have no clue what a hyperparameter should have different dictionaries in param_grid specify separate grid searches  grid_search.best_params_  {'max_features': 8, 'n_estimators': 30}  grid_search.best_estimator_  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)   refit=True argument for GridSearchCV will retrain on the entire training set after finding best estimator hyperparams  cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[\u0026quot;mean_test_score\u0026quot;], cvres[\u0026quot;params\u0026quot;]): print(np.sqrt(-mean_score), params)  65307.87885616995 {'max_features': 2, 'n_estimators': 3} 56745.41295641033 {'max_features': 2, 'n_estimators': 10} 52896.1050644663 {'max_features': 2, 'n_estimators': 30} 59681.45476015563 {'max_features': 4, 'n_estimators': 3} 52735.85674503226 {'max_features': 4, 'n_estimators': 10} 50892.22744742235 {'max_features': 4, 'n_estimators': 30} 59799.42942971745 {'max_features': 6, 'n_estimators': 3} 52650.77632649698 {'max_features': 6, 'n_estimators': 10} 50570.40999405222 {'max_features': 6, 'n_estimators': 30} 59094.68896450388 {'max_features': 8, 'n_estimators': 3} 52492.707418330094 {'max_features': 8, 'n_estimators': 10} 50432.33406200777 {'max_features': 8, 'n_estimators': 30} 62530.835324885855 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3} 55183.05939812397 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10} 60507.19412492283 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3} 53097.84240758412 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10} 59391.108929740585 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3} 52293.331430755774 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}   can include data preparation steps as hyperparameters in grid search  Randomized Search\n when hyperparameter search space is large, randomized usually gets comparable results faster explores $n$ different values for each hyperparameter for $n$ total iterations gives you more control over computing budget  Ensemble Methods\n Can combine models that perform best Will often perform better than the best individual model, especially if the models make different types of errors  Analyze the Best Models and their Errors\n Can gain insights by inspecting best models Random Forest can give feature importances:  feature_importances = grid_search.best_estimator_.feature_importances_  extra_attribs = [\u0026quot;rooms_per_hhold\u0026quot;, \u0026quot;pop_per_hhold\u0026quot;, \u0026quot;bedrooms_per_room\u0026quot;] cat_encoder = full_pipeline.named_transformers_[\u0026quot;cat\u0026quot;] cat_one_hot_attribs = list(cat_encoder.categories_[0]) attributes = num_attribs + extra_attribs + cat_one_hot_attribs sorted(zip(feature_importances, attributes), reverse=True)  [(0.3938202057578966, 'median_income'), (0.15133729393112902, 'INLAND'), (0.1029603450675752, 'bedrooms_per_room'), (0.0754751387108862, 'pop_per_hhold'), (0.06620720663844283, 'longitude'), (0.06338183338235212, 'latitude'), (0.04206999831411881, 'housing_median_age'), (0.03242004947783398, 'rooms_per_hhold'), (0.015482413139961633, 'total_bedrooms'), (0.014766900750197388, 'population'), (0.014573461291699393, 'total_rooms'), (0.014411203134276336, 'households'), (0.006778298384461372, '\u0026lt;1H OCEAN'), (0.0032380720741344592, 'NEAR OCEAN'), (0.002983526044293852, 'NEAR BAY'), (9.40539007408315e-05, 'ISLAND')]   could drop less important features and try retraining could also analyze the specific errors the system makes and try to understand why they\u0026rsquo;re occuring and what could fix (extra features, cleaning outliers, removing uninformative features)  Evaluate Your Sytem on the Test Set  just apply pipeline to the test set (run transform, not fit_transform - we do not want to fit it to the training set!)  final_model = grid_search.best_estimator_ X_test = strat_test_set.drop(\u0026quot;median_house_value\u0026quot;, axis=1) y_test = strat_test_set[\u0026quot;median_house_value\u0026quot;].copy()  X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared)  final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse)  print(final_rmse)  48026.91757298586   in some situations a point estimate of the generalization error will not be enough evidence to launch you can get an idea of precision by computing a 95% confidence interval for the generalization error:  from scipy import stats  confidence = 0.95 squared_errors = (final_predictions - y_test) ** 2 np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(), scale=stats.sem(squared_errors)))  array([46060.77003181, 49915.67977301])   performance will likely be a bit worse than what was measured in cross-validation resist temptation to tweak numbers on the test set present solution, highlighting assumptions, methodology, system limitations create presentations with clear visuals and keep points summarized  Launch, Monitor, and Maintain Your System   launch in prod, polishing code, writing tests and documentation, etc\n  can deploy model by loading it in production environment or wrapping it in a REST api (i.e. cloud functions, microservices)\n  this offers easier updating of system without disrupting main application, and can handle load balancing\n  can deploy in cloud (GCP)\n  need to write monitoring code after deployment to make sure performance doesn\u0026rsquo;t rot over time\n  models decay as the world changes and old data becomes less relevant, need to feed with new data\n  model performance can be inferred from downstream metrics (e.g. how many sales your recommender system is generating)\n  can use human testers to verfiy model outputs (experts, or crowdsourced from platforms like Amazon Mechanical Turk)\n  putting models in prod and maintaining them can be more work than actually creating them\n  automate as much of the process as possible:\n  collecting fresh data regularly and labelling (e.g. using human raters)\n  writing scripts to train the model and fine-tune hyperparameters automatically\n  writing scripts to evaluate new model vs. old model on updated test sets, and deploying the new model if performance has not decreased\n  monitor data quality as well, poor quality data leaking in can be hard to detect\n  create backups of every model and have infrastructure to rollback previous models quickly, in case new ones start failing badly for some reason\n  backups help for investigation/comparison\n  keep backups of datasets as well\n  Exercises  Try a Support Vector Machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\u0026ldquo;linear\u0026rdquo; (with various values for the C hyperparameter) or kernel=\u0026ldquo;rbf\u0026rdquo; (with various values for the C and gamma hyperparameters). Don’t worry about what these hyperparameters mean for now. How does the best SVR predictor perform?  from sklearn.svm import SVR  svr = SVR() param_grid = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1.0, 10.0, 100.0]}, {'kernel': ['rbf'], 'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'gamma': ['scale', 'auto']} ] grid_search = GridSearchCV(svr, param_grid, scoring='neg_mean_squared_error', return_train_score=True) grid_search.fit(housing_prepared, housing_labels)  /Users/willbarker/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning. warnings.warn(CV_WARNING, FutureWarning) GridSearchCV(cv='warn', error_score='raise-deprecating', estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False), iid='warn', n_jobs=None, param_grid=[{'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'kernel': ['linear']}, {'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'gamma': ['scale', 'auto'], 'kernel': ['rbf']}], pre_dispatch='2*n_jobs', refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=0)  best_svr = grid_search.best_estimator_  best_svr  SVR(C=100.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto_deprecated', kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)  np.sqrt(-grid_search.best_score_)  72427.44454616884  Try replacing GridSearchCV with RandomizedSearchCV.  from sklearn.model_selection import RandomizedSearchCV from scipy.stats import uniform from scipy.stats import norm from pprint import pprint  random_forest = RandomForestRegressor()  print(\u0026quot;Parameters currently in use:\\n\u0026quot;) pprint(random_forest.get_params())  Parameters currently in use: {'bootstrap': True, 'criterion': 'mse', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}  random_forest = RandomForestRegressor() n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)] max_features = ['auto', 'sqrt'] max_depth = [int(x) for x in np.linspace(10,110, num=11)] max_depth.append(None) min_samples_split = [2, 5, 10] min_samples_leaf = [1,2,4] bootstrap = [True, False] random_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap} random_search = RandomizedSearchCV(estimator=random_forest, param_distributions=random_grid, scoring='neg_mean_squared_error', verbose=1, n_iter=10, cv=3, n_jobs=-1, return_train_score=True)  random_search.fit(housing_prepared, housing_labels)  Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 31.4min finished RandomizedSearchCV(cv=3, error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_sta... param_distributions={'bootstrap': [True, False], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4], 'min_samples_split': [2, 5, 10], 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}, pre_dispatch='2*n_jobs', random_state=None, refit=True, return_train_score=True, scoring='neg_mean_squared_error', verbose=1)  best_forest = random_search.best_estimator_  best_forest  RandomForestRegressor(bootstrap=False, criterion='mse', max_depth=110, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)  np.sqrt(-random_search.best_score_)  49359.61219717077  Try adding a transformer in the preparation pipeline to select only the most important attributes.  class FeatureImportancesFilter(BaseEstimator, TransformerMixin): def __init__(self, feature_importances, importance_cutoff = 0.9): # no *args or **kargs self.feature_importances = feature_importances feature_indices = [i for i in range(len(self.feature_importances))] self.sorted_feature_importances = sorted(zip(self.feature_importances, feature_indices), reverse=True) self.importance_cutoff = importance_cutoff def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): importance_sum = 0 included_features = list() for importance, ix in self.sorted_feature_importances: if not importance_sum \u0026gt;= self.importance_cutoff: importance_sum += importance included_features.append(ix) else: break return X[:, included_features]   note: might need to fit to actual random forest regressor, or breaks if input shape changes  housing_prepared.shape  (16512, 16)  importance_filter = FeatureImportancesFilter(feature_importances, importance_cutoff=0.9)  importance_filter.transform(housing_prepared).shape  (16512, 8)  Try creating a single pipeline that does the full data preparation plus the final prediction.  e2e_pipeline = Pipeline([ ('data_prep', full_pipeline), ('filter', FeatureImportancesFilter(feature_importances)), ('clf', best_forest) ])  Automatically explore some preparation options using GridSearchCV.  e2e_grid_params = {'filter__importance_cutoff': (0.5, 0.6, 0.65)}  e2e_grid_search = GridSearchCV(e2e_pipeline, e2e_grid_params, scoring='neg_mean_squared_error', verbose=1, cv=3)  e2e_grid_search.fit(housing, housing_labels)  Fitting 3 folds for each of 3 candidates, totalling 9 fits [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 9 out of 9 | elapsed: 1.4min finished GridSearchCV(cv=3, error_score='raise-deprecating', estimator=Pipeline(memory=None, steps=[('data_prep', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3, transformer_weights=None, transformers=[('num', Pipeline(memory=None, steps=[('imputer', SimpleImputer(add_indicator=False, copy=True, fill_value=None, missing_values=nan, strategy='median'... min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False))], verbose=False), iid='warn', n_jobs=None, param_grid={'filter__importance_cutoff': (0.5, 0.6, 0.65)}, pre_dispatch='2*n_jobs', refit=True, return_train_score=False, scoring='neg_mean_squared_error', verbose=1)  np.sqrt(-e2e_grid_search.best_score_)  66661.60437351998  e2e_grid_search.best_params_  {'filter__importance_cutoff': 0.65} ","date":"2020-08-17","permalink":"https://billwarker.com/posts/handson-ml-ch2/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 2 Notes"},{"content":"Notes Types of ML\n  ML: algorithms learning from data and improving performance on a task\n  advantage over rule based systems is that machine can update parameters/logic with new/more data (refreshing the model)\n  applying ML to discover/understand patterns in the data: data mining\n  Supervised learning: training a model with labelled examples. can be used for classification tasks (predict a discrete category) or regression (predict a continuous number)\n  Unsupervised learning algos: clustering unlabelled data, visualizing and dimensionality reduction, association rule learning\n  hierarchical clustering: sub-dividing clusters into smaller groups\n  dimensionality reduction: simplify data without losing too much information. i.e. merging correlated features. can help performance, takes up less disk and memory space\n  anomoly detection\n  association rule learning: relations between attributes (similar to data mining)\n  Semi-supervised learning. i.e. Google Photos, looks at unlabelled pictures to find common faces. once given a label for a person it can name everyone in the photo. mixed hierarchical models\n  Reinforcement learning: agent learns its environment and selects a policy (actions, strategy) to optimize some reward\n  Batch vs. Online Learning\n  Batch: training a model on all available data. train the system and then put it in prod. retrain new versions of the model with new data\n  requires a lot of computing resources, can be expensive. not suitable for autonomous systems with limited space for data (i.e. Mars rover)\n  Online: train system incrementally on mini-batches of data\n  great for continuous flows of data or limited storage resources\n  can be used to train on datasets that won\u0026rsquo;t fit in memory (out-of-core learning)\n  learning rate: how fast model adapts to new data\n  too high, forgets old data rapidly, too low, system has inertia (also less sensitive to noisey data)\n  need to monitor online systems if garbage data starts coming in\n  Instance-Based vs. Model-Based Learning\n  how a system generalizes (i.e. answers examples its never seen before)\n  Instance-based: generalizes to new examples by comparing similarity to training examples\n  i.e. KNN\n  Model-based: builds a model to predict new data\n  select a model (i.e. linear model) to represent the data\u0026rsquo;s pattern\n  tune model on a utility or cost function\n  if a model doesn\u0026rsquo;t generalize well, you can try again with better quality training data, more features, or a stronger model (e.g. polynomial vs. basic linear)\n  adding more data tends to get better results on all kinds of algos, to the point where performance can be identical with enough data\n  data needs to be representitive of the problem space trying to model, more data can eliminate noise but procedure needs to be solid or it risks sampling bias\n  Feature Engineering\n feature selection, picking the most useful features feature extraction, combining existing features to produce more useful ones + dimensionality reduction creating new features with the intro of new data  Performance\n  overfitting: performing well on the training data but not generalizing well\n  machine learning can pick up on noise in the data and sometimes even irrelevant features/useless metadata (i.e. data ids/index), detecting false patterns\n  overfitting happens when the model is too complex relative to the amount and noiseness of the data\n  regularization is constraining a model to make it simpler can help overfitting\n  controlled by model hyperparameters (knobs to tweak on the model itself, such as learning rate)\n  underfitting is opposite problem, model is too simple\n  fix it with a more complex model, more/better features, reducing regularization constraints\n  test models to see if they generalize well\n  split data into training and test sets to get error rate on new cases (out of sample/generalization error)\n  if training error is now but oos error is high, overfitting\n  use a third validation set to compare models/hyperparameters, then select the best one and use it one the test\n  cross validation splits the training set into subsets which are used for validation\n  need to make assumptions about the data to pick models reasonably\n  Questions How would you define ML?\n Algorithms that allow a computer to learn from data to improve on a task and generalize to new examples well  Four types of problems where it shines?\n Problems that traditionally require too many rules or hand-tuning Complex problems with no easy logic based model Problems where underlying strategy/solutions change over time, so new model can help adapt Data mining, learning underlying patterns in data  What is a labelled training set?\n Data that has the variable of interest classified (independent variable), and information about it in other attribures (dependent variables)  Two most common supervised tasks?\n Regression: predicting a continuous number for the target variable Classification: predicting a discrete category of target variable  Four kinds of unsupervised tasks?\n Clustering: creating groups in unstructured data Dimesionality Reduction: reducing the number of attributes in the training set while keeping most of the variance(underlying signal) Anomoly Dection: finding outliers Visualization Association Rule learning: data mining, learning patterns in the data  What type of ML algo would you use to allow a robot to walk in various unknown terrains?\n Reinforcement Learning  What type of algo would you use to segment your customers into multiple groups?\n Clustering, Unsupervised Learning  Would you frame the problem of spam detection as a supervised learning problem?\n Supervised; we can use examples of spam and ham (labelled training data) to create a model that identifies which is which  What is an online learning system?\n A system that can update with new data as it comes in, ingesting as it comes in through mini-batches  What is an out-of-core learning system?\n Using online learning to train the model on a dataset that wouldn\u0026rsquo;t fit inside the computers memory if you tried to train it in one giant batch.  What type of learning algo relies on similarity measures to make predictions?\n Instanced-based models, i.e. KNN  Difference between a model parameter and a hyperparameter?\n A model parameter is the coefficient determined by the algorithm to apply to a attribute/feature in the data when making predictions, a hyperparameter is an aspect of the model that you can adjust to change how it is trained  What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?\n These algos look to fit the model to the data (i.e. represent the problem with some simplified version of it). The strategy is to improve performance in respect to some cost/utility function. They make new predictions by applying the policy/parameters determined through training on the new data\u0026rsquo;s features  Four main challenges in ML?\n Not enough data Non-representative data Poor quality data Overfitting/Underfitting  If a model performs well on the training data but generalizes poorly to new examples, what is happening? 3 possible solutions?\n Model is overfitting to the training data and can\u0026rsquo;t generalize well to the new examples You can regularize the model (i.e. constrain it) to be less representative of the training data Train the model on more data/more representative data Tune the model on a validation set  What is a test set and why would you want to use it?\n Test the performance of the model on new examples to understand how well it generalizes to new data (i.e. the whole point)  What can go wrong if you tune hyperparameters on the test set?\n You fit the model to work well on the testing set, overfitting it and reducing the chance of generalizing well  What is cross validation, why is it better than a validation set?\n Cross validation takes different chunks of the training data and uses them iteratively to train and validate the model, creating a more robust model (training on different samples) and is a more economic use of data. ","date":"2020-08-02","permalink":"https://billwarker.com/posts/handson-ml-ch1/","tags":["handson-ml","ml"],"title":"Hands on Machine Learning - Chapter 1 Notes"},{"content":"The Standard Error of the Mean The Standard Error of the Mean ($SE$) is the standard deviation of the sample distribution of the sample mean. To understand what this means, let\u0026rsquo;s break that sentence down in reverse order (i.e. chronologically):\nSample Mean: we have some probability density function $P$ for a population. We take a sample of $N$ instances from it and calculate our statistic of interest - in this case the mean, $\\bar{x}$. We have to take samples because it is hard/impossible to look at every instance in an entire population to calculate the true mean $\\mu$ hidden to us in nature (i.e. say we were interested in the average weight of all monkeys on the planet).\nSample Distribution: we take many samples (the number of which denoted by $M$) from the population\u0026rsquo;s probability density function $P$ and calculate the sample mean $\\bar{x}$ for each one. All of these sample means can be lumped together into a distribution $S$, which approximates a normal distribution the higher $M$ is due to the Central Limit Theorum. We want to create a sampling distribution because it allows us to reason about the likelihood of different values the sample mean can be. By doing so, we can look at the mean of this distribution and conclude that its the most likely value for $\\mu$ given the data we\u0026rsquo;ve used.\nStandard Deviation: The standard deviation is a metric that describes the dispersion of a dataset/distribution in relation to its mean. It speaks to how close/far the density of the distribution is spread in relation to its mean. The standard deviation of a sample distribution - our standard error $SE$ - is an indication of how representative the distribution is of our true mean $\\mu$. The smaller it is, the more representative it can be said to be; the larger it is, the harder it is to trust.\nTLDR: The Standard Error matters because it allows us to better understand how representative our sampling distribution is (i.e. our model of the true mean).\n$SE$ can be calculated with this formula:\n$$SE = \\frac{\\sigma}{\\sqrt{N}}$$\nwhere ${\\sigma}$ is the standard deviation of the population\u0026rsquo;s probability density function $P$ and $N$ is the number of instances in a sample.\nUnderstanding the Standard Error through Simulation import numpy as np import scipy as sp import pandas as pd import matplotlib.pyplot as plt  # create a random probability distribution function to model our population # in this case a Maxwell continuous random variable (picked randomly) x = np.linspace(sp.stats.maxwell.ppf(0.01), sp.stats.maxwell.ppf(0.99), 100) plt.plot(x, sp.stats.maxwell.pdf(x), 'b-', lw=5, alpha=0.6, label='maxwell pdf') plt.show() mean, var, skew, kurt = sp.stats.maxwell.stats(moments='mvsk') print(f'The true mean of the population is {mean} and its standard deviation is {var**(1/2)}')  The true mean of the population is 1.5957691216057308 and its standard deviation is 0.6734396116428514  # to create our sampling distribution S we take M samples of N instances each # we calculate the mean of each sample and add it to a list, which we can make a histogram with m_samples = 5000 n_instances = 250 sample_means = [] for m in range(m_samples): sample = sp.stats.maxwell.rvs(size=n_instances) # draw a sample from the population sample_means.append(sample.mean()) # add it to our sampling distribution sample_dist = pd.Series(sample_means) # our sample distribution as a pandas series  sample_dist.hist(bins=30) # visualizing our sample distribution of the sample mean  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1341e8fd0\u0026gt;  print(f\u0026quot;The mean of our sample distribution is {sample_dist.mean()}\u0026quot;) print(f\u0026quot;Its standard deviation (The Standard Error) is {sample_dist.std()}\u0026quot;)  The mean of our sample distribution is 1.5960355829136528 Its standard deviation (The Standard Error) is 0.04222284486122923  # calculating the same SE from the formula above se = np.sqrt(var)/np.sqrt(n_instances) print(f\u0026quot;The standard error calculated with the formula is: {se}\u0026quot;)  The standard error calculated with the formula is: 0.042592060787413163 ","date":"2020-07-12","permalink":"https://billwarker.com/posts/standard-error-of-the-mean/","tags":["stats"],"title":"The Standard Error of the Mean"},{"content":"Permutations of a Set Permutations of a set are particular orderings of its elements. To calculate the number of permutations (i.e. the possible orderings) of a subset $k$ distinct elements from a set of size $n$, the formula is:\n$$N(n, k) = \\frac{n!}{(n-k)!}$$\n$n!$ represents all orderings for every element in the set. $(n-k)!$ represents the orderings of the elements we\u0026rsquo;re not including in the subset of $k$ elements - dividing by this number allows us to factor them out of $n!$ in the numerator and give us the number of permutations.\nimport itertools import math S = ['A', 'B', 'C', 'D'] permutations = itertools.permutations(S, 2) # P(4,2) for p in permutations: print(p)  ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'A') ('B', 'C') ('B', 'D') ('C', 'A') ('C', 'B') ('C', 'D') ('D', 'A') ('D', 'B') ('D', 'C')  def permutations(n, k): return int(math.factorial(n)/math.factorial(n-k)) permutations(4, 2)  12  Combinations of a Set Combinations don\u0026rsquo;t care about order - they are just the different subsets of elements in the set. To calculate the number of combinations (i.e. subsets) of $k$ distinct elements from a set of size $n$, the formula is:\n$$C(n, k) = \\frac{n!}{k!(n-k)!}$$\nAn easy way to understand combinations is in relation to permutations - basically, we are calculating the number of permutations of a subset created by $P(n, k)$ and then just dividing that number by the possible ways to order its elements (since combinations don\u0026rsquo;t care about that).\n$$C(n, k) = \\frac{P(n, k)}{k!} = \\frac{n!}{(n-k)!}*\\frac{1}{k!}$$\nS = ['A', 'B', 'C', 'D'] combinations = itertools.combinations(S, 2) # P(4,2) for c in combinations: print(c)  ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'C') ('B', 'D') ('C', 'D')  def combinations(n, k): return int(permutations(n, k)/math.factorial(k)) combinations(4, 2)  6 ","date":"2020-07-07","permalink":"https://billwarker.com/posts/permutations-and-combinations/","tags":["stats"],"title":"Permutations and Combinations"}]