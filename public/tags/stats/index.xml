<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stats on Data Science Notes</title>
    <link>https://billwarker.com/tags/stats/</link>
    <description>Recent content in stats on Data Science Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 15 Mar 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://billwarker.com/tags/stats/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Causality for the Brave and True: Introduction to Causality</title>
      <link>https://billwarker.com/posts/introduction-to-causality/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/introduction-to-causality/</guid>
      <description>&lt;p&gt;Notes from &lt;i&gt;Causal Inference for the Brave and True&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://matheusfacure.github.io/python-causality-handbook/01-Introduction-To-Causality.html#bias&#34;&gt;https://matheusfacure.github.io/python-causality-handbook/01-Introduction-To-Causality.html#bias&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;introduction-to-causality&#34;&gt;Introduction to Causality&lt;/h1&gt;
&lt;p&gt;Data Science is kind of like a cup of beer, with a little bit of foam on the top:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The beer is statistical foundations, scientific curiousity, passion for difficult problems&lt;/li&gt;
&lt;li&gt;The foam is the hype and unrealistic expectations that will disappear eventually&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Focus on what makes your work valuable.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ANOVA</title>
      <link>https://billwarker.com/posts/anova/</link>
      <pubDate>Sun, 20 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/anova/</guid>
      <description>&lt;h3 id=&#34;use&#34;&gt;Use&lt;/h3&gt;
&lt;p&gt;ANOVA (Analysis of Variance) is a significance test that tests whether the population means from $n$ different groups are the same using the F distribution (therefore only works on numerical response data). It can test between two or more populations, therefore generalizing the t-test beyond just two groups. Examples of when you might want to do an ANOVA test:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Testing whether employee stress levels are the same or different before, during, and after layoffs.&lt;/li&gt;
&lt;li&gt;Students from different colleges take the same exam. You want to see if one college outperforms the other.&lt;/li&gt;
&lt;li&gt;A factory is testing three different methods for producing breadsticks and wants to know which yields a superior crunchiness.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Calculating Sample Size to Ensure High Power</title>
      <link>https://billwarker.com/posts/calculating-sample-size-to-ensure-high-power/</link>
      <pubDate>Sun, 29 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/calculating-sample-size-to-ensure-high-power/</guid>
      <description>&lt;p&gt;Part 3 of 3 on a series of notes covering margin of error, power, and sample size calculations.&lt;/p&gt;
&lt;p&gt;Notes, with questions and examples, taken from the following reading: &lt;a href=&#34;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&#34;&gt;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding Statistical Power</title>
      <link>https://billwarker.com/posts/understanding-statistical-power/</link>
      <pubDate>Sun, 22 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/understanding-statistical-power/</guid>
      <description>&lt;p&gt;Part 2 of 3 on a series of notes covering margin of error, power, and sample size calculations.&lt;/p&gt;
&lt;p&gt;Notes, with questions and examples, taken from the following reading: &lt;a href=&#34;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&#34;&gt;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Understanding Margin of Error and Sample Size</title>
      <link>https://billwarker.com/posts/understanding-margin-of-error-and-sample-size/</link>
      <pubDate>Sat, 21 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/understanding-margin-of-error-and-sample-size/</guid>
      <description>&lt;p&gt;Part 1 of 3 on a series of notes covering margin of error, power, and sample size calculations.&lt;/p&gt;
&lt;p&gt;Notes, with questions and examples, taken from the following reading: &lt;a href=&#34;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&#34;&gt;https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Power/BS704_Power_print.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Standard Error of the Mean</title>
      <link>https://billwarker.com/posts/standard-error-of-the-mean/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/standard-error-of-the-mean/</guid>
      <description>&lt;h2 id=&#34;the-standard-error-of-the-mean&#34;&gt;The Standard Error of the Mean&lt;/h2&gt;
&lt;p&gt;The Standard Error of the Mean ($SE$) is the &lt;strong&gt;standard deviation&lt;/strong&gt; of the &lt;strong&gt;sample distribution&lt;/strong&gt; of the &lt;strong&gt;sample mean&lt;/strong&gt;. To understand what this means, let&amp;rsquo;s break that sentence down in reverse order (i.e. chronologically):&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample Mean&lt;/strong&gt;: we have some probability density function $P$ for a population. We take a sample of $N$ instances from it and calculate our statistic of interest - in this case the mean, $\bar{x}$. We have to take samples because it is hard/impossible to look at every instance in an entire population to calculate the true mean $\mu$ hidden to us in nature (i.e. say we were interested in the average weight of all monkeys on the planet).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample Distribution&lt;/strong&gt;: we take many samples (the number of which denoted by $M$) from the population&amp;rsquo;s probability density function $P$ and calculate the sample mean $\bar{x}$ for each one. All of these sample means can be lumped together into a distribution $S$, which approximates a normal distribution the higher $M$ is due to the &lt;strong&gt;Central Limit Theorum&lt;/strong&gt;. We want to create a sampling distribution because it allows us to reason about the likelihood of different values the sample mean can be. By doing so, we can look at the mean of this distribution and conclude that its the most likely value for $\mu$ given the data we&amp;rsquo;ve used.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Standard Deviation&lt;/strong&gt;: The standard deviation is a metric that describes the dispersion of a dataset/distribution in relation to its mean. It speaks to how close/far the density of the distribution is spread in relation to its mean. The standard deviation of a sample distribution - our standard error $SE$ - is an indication of how representative the distribution is of our true mean $\mu$. The smaller it is, the more representative it can be said to be; the larger it is, the harder it is to trust.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TLDR: The Standard Error matters because it allows us to better understand how representative our sampling distribution is (i.e. our model of the true mean).&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Permutations and Combinations</title>
      <link>https://billwarker.com/posts/permutations-and-combinations/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://billwarker.com/posts/permutations-and-combinations/</guid>
      <description>&lt;h1 id=&#34;permutations-of-a-set&#34;&gt;Permutations of a Set&lt;/h1&gt;
&lt;p&gt;Permutations of a set are particular orderings of its elements. To calculate the number of permutations (i.e. the possible orderings) of a subset $k$ distinct elements from a set of size $n$, the formula is:&lt;/p&gt;
&lt;p&gt;$$N(n, k) = \frac{n!}{(n-k)!}$$&lt;/p&gt;
&lt;p&gt;$n!$ represents all orderings for every element in the set. $(n-k)!$ represents the orderings of the elements we&amp;rsquo;re not including in the subset of $k$ elements - dividing by this number allows us to factor them out of $n!$ in the numerator and give us the number of permutations.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>